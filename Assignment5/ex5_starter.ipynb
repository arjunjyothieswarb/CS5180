{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunjyothieswarb/CS5180/blob/main/Assignment5/ex5_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkJRVQL8LiQK"
      },
      "source": [
        "# Question 4 and 5 Windy Gridworld Domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kIWquRnyLiQM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n2obwIjyLiQN"
      },
      "outputs": [],
      "source": [
        "class WindyGridWorld(object):\n",
        "    def __init__(self, enable_king_move=False, enable_no_move=False):\n",
        "        # define the grid space\n",
        "        self.grid = np.zeros((7, 10))\n",
        "\n",
        "        # define the state space\n",
        "        self.state_space = [[r, c] for r, c in zip(np.where(self.grid == 0.0)[0],\n",
        "                                                   np.where(self.grid == 0.0)[1])]\n",
        "\n",
        "        # define the start state\n",
        "        self.start_state = [3, 0]\n",
        "\n",
        "        # define the goal state\n",
        "        self.goal_state = [3, 7]\n",
        "\n",
        "        # define the wind\n",
        "        self.wind = np.array([0, 0, 0, 1, 1, 1, 2, 2, 1, 0], dtype=int)\n",
        "\n",
        "        # define the action space\n",
        "        self.action_space = {\n",
        "            \"up\": np.array([-1, 0]),\n",
        "            \"down\": np.array([1, 0]),\n",
        "            \"left\": np.array([0, -1]),\n",
        "            \"right\": np.array([0, 1])\n",
        "        }\n",
        "\n",
        "        # Enable King's moves (Comment out the above action space to create a new one for King's moves)\n",
        "        # if enable_king_move:\n",
        "\n",
        "        #     if enable_no_move:\n",
        "        #         #TODO\n",
        "        #     else:\n",
        "        #         #TODO\n",
        "        #\n",
        "        # else:\n",
        "        #     #TODO\n",
        "\n",
        "\n",
        "        # track the current state, time step, and action\n",
        "        self.state = None\n",
        "        self.t = None\n",
        "        self.act = None\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # reset the agent to the start state\n",
        "        self.state = self.start_state\n",
        "        # reset the time step tracker\n",
        "        self.t = 0\n",
        "        # reset the action tracker\n",
        "        self.act = None\n",
        "        # reset the terminal flag\n",
        "        terminated = False\n",
        "        return self.state, terminated\n",
        "\n",
        "    def step(self, act):\n",
        "\n",
        "        #TODO\n",
        "        terminated = False\n",
        "\n",
        "        # Getting the action\n",
        "        action = self.action_space[act]\n",
        "\n",
        "        # Updating the state\n",
        "        self.state[0] += action[0] + self.wind[self.state[1]]\n",
        "        self.state[1] += action[1]\n",
        "\n",
        "        # Clipping the state values\n",
        "        self.state[0] = min(self.state[0], 6)\n",
        "        self.state[0] = max(self.state[0], 0)\n",
        "\n",
        "        self.state[1] = min(self.state[1], 9)\n",
        "        self.state[1] = max(self.state[1], 0)\n",
        "\n",
        "        # Checking if the agent has reached the goal state\n",
        "        if self.state == self.goal_state:\n",
        "            reward = 0\n",
        "            terminated = True\n",
        "        else:\n",
        "            reward = -1\n",
        "\n",
        "        self.t += 1\n",
        "        self.act = act\n",
        "\n",
        "        return self.state, reward, terminated\n",
        "\n",
        "    def next_state(self, state, act):\n",
        "\n",
        "        # Getting the action\n",
        "        action = self.action_space[act]\n",
        "\n",
        "        # Updating the state\n",
        "        state[0] += action[0] + self.wind[self.state[1]]\n",
        "        state[1] += action[1]\n",
        "\n",
        "        # Clipping the state values\n",
        "        state[0] = min(state[0], 6)\n",
        "        state[0] = max(state[0], 0)\n",
        "\n",
        "        state[1] = min(state[1], 9)\n",
        "        state[1] = max(state[1], 0)\n",
        "\n",
        "        return state\n",
        "\n",
        "    def render(self):\n",
        "        # plot the agent and the goal\n",
        "        # agent = 1\n",
        "        # goal = 2\n",
        "        plot_arr = self.grid.copy()\n",
        "        plot_arr[self.state[0], self.state[1]] = 1.0\n",
        "        plot_arr[self.goal_state[0], self.goal_state[1]] = 2.0\n",
        "        plt.clf()\n",
        "        fig, arr = plt.subplots(1, 1)\n",
        "        arr.set_title(f\"state={self.state}, act={self.act}\")\n",
        "        arr.imshow(plot_arr)\n",
        "        plt.show(block=False)\n",
        "        plt.pause(1)\n",
        "        plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windy = WindyGridWorld()\n",
        "print(windy.state_space)"
      ],
      "metadata": {
        "id": "s2Hy4YypshaJ",
        "outputId": "82e27cc1-2102-4cd6-8b4c-7a9f0ebfb688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [3, 9], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [5, 8], [5, 9], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 6], [6, 7], [6, 8], [6, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XY2dnbwuLiQO"
      },
      "outputs": [],
      "source": [
        "class SARSA(object):\n",
        "    def __init__(self, env, alpha, epsilon, gamma, timeout):\n",
        "        # define the parameters\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # environment\n",
        "        self.env = env\n",
        "\n",
        "        # define the Q value table\n",
        "        self.state_num = len(self.env.state_space)\n",
        "        self.action_num = len(self.env.action_space.keys())\n",
        "        self.Q = np.zeros((self.state_num, self.action_num))\n",
        "\n",
        "        # define the timeout\n",
        "        self.timeout = timeout\n",
        "\n",
        "        # define action_space_index\n",
        "        self.action_keys = list(self.env.action_space.keys())\n",
        "\n",
        "        # define the action_key_index\n",
        "        self.action_key_index = {\"up\":0, \"down\":1, \"left\":2, \"right\":3, \"up-left\":4, \"up-right\":5, \"down-left\":6, \"down-right\":7, \"freeze\":8}\n",
        "\n",
        "    def behavior_policy(self, state):\n",
        "        #TODO\n",
        "\n",
        "        # Generating a random variable\n",
        "        choice = np.random.random()\n",
        "\n",
        "        # Exploration vs Exploitation\n",
        "        if choice < self.epsilon:\n",
        "            action = self.action_keys[np.random.randint(0,self.action_num)]\n",
        "        else:\n",
        "            Q_list = []\n",
        "            for i in range(self.action_num):\n",
        "                act = self.action_keys[i]\n",
        "\n",
        "                state_prime = self.env.next_state(state, act)\n",
        "                state_prime_index = (state_prime[0] * 10) + state_prime[1]\n",
        "\n",
        "                Q_list.append(self.Q[state_prime][i])\n",
        "\n",
        "            max_indices = np.argwhere(Q_list == np.max(Q_list)).T[0]\n",
        "            action_index = np.random.choice(max_indices)\n",
        "\n",
        "            action = self.action_keys[action_index]\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, s, a, r, s_prime, a_prime):\n",
        "        #TODO\n",
        "\n",
        "        # Getting the indices of states and actions\n",
        "        state_index = (s[0] * 10) + s[1]\n",
        "        state_prime_index = (s_prime[0] * 10) + s_prime[1]\n",
        "\n",
        "        action_index = self.action_key_index[a]\n",
        "        action_prime_index = self.action_key_index[a_prime]\n",
        "\n",
        "        # Updating using SARSA\n",
        "        self.Q[state_index][action_index] += self.alpha * (r + (self.gamma * self.Q[state_prime_index][action_prime_index]) - self.Q[state_index][action_index])\n",
        "\n",
        "        return None\n",
        "\n",
        "    def epsilon_greedy(self, state):\n",
        "\n",
        "        # Generating a random variable\n",
        "        choice = np.random.random()\n",
        "\n",
        "        # Exploration vs Exploitation\n",
        "        if choice < self.epsilon:\n",
        "            action = self.action_keys[np.random.randint(0,self.action_num)]\n",
        "        else:\n",
        "            action = self.behavior_policy(state)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def rollout(self):\n",
        "\n",
        "        # Resetting the env\n",
        "        state, terminated = self.env.reset()\n",
        "        prev_state = state\n",
        "\n",
        "        # Getting action\n",
        "        action = self.epsilon_greedy(state)\n",
        "\n",
        "        while(not terminated):\n",
        "\n",
        "            # Checking for timeout\n",
        "            if self.env.t > self.timeout:\n",
        "                break\n",
        "\n",
        "            # Taking the step\n",
        "            state, reward, terminated = self.env.step(action)\n",
        "\n",
        "            action_prime = self.epsilon_greedy(state)\n",
        "\n",
        "            self.update(prev_state, action, reward, state, action_prime)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def run(self):\n",
        "        #TODO\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb-upRCSLiQO"
      },
      "outputs": [],
      "source": [
        "class ExpectedSARSA(object):\n",
        "    def __init__(self, env, alpha, epsilon, gamma, timeout):\n",
        "        # define the parameters\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # environment\n",
        "        self.env = env\n",
        "\n",
        "        # define the Q value table\n",
        "        self.state_num = len(self.env.state_space)\n",
        "        self.action_num = len(self.env.action_space.keys())\n",
        "        self.Q = np.zeros((self.state_num, self.action_num))\n",
        "\n",
        "        # define the timeout\n",
        "        self.timeout = timeout\n",
        "\n",
        "    def behavior_policy(self, state):\n",
        "        #TODO\n",
        "        return None\n",
        "\n",
        "    def update(self, s, a, r, s_prime, a_prime):\n",
        "        #TODO\n",
        "        return None\n",
        "\n",
        "    def rollout(self):\n",
        "        pass\n",
        "\n",
        "    def run(self):\n",
        "        #TODO\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5FQi-erLiQO"
      },
      "outputs": [],
      "source": [
        "class QLearning(object):\n",
        "    def __init__(self, env, alpha, epsilon, gamma, timeout):\n",
        "        # define the parameters\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # environment\n",
        "        self.env = env\n",
        "\n",
        "        # define the Q value table\n",
        "        self.state_num = len(self.env.state_space)\n",
        "        self.action_num = len(self.env.action_space.keys())\n",
        "        self.Q = np.zeros((self.state_num, self.action_num))\n",
        "\n",
        "        # define the timeout\n",
        "        self.timeout = timeout\n",
        "\n",
        "    def behavior_policy(self, state):\n",
        "        #TODO\n",
        "        return None\n",
        "\n",
        "    def update(self, s, a, r, s_prime):\n",
        "        #TODO\n",
        "        return None\n",
        "\n",
        "    def rollout(self):\n",
        "        pass\n",
        "\n",
        "    def run(self):\n",
        "        #TODO\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rca2JVt3LiQP"
      },
      "outputs": [],
      "source": [
        "def plot_curves(arr_list, legend_list, color_list, ylabel):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        arr_list (list): list of results arrays to plot\n",
        "        legend_list (list): list of legends corresponding to each result array\n",
        "        color_list (list): list of color corresponding to each result array\n",
        "        ylabel (string): label of the Y axis\n",
        "\n",
        "    Make sure the elements in the arr_list, legend_list, and color_list are associated with each other correctly.\n",
        "    Do not forget to change the ylabel for different plots.\n",
        "    \"\"\"\n",
        "    # Clear the current figure\n",
        "    plt.clf()\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Set labels\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xlabel(\"Time Steps\")\n",
        "\n",
        "    # Plot results\n",
        "    h_list = []\n",
        "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
        "        # Compute the mean and standard error while ignoring NaN values\n",
        "        mean_arr = np.nanmean(arr, axis=0)\n",
        "        arr_err = np.nanstd(arr, axis=0) / np.sqrt(np.sum(~np.isnan(arr), axis=0))\n",
        "\n",
        "        # Plot the mean\n",
        "        h, = ax.plot(range(len(mean_arr)), mean_arr, color=color, label=legend)\n",
        "\n",
        "        # Plot the confidence band\n",
        "        arr_err = 1.96 * arr_err  # 95% confidence interval\n",
        "        ax.fill_between(range(len(mean_arr)),\n",
        "                        mean_arr - arr_err,\n",
        "                        mean_arr + arr_err,\n",
        "                        alpha=0.3, color=color)\n",
        "        # Save the plot handle\n",
        "        h_list.append(h)\n",
        "\n",
        "    # Set the title (adjust as needed)\n",
        "    ax.set_title(\"Windy Gridworld Results\")\n",
        "    ax.legend(handles=h_list)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj-_-y3aLiQP"
      },
      "outputs": [],
      "source": [
        "def run_on_policy_td_control(run_num, timeout):\n",
        "\n",
        "    enable_king_move_actions = False\n",
        "    enable_no_move_actions = False\n",
        "\n",
        "    # create the environment\n",
        "    env = WindyGridWorld(enable_king_move=enable_king_move_actions, enable_no_move=enable_no_move_actions)\n",
        "\n",
        "    # parameters\n",
        "    epsilon = 0.1\n",
        "    alpha = 0.5\n",
        "    gamma = 1.0\n",
        "\n",
        "    # create the expected SARSA\n",
        "    expected_sarsa_results_list = []\n",
        "    for _ in range(run_num):\n",
        "        # run for each trial\n",
        "        controller_expected_sarsa = ExpectedSARSA(env, alpha, epsilon, gamma, timeout)\n",
        "        episodes = controller_expected_sarsa.run()\n",
        "        # append the results\n",
        "        expected_sarsa_results_list.append(episodes[0:8000])\n",
        "\n",
        "    # create the SARSA\n",
        "    sarsa_results_list = []\n",
        "    for _ in range(run_num):\n",
        "        # run for each trial\n",
        "        controller_sarsa = SARSA(env, alpha, epsilon, gamma, timeout)\n",
        "        episodes = controller_sarsa.run()\n",
        "        # append the results\n",
        "        sarsa_results_list.append(episodes[0:8000])\n",
        "\n",
        "    # create the Q learning\n",
        "    q_learning_results_list = []\n",
        "    for _ in range(run_num):\n",
        "        # run for each trial\n",
        "        controller_q_learning = QLearning(env, alpha, epsilon, gamma, timeout)\n",
        "        episodes = controller_q_learning.run()\n",
        "        # append the results\n",
        "        q_learning_results_list.append(episodes[0:8000])\n",
        "\n",
        "    sarsa_array = np.array(sarsa_results_list)\n",
        "    expected_sarsa_array = np.array(expected_sarsa_results_list)\n",
        "    q_learning_array = np.array(q_learning_results_list)\n",
        "\n",
        "    # Plot the results\n",
        "    plot_curves(\n",
        "        [sarsa_array, expected_sarsa_array, q_learning_array],\n",
        "        ['SARSA', 'Expected SARSA', 'Q-learning'],\n",
        "        ['r', 'b', 'g'],\n",
        "        \"Episodes\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr_nSoBuLiQQ"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # set randomness\n",
        "    np.random.seed(1234)\n",
        "    random.seed(1234)\n",
        "\n",
        "    # trial number\n",
        "    trial_num = 10\n",
        "    # maximal time steps\n",
        "    max_time_steps = 8000\n",
        "\n",
        "\n",
        "    # run SARSA and Q Learning\n",
        "    run_on_policy_td_control(trial_num, max_time_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YAqiFRfLiQQ"
      },
      "source": [
        "# Question 5 [5180 ONLY]\n",
        "- Please refer to starter code from question 4 to help you get started. You will create your own TD(0) and Monte Carlo classes.\n",
        "- We will continue with the original windy gridworld domain.\n",
        "- A fixed policy π will be specified to collect episodes.\n",
        "- A certain number of “training” episodes N ∈ {1, 10, 50} will be collected.\n",
        "- Each method being investigated ( On-policy TD(0), On-policy Monte-Carlo prediction) will learn to      estimate the state-value.\n",
        "function using the N “training“ episodes, respectively.\n",
        "- We then evaluate the distribution of learning targets each method experiences at a specified state S. In\n",
        "this question, S is the initial state marked as S in the Example 6.5.\n",
        "- To do so, you need to collect additional 100 “evaluation” episodes. Instead of using these to perform\n",
        "further updates to the state-value function, we will instead evaluate the distribution of learning targets\n",
        "V(S) based on the “evaluation” episodes. For example, TD(0) will experience a set of {R+ V(S′)} targets,\n",
        "whereas Monte-Carlo will experience a set of {G} targets.\n",
        "- Note that in practice you should pre-collect both the training and evaluation episodes for efficiency and to\n",
        "ensure consistency while comparing between different methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-lq5dYQLiQQ"
      },
      "outputs": [],
      "source": [
        "def run_on_policy_mc_td_epsilon_greedy_windy_gridworld():\n",
        "    enable_king_move_actions = False\n",
        "    enable_no_move_actions = False\n",
        "\n",
        "    # create environments\n",
        "    env = WindyGridWorld(enable_king_move=enable_king_move_actions,\n",
        "                         enable_no_move=enable_no_move_actions)\n",
        "    env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_c1S9fvLiQR"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # set randomness\n",
        "    np.random.seed(1234)\n",
        "    random.seed(1234)\n",
        "\n",
        "    # run Monte Carlo and TD(0) for Question 6. Modify as necessary\n",
        "    run_on_policy_mc_td_epsilon_greedy_windy_gridworld()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}