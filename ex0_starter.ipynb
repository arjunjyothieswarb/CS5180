{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunjyothieswarb/CS5180/blob/main/ex0_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE55jRGV71qi"
      },
      "source": [
        "# Please install the following python libraries\n",
        "- python3: https://www.python.org/\n",
        "- numpy: https://numpy.org/install/\n",
        "- tqdm: https://github.com/tqdm/tqdm#installation\n",
        "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
        "- ipywidgets and ipykernel if necessary\n",
        "- Starter Code was tested on Python 3.11.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jVY8rzWc71qk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tqdm.notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnJ-Cn7T71qk"
      },
      "source": [
        "# 1: Complete the Implementation of the Four Rooms environment\n",
        "\n",
        "- The FourRooms is implemented as a python class. We explain the attributes and methods as follows\n",
        "    - **init** function: Define all the attributes of the Four Rooms environment. For example, the state space, the action space, the start state, the goal state and so on.\n",
        "    - **reset** function: Resets the agent to the start state (0, 0)\n",
        "    - **step** function: Takes the current state and one action, returns the next state and a reward\n",
        "   \n",
        "- Please complete the implementation in the step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZU8lh9un71qk"
      },
      "outputs": [],
      "source": [
        "# FOUR ROOM ENVIRONMENT\n",
        "class FourRooms(object):\n",
        "    def __init__(self):\n",
        "        # define the four room as a 2-D array for easy state space reference and visualization\n",
        "        # 0 represents an empty cell; 1 represents a wall cell\n",
        "        self.four_room_space = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                                         [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
        "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
        "\n",
        "        # find the positions for all empty cells\n",
        "        # not that: the origin for a 2-D numpy array is located at top-left while the origin for the FourRooms is at\n",
        "        # the bottom-left. The following codes performs the re-projection.\n",
        "        empty_cells = np.where(self.four_room_space == 0.0)\n",
        "        self.state_space = [[col, 10 - row] for row, col in zip(empty_cells[0], empty_cells[1])]\n",
        "\n",
        "        # define the action space\n",
        "        self.action_space = {'LEFT': np.array([-1, 0]),\n",
        "                             'RIGHT': np.array([1, 0]),\n",
        "                             'DOWN': np.array([0, -1]),\n",
        "                             'UP': np.array([0, 1])}\n",
        "\n",
        "        self.actions_possible = {\n",
        "            \"UP\": [[0,1],[-1,0],[1,0]],\n",
        "            \"DOWN\": [[0,-1],[-1,0],[1,0]],\n",
        "            \"LEFT\":[[-1,0],[0,1],[0,-1]],\n",
        "            \"RIGHT\":[[1,0],[0,1],[0,-1]]\n",
        "        }\n",
        "\n",
        "        # define the start state\n",
        "        self.start_state = [0, 0]\n",
        "\n",
        "        # define the goal state\n",
        "        self.goal_state = [10, 10]\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the agent's state to the start state [0, 0]\n",
        "        Return both the start state and reward\n",
        "        \"\"\"\n",
        "        state = self.start_state  # reset the agent to [0, 0]\n",
        "        reward = 0  # reward is 0\n",
        "        return state, reward\n",
        "\n",
        "\n",
        "    def step(self, state, act):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
        "            act: a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
        "        Output args:\n",
        "            next_state: a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
        "            reward: an integer. it can be either 0 or 1.\n",
        "        \"\"\"\n",
        "\n",
        "        # CODE HERE: implement the stochastic dynamics as described in Q1.\n",
        "        # Please note, we provide you with the deterministic transition function \"take_action\" below.\n",
        "        # Therefore, you only have to implement the logics of the stochasticity.\n",
        "        if(state[0] == 10 and state[1] == 10):\n",
        "            self.reset()\n",
        "\n",
        "        movement_stochastic = [0.8, 0.9, 1]\n",
        "\n",
        "        rand_val = random.random()\n",
        "\n",
        "        # Simulating noise\n",
        "        if(rand_val<=movement_stochastic[0]):\n",
        "            noise_selector = 0\n",
        "        elif(rand_val<=movement_stochastic[1]):\n",
        "            noise_selector = 1\n",
        "        else:\n",
        "            noise_selector = 2\n",
        "\n",
        "        # print(\"\\nNoise selected: \",noise_selector)\n",
        "\n",
        "        # Calculating the next state given the current state and stochastic action taken\n",
        "        est_next_state = [state[0] + self.actions_possible[act][noise_selector][0], state[1] + self.actions_possible[act][noise_selector][1]]\n",
        "\n",
        "        # Checking wether action is possible\n",
        "        if(est_next_state[0] > 10 or est_next_state[1] > 10 or est_next_state[0] < 0 or est_next_state[1] < 0):\n",
        "            next_state = state\n",
        "        elif(self.four_room_space[est_next_state[0], est_next_state[1]]):\n",
        "            next_state = state\n",
        "        else:\n",
        "            next_state = est_next_state\n",
        "\n",
        "\n",
        "        # CODE HERE: compute the reward based on the resulting state\n",
        "        if(next_state[0] == 10 and next_state[1] == 10):\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        # return the current state, reward\n",
        "        return next_state, reward\n",
        "\n",
        "\n",
        "    \"\"\" DO NOT CHANGE BELOW \"\"\"\n",
        "    def take_action(self, state, act):\n",
        "        \"\"\"\n",
        "        Input args:\n",
        "            state (list): a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
        "            act (string): a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
        "        Output args:\n",
        "            next_state (list): a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
        "        \"\"\"\n",
        "        state = np.array(state)\n",
        "        next_state = state + self.action_space[act]\n",
        "        return next_state.tolist() if next_state.tolist() in self.state_space else state.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-CptAPUt71ql"
      },
      "outputs": [],
      "source": [
        "\"\"\"Here is the plot function you can use to generate the figure. DO NOT CHANGE\"\"\"\n",
        "# PLOT FUNCTION\n",
        "def plot_func(res_list):\n",
        "    # set the figure size\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # plot each trial\n",
        "    for re in res_list:\n",
        "        plt.plot(list(range(len(res_list[0]))), re, linestyle=\"--\", linewidth=1, alpha=0.7)\n",
        "\n",
        "    # plot mean reward\n",
        "    mean_reward = np.array(res_list).mean(axis=0).tolist()\n",
        "    plt.plot(list(range(len(res_list[0]))), mean_reward, linestyle=\"-\", linewidth=2, color=\"k\")\n",
        "\n",
        "    # plot the figure\n",
        "    plt.ylabel(\"Cumulative reward\")\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaZNjtub71qm"
      },
      "source": [
        "# 2 Implement the manual policy\n",
        "\n",
        "Use this to check your whether your implementation of the step function is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIq-nKXa71qm",
        "outputId": "2713ff74-163f-46df-e327-3b7cc9ede51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step = 0, state = [0, 0], action = RIGHT, next state = [0, 0], reward = 0\n",
            "Step = 1, state = [0, 0], action = RIGHT, next state = [1, 0], reward = 0\n",
            "Step = 2, state = [1, 0], action = RIGHT, next state = [2, 0], reward = 0\n",
            "Step = 3, state = [2, 0], action = RIGHT, next state = [2, 0], reward = 0\n",
            "Step = 4, state = [2, 0], action = RIGHT, next state = [2, 0], reward = 0\n",
            "Step = 5, state = [2, 0], action = RIGHT, next state = [3, 0], reward = 0\n",
            "Step = 6, state = [3, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 7, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 8, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 9, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 10, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 11, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 12, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 13, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 14, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 15, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 16, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 17, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 18, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 19, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 20, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 21, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 22, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 23, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 24, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 25, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 26, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 27, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 28, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 29, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 30, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 31, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 32, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 33, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 34, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 35, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 36, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 37, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 38, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 39, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 40, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 41, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 42, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 43, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 44, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 45, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 46, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 47, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 48, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 49, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 50, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 51, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 52, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 53, state = [4, 0], action = RIGHT, next state = [4, 0], reward = 0\n",
            "Step = 54, state = [4, 0], action = RIGHT, next state = [4, 1], reward = 0\n",
            "Step = 55, state = [4, 1], action = RIGHT, next state = [5, 1], reward = 0\n",
            "Step = 56, state = [5, 1], action = RIGHT, next state = [6, 1], reward = 0\n",
            "Step = 57, state = [6, 1], action = RIGHT, next state = [6, 0], reward = 0\n",
            "Step = 58, state = [6, 0], action = RIGHT, next state = [7, 0], reward = 0\n",
            "Step = 59, state = [7, 0], action = RIGHT, next state = [7, 1], reward = 0\n",
            "Step = 60, state = [7, 1], action = RIGHT, next state = [8, 1], reward = 0\n",
            "Step = 61, state = [8, 1], action = RIGHT, next state = [9, 1], reward = 0\n",
            "Step = 62, state = [9, 1], action = RIGHT, next state = [10, 1], reward = 0\n",
            "Step = 63, state = [10, 1], action = RIGHT, next state = [10, 1], reward = 0\n",
            "Step = 64, state = [10, 1], action = RIGHT, next state = [10, 2], reward = 0\n",
            "Step = 65, state = [10, 2], action = RIGHT, next state = [10, 2], reward = 0\n",
            "Step = 66, state = [10, 2], action = RIGHT, next state = [10, 2], reward = 0\n",
            "Step = 67, state = [10, 2], action = RIGHT, next state = [10, 2], reward = 0\n",
            "Step = 68, state = [10, 2], action = RIGHT, next state = [10, 3], reward = 0\n",
            "Step = 69, state = [10, 3], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 70, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 71, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 72, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 73, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 74, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 75, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 76, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 77, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 78, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 79, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 80, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 81, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 82, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 83, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 84, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 85, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 86, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 87, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 88, state = [10, 4], action = RIGHT, next state = [10, 3], reward = 0\n",
            "Step = 89, state = [10, 3], action = RIGHT, next state = [10, 3], reward = 0\n",
            "Step = 90, state = [10, 3], action = RIGHT, next state = [10, 3], reward = 0\n",
            "Step = 91, state = [10, 3], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 92, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 93, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 94, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 95, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 96, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 97, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 98, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n",
            "Step = 99, state = [10, 4], action = RIGHT, next state = [10, 4], reward = 0\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # fix the randomness for reproduction\n",
        "    random.seed(1234)\n",
        "    np.random.seed(1234)\n",
        "\n",
        "    # create the environment\n",
        "    env = FourRooms()\n",
        "    state, reward = env.reset()  # always call reset() before interaction\n",
        "\n",
        "    # manual time step (YOU CAN CHANGE THIS TO ANY TIME STEP YOU WANT)\n",
        "    time_step = 100\n",
        "\n",
        "    # create a loop\n",
        "    for t in range(time_step):\n",
        "\n",
        "        # CODE HERE: implement your manual agent/policy function that takes in the action from the standard input\n",
        "        action = \"RIGHT\"\n",
        "        # action = None\n",
        "\n",
        "        # CODE HERE: implement the code to interact with the Four Rooms environment above.\n",
        "        # it should takes in the current state and action and returns the next_state and a reward\n",
        "        # Hint: use the step function that you implement.\n",
        "        next_state, reward = env.step(state,action)\n",
        "        # next_state, reward = None, None\n",
        "\n",
        "        \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
        "        # print interaction\n",
        "        print(f\"Step = {t}, state = {state}, action = {action}, next state = {next_state}, reward = {reward}\")\n",
        "\n",
        "        # reset if the agent reaches the goal\n",
        "        if reward == 1:\n",
        "            print(\"Reset the agent to the start state!\")\n",
        "            state, reward = env.reset()\n",
        "        else:\n",
        "            state = next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqxHHdLL71qn"
      },
      "source": [
        "# 3 Implement a random policy\n",
        "\n",
        "We provide the scaffolding code for running and plotting. Please implement a random policy\n",
        "\n",
        "**Please note: you should read the code carefully before implementing to make sure the variable names are aligned.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "l1UfWZ6d71qn"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # fix the randomness for reproduction\n",
        "    random.seed(1234)\n",
        "    np.random.seed(1234)\n",
        "\n",
        "    # create the environment\n",
        "    env = FourRooms()\n",
        "\n",
        "    # number of the trail (YOU CAN MODIFIED HERE WITH SMALL VALUES FOR DEBUG ONLY)\n",
        "    trial_num = 10\n",
        "    # length of each trail (YOU CAN MODIFIED HERE WITH SMALL VALUES FOR DEBUG ONLY)\n",
        "    trial_length = int(1e4)\n",
        "\n",
        "    # save the rewards for plot\n",
        "    rewards_list = []\n",
        "\n",
        "    # run experiment\n",
        "    for e in tqdm(range(trial_num), desc=\"Run trial\",position=0):\n",
        "\n",
        "        # reset for every trail\n",
        "        reward_per_trial = []\n",
        "        reward_counter = 0\n",
        "\n",
        "        # reset the environment\n",
        "        state, reward = env.reset()\n",
        "\n",
        "        # run each trial\n",
        "        for t in tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
        "\n",
        "\n",
        "            # CODE HERE: please implement a random policy to obtain an action.\n",
        "            # it should return a random action from [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
        "            action = None\n",
        "\n",
        "            # CODE HERE: please implement the code to get the next state and reward\n",
        "            # it should takes in the current state and action\n",
        "            # it should returns the next_state and reward\n",
        "            next_state, reward = None, None\n",
        "\n",
        "\n",
        "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
        "            # save the reward\n",
        "            reward_counter += reward\n",
        "            reward_per_trial.append(reward_counter)\n",
        "\n",
        "            # reset\n",
        "            if reward == 1:\n",
        "                state, reward = env.reset()\n",
        "            else:\n",
        "                state = next_state\n",
        "\n",
        "        # save the rewards\n",
        "        rewards_list.append(reward_per_trial)\n",
        "\n",
        "# PLOT THE RESULTS\n",
        "plot_func(rewards_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSe2WsAX71qo"
      },
      "source": [
        "# 4 Implement better & worse policies against the Random Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRdeTSFg71qo"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # fix the randomness for reproduction\n",
        "    random.seed(1234)\n",
        "    np.random.seed(1234)\n",
        "\n",
        "    # create the environment\n",
        "    env = FourRooms()\n",
        "\n",
        "    # number of the trail\n",
        "    trial_num = 10\n",
        "    # length of each trail\n",
        "    trial_length = int(1e4)\n",
        "\n",
        "    # save the rewards for plot\n",
        "    rewards_list = []\n",
        "\n",
        "    # run experiment\n",
        "    for e in tqdm(range(trial_num), desc=\"Run trial\",position=0):\n",
        "\n",
        "        # reset for every trail\n",
        "        reward_per_trial = []\n",
        "        reward_counter = 0\n",
        "\n",
        "        # reset the environment\n",
        "        state, reward = env.reset()\n",
        "\n",
        "        # run each trial\n",
        "        for t in tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
        "\n",
        "\n",
        "            # CODE HERE: please implement a policy that is worse than the random policy.\n",
        "            # It should takes in the current state and output an action\n",
        "            action = None\n",
        "\n",
        "            # CODE HERE: please implement the code to get the next state and reward\n",
        "            next_state, reward = None, None\n",
        "\n",
        "\n",
        "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
        "            # save the reward\n",
        "            reward_counter += reward\n",
        "            reward_per_trial.append(reward_counter)\n",
        "\n",
        "            # reset\n",
        "            if reward == 1:\n",
        "                state, reward = env.reset()\n",
        "            else:\n",
        "                state = next_state\n",
        "\n",
        "        # save the rewards\n",
        "        rewards_list.append(reward_per_trial)\n",
        "\n",
        "# PLOT THE RESULTS\n",
        "plot_func(rewards_list)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}