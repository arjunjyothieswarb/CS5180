{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunjyothieswarb/CS5180/blob/main/Assignment6/ex6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "F0rTlgRsvJKC"
      },
      "source": [
        "# Please install the following python libraries\n",
        "- python3: https://www.python.org/\n",
        "- numpy: https://numpy.org/install/\n",
        "- tqdm: https://github.com/tqdm/tqdm#installation\n",
        "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
        "\n",
        "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
        "    \n",
        "Please install the ipywidgets as follows:\n",
        "\n",
        "    with pip, do\n",
        "    - pip install ipywidgets\n",
        "    \n",
        "    with conda, do\n",
        "    - conda install -c conda-forge ipywidgets\n",
        "    \n",
        "Restart your notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_S3SpThIvJKD"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP97gD9KvJKE"
      },
      "source": [
        "# Pre-implemented environments\n",
        "We provide you the implementation of all the environments that you would use to finish this assignment. They are:\n",
        "- **Blocking Maze**: Please read the Example 8.2 (on page 166 in RL2e) for more details\n",
        "- **Shortcut Maze**: Please read the Example 8.3 (on page 167 in RL2e) for more details\n",
        "- **Stochastic Windy GridWorld**: Please read the Example 6.5 (on page 130 in RL2e) for more details\n",
        "\n",
        "For all the environments, in general, they consist of:\n",
        "- A **grid** that represents the domain space. In other words, the state space contains the coordinates (e.g., [x, y]) of all empty cells that are accessible by the agent.\n",
        "- A discrete **action space** that contains 4 actions: up, down, left and right. Specifically, the **step** function in each implementation takes a **string variable** that contains the name of the action.\n",
        "- All the tasks are episodic task. But, please read the description carefully in the book. Because in some task, a new episode will start only when the agent reaches the goal location. In other words, they might not have a timeout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_5qoSRvJKE"
      },
      "source": [
        "# Blocking Maze\n",
        "In **Blocking Maze**, the agent starts the experiment by interacting with the maze on the left. Then, at time step = 1000, the environment is switched to the maze on the right. For each episode, the agent always starts at the start location (marked as S) and is asked to navigate to the goal location (marked as G). The agent receives reward = 1 when it reaches the goal and receives 0 otherwise.\n",
        "\n",
        "Before you use this implementation, please read the code and make sure you understand it. You are also welcome to implement your own version. If you do that, please replace the following block with your own implementation and add reasonable comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LG-D5vf5vJKE"
      },
      "outputs": [],
      "source": [
        "class BlockingMaze(object):\n",
        "    \"\"\" Implementation of the Blocking Maze \"\"\"\n",
        "    def __init__(self):\n",
        "        # We define the grid for the maze on the left in Example 8.2\n",
        "        self.left_grid = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "        # We define the grid for the maze on the right in Example 8.2\n",
        "        self.right_grid = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "        # We initialize the grid with using the left maze first\n",
        "        self.grid = self.left_grid.copy()\n",
        "        # Save the size of the maze\n",
        "        self.grid_height, self.grid_width = self.grid.shape\n",
        "\n",
        "        # We define the observation space using all empty cells.\n",
        "        # We consider all cells. Because the environment switch will results in change of the state space\n",
        "        self.observation_space = np.argwhere(np.zeros((self.grid_height, self.grid_width)) == 0.0).tolist()\n",
        "\n",
        "        # We define the action space\n",
        "        self.action_space = {\n",
        "            \"up\": np.array([-1, 0]),\n",
        "            \"down\": np.array([1, 0]),\n",
        "            \"left\": np.array([0, -1]),\n",
        "            \"right\": np.array([0, 1])\n",
        "        }\n",
        "        self.action_names = [\"up\", \"down\", \"left\", \"right\"]\n",
        "\n",
        "        # We define the start state\n",
        "        self.start_location = [5, 3]\n",
        "\n",
        "        # We define the goal state\n",
        "        self.goal_location = [0, 8]\n",
        "\n",
        "        # We find all wall locations in the grid\n",
        "        self.walls = np.argwhere(self.grid == 1.0).tolist()\n",
        "\n",
        "        # We define other useful variables\n",
        "        self.agent_location = None\n",
        "        self.action = None\n",
        "\n",
        "    def reset(self, shift_maze=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            shift_maze (string): if None, reset function simply set the agent back to the start location\n",
        "                                 if left, reset function will switch to the maze on the left\n",
        "                                 if right, reset function will switch to the maze on the right\n",
        "        \"\"\"\n",
        "        if shift_maze is not None:\n",
        "            if shift_maze == \"left\":\n",
        "                self.grid = self.left_grid.copy()\n",
        "            elif shift_maze == \"right\":\n",
        "                self.grid = self.right_grid.copy()\n",
        "            else:\n",
        "                raise Exception(\"Invalid shift operation\")\n",
        "\n",
        "            # reset the shape\n",
        "            self.grid_height, self.grid_width = self.grid.shape\n",
        "\n",
        "            # reset the walls\n",
        "            self.walls = np.argwhere(self.grid == 1.0).tolist()\n",
        "\n",
        "        # We reset the agent location to the start state\n",
        "        self.agent_location = self.start_location\n",
        "\n",
        "        # We set the information\n",
        "        info = {}\n",
        "        return self.agent_location, info\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action (string): name of the action (e.g., \"up\"/\"down\"/\"left\"/\"right\")\n",
        "        \"\"\"\n",
        "        # Convert the agent's location to array\n",
        "        loc_arr = np.array(self.agent_location)\n",
        "\n",
        "        # Convert the abstract action to movement array\n",
        "        act_arr = self.action_space[action]\n",
        "\n",
        "        # Compute the next location\n",
        "        next_agent_location = np.clip(loc_arr + act_arr,\n",
        "                                      a_min=np.array([0, 0]),\n",
        "                                      a_max=np.array([self.grid_height - 1, self.grid_width - 1])).tolist()\n",
        "\n",
        "        # Check if it crashes into a wall\n",
        "        if next_agent_location in self.walls:\n",
        "            # If True, the agent keeps in the original state\n",
        "            next_agent_location = self.agent_location\n",
        "\n",
        "        # Compute the reward\n",
        "        reward = 1 if next_agent_location == self.goal_location else 0\n",
        "\n",
        "        # Check the termination\n",
        "        terminated = True if reward == 1 else False\n",
        "\n",
        "        # Update the action location\n",
        "        self.agent_location = next_agent_location\n",
        "        self.action = action\n",
        "\n",
        "        return next_agent_location, reward, terminated, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        # plot the agent and the goal\n",
        "        # agent = 1\n",
        "        # goal = 2\n",
        "        plot_arr = self.grid.copy()\n",
        "        plot_arr[self.agent_location[0], self.agent_location[1]] = 2\n",
        "        plot_arr[self.goal_location[0], self.goal_location[1]] = 3\n",
        "        plt.clf()\n",
        "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
        "        plt.imshow(plot_arr)\n",
        "        plt.show(block=False)\n",
        "        plt.pause(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqqTYEOavJKE"
      },
      "source": [
        "# Shortcut Maze\n",
        "In **Shortcut Maze**, the agent starts the experiment by interacting with the maze on the left. Then, at time step = 3000, the environment is switched to the maze on the right. For each episode, the agent always starts at the start location (marked as S) and is asked to navigate to the goal location (marked as G). The agent receives reward = 1 when it reaches the goal and receives 0 otherwise.\n",
        "\n",
        "Before you use this implementation, please read the code and make sure you understand it. You are also welcome to implement your own version. If you do that, please replace the following block with your own implementation and add reasonable comments\n",
        "\n",
        "Since **Shortcut Maze** is different with **Blocking Maze** in terms of the maze grids. We will inherit **Blocking Maze** and only change the followings:\n",
        "1. Reset the grid of the left maze\n",
        "2. Reset the grid of the right maze\n",
        "3. Reset the initial maze to be the left one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HalGWifFvJKF"
      },
      "outputs": [],
      "source": [
        "class ShortcutMaze(BlockingMaze):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # We define the grid for the maze on the left in Example 8.3\n",
        "        self.left_grid = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                   [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "        # We define the grid for the maze on the right in Example 8.3\n",
        "        self.right_grid = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "        # We reset the domain to be the left maze first\n",
        "        self.reset(shift_maze=\"left\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjRUqVgUvJKF"
      },
      "source": [
        "# The plot function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oicXhrA9vJKF"
      },
      "outputs": [],
      "source": [
        "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        arr_list (list): list of results arrays to plot\n",
        "        legend_list (list): list of legends corresponding to each result array\n",
        "        color_list (list): list of color corresponding to each result array\n",
        "        ylabel (string): label of the Y axis\n",
        "\n",
        "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
        "        Do not forget to change the ylabel for different plots.\n",
        "    \"\"\"\n",
        "    # set the figure type\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # PLEASE NOTE: Change the labels for different plots\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xlabel(\"Time Steps\")\n",
        "\n",
        "    # ploth results\n",
        "    h_list = []\n",
        "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
        "        # compute the standard error\n",
        "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
        "        # plot the mean\n",
        "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
        "        # plot the confidence band\n",
        "        arr_err *= 1.96\n",
        "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
        "                        color=color)\n",
        "        # save the plot handle\n",
        "        h_list.append(h)\n",
        "\n",
        "    # plot legends\n",
        "    ax.set_title(f\"{fig_title}\")\n",
        "    ax.legend(handles=h_list)\n",
        "\n",
        "    # save the figure\n",
        "    plt.savefig(f\"{fig_title}.png\", dpi=200)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ffA9fUvJKF"
      },
      "source": [
        "# Q3: (a) - Implement Dyna-Q and Dyna-Q+ to reproduce Figure 8.4\n",
        "\n",
        "- Use the **BlockingMaze** environment;\n",
        "- Reproduce the Figure 8.4 on page 167\n",
        "    - X axis: the learning time step. In other words, it is the number of time steps that agent really interacts with the environment;\n",
        "    - Y axis: the cumulative reward, a sum of all rewards received by the agent by a particular time step t.\n",
        "- Hyperparameters:\n",
        "    - Dyna-Q:\n",
        "        - learning time steps: 3000\n",
        "        - planning steps number: 250\n",
        "        - alpha: 0.1\n",
        "        - gamma: 0.95\n",
        "        - epsilon: 0.1\n",
        "        - time to switch the maze: 1000\n",
        "    - Dyna-Q+:\n",
        "        - learning time steps: 3000\n",
        "        - planning steps number: 250\n",
        "        - alpha: 0.1\n",
        "        - gamma: 0.95\n",
        "        - epsilon: 0.1\n",
        "        - kappa: 5e-4\n",
        "        - time to switch the maze: 1000\n",
        "        \n",
        "Note that, to implement the Dyna-Q+, please read the section 8.3 and the footnote on page 168 carefully. For Dyna-Q+, we ask you to implement **two variants**. For the first variant, it does **not** use the strategy in the footnote. For the second, it use the strategy proposed in the footnote. Finally, besides the two lines in Figure 8.4, please add a third line for Dyna-Q+ without using the footnote strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw7NRftxvJKF"
      },
      "outputs": [],
      "source": [
        "\"\"\" We will implement the Dyna-Q as a class that contains the following attributes:\n",
        "        - self.env: an environment object\n",
        "        - self.q_table: a 2-D array with shape (num_state, num_action) that stores the Q values\n",
        "        - self.model: a nested dictionary that stores the past experiences\n",
        "    Note that:\n",
        "        There exists one attribute \"self.maze_switch_time\". It indicates the time to switch from the left maze\n",
        "        to the right maze during the learning process.\n",
        "\n",
        "    Please complete this class by adding your implementation in the following functions:\n",
        "        - self.update_model\n",
        "        - self.update_q_table\n",
        "        - self.run_planning_step\n",
        "        - self.run\n",
        "\n",
        "\"\"\"\n",
        "class TabularDynaQAgent(object):\n",
        "    def __init__(self, env, info):\n",
        "        # Store the environment\n",
        "        self.env = env\n",
        "\n",
        "        # Store the maximal learning steps\n",
        "        self.k = info['learning_step']\n",
        "\n",
        "        # Store the planning steps\n",
        "        self.n = info['planning_step']\n",
        "\n",
        "        # Store the Q-learning step size alpha\n",
        "        self.alpha = info['alpha']\n",
        "\n",
        "        # Store the discount factor\n",
        "        self.gamma = info['gamma']\n",
        "\n",
        "        # Initialize the epsilon for epsilon-greedy policy\n",
        "        self.epsilon = info['epsilon']\n",
        "\n",
        "        # Initialize the switch time\n",
        "        self.maze_switch_time = info['switch_time_step']\n",
        "\n",
        "        # Compute total number of the state\n",
        "        self.state_num = len(self.env.observation_space)\n",
        "\n",
        "        # Compute the number of the actions\n",
        "        self.action_num = len(self.env.action_names)\n",
        "\n",
        "        # Initialize the Q(s, a): a table stores (s, a)\n",
        "        self.q_table = np.zeros((self.state_num, self.action_num))\n",
        "\n",
        "        # Initialize the model\n",
        "        # We represent the model as a dictionary of dictionary.\n",
        "        # Specifically, the keys for the outside dictionary is a state tuple (the key should be immutable).\n",
        "        # Given one state tuple key, its value is also a dictionary whose keys are actions names and values are a tuple consisting of (reward, next_state).\n",
        "        # Given an empty model and a transition (s, a, r, s'), the self.model can be updated as\n",
        "        # {s: {a: (r, s')}}.\n",
        "        # E.g. {(0, 0): {'right': (0, [0, 2]), 'left': (0, [0, 0]), ...}, ...}\n",
        "        # We usually call (s, a, r, s') a transition tuple\n",
        "        self.model = {}\n",
        "\n",
        "    def _get_state_idx(self, state):\n",
        "        \"\"\"\n",
        "        Function returns the index of the state in the q table (i.e., row index)\n",
        "        Args:\n",
        "             state (tuple): a tuple contains the state\n",
        "        \"\"\"\n",
        "        state = list(state)\n",
        "        return self.env.observation_space.index(state)\n",
        "\n",
        "    def _get_action_idx(self, action):\n",
        "        \"\"\"\n",
        "        Function returns the index of the action in the q table (i.e., column index)\n",
        "        Args:\n",
        "            action (string): action name\n",
        "        \"\"\"\n",
        "        return self.env.action_names.index(action)\n",
        "\n",
        "    def epsilon_greedy_policy(self, state):\n",
        "        \"\"\"\n",
        "        Function implements the epsilon-greedy policy\n",
        "        Args:\n",
        "            state (tuple): a tuple contains the state\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:  # with p = epsilon, we randomly sample an action\n",
        "            return random.sample(self.env.action_names, 1)[0]\n",
        "        else:  # With p = 1 - epsilon, derive a greedy policy from the Q table\n",
        "            state_idx = self._get_state_idx(state)\n",
        "            q_values = self.q_table[state_idx]\n",
        "            max_action_list = np.where(q_values == q_values.max())[0].tolist()\n",
        "            max_action_idx = random.sample(max_action_list, 1)[0]\n",
        "            return self.env.action_names[max_action_idx]  # break ties\n",
        "\n",
        "    def update_model(self, s, a, r, next_s):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (list): contains the location of the agent\n",
        "            a (string): name of the action\n",
        "            r (int): reward\n",
        "            next_s (list): contains the location of the agent after taking the action\n",
        "        \"\"\"\n",
        "        \"\"\" CODE HERE: adding transition to the model \"\"\"\n",
        "        # check whether tuple s is a key in the dictionary\n",
        "        model_keys = list(self.model.keys())\n",
        "        if tuple(s) in model_keys:\n",
        "            pass\n",
        "\n",
        "        # update the model by adding the transition as \"tuple s\": {a: (r, next_s)}\n",
        "        self.model[tuple(s)] = {a: (r, next_s)}\n",
        "\n",
        "\n",
        "    def update_q_table(self, s, a, r, next_s):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (list): contains the location of the agent\n",
        "            a (string): name of the action\n",
        "            r (int): reward\n",
        "            next_s (list): contains the location of the agent after taking the action\n",
        "        \"\"\"\n",
        "        \"\"\" CODE HERE: Q-learning \"\"\"\n",
        "        # obtain the table index for s and next_s\n",
        "\n",
        "\n",
        "        # obtain the table index for a\n",
        "\n",
        "        # update the Q table\n",
        "\n",
        "    def run_planning_step(self):\n",
        "        \"\"\" CODE HERE: planning step see (f) in the pseudocode \"\"\"\n",
        "        for _ in range(self.n):\n",
        "            # randomly sample a previously observe state, you can sample it from the self.model.keys()\n",
        "            state = None\n",
        "\n",
        "            # randomly sample an action previously taken in state, you can sample it from the self.model[state].keys()\n",
        "            action = None\n",
        "\n",
        "            # render the reward and the next state given the state and action use the model\n",
        "            reward, next_state = None, None\n",
        "\n",
        "            # update the Q table using Q-learning\n",
        "            self.update_q_table(state, action, reward, next_state)\n",
        "\n",
        "    def run(self):\n",
        "        # record the reward\n",
        "        cumulative_reward = 0\n",
        "\n",
        "        # save the cumulative reward w.r.t. each time step\n",
        "        cumulative_rewards_wrt_time_steps = np.zeros(self.k)\n",
        "\n",
        "        # Dyna-Q starts\n",
        "        state, _ = self.env.reset()\n",
        "        for t in range(self.k):\n",
        "            \"\"\" CODE HERE: finish the Dyna-Q learning, you should use state, action, reward, next_state, done\n",
        "                as variables names to avoid bugs.\n",
        "            \"\"\"\n",
        "            # derive an action with epsilon-greedy policy\n",
        "\n",
        "            # take the action and observe reward and next state\n",
        "\n",
        "            # update the Q table using Q-learning\n",
        "\n",
        "            # update model\n",
        "\n",
        "            # planning starts\n",
        "\n",
        "            # uncomment print info for debug\n",
        "            # print(f\"t = {t}, s = {state}, a = {action}, r = {reward}, next_s = {next_state}, done = {done}\")\n",
        "\n",
        "            \"\"\" SAVE PLOTTING RESULTS DO NOT CHANGE \"\"\"\n",
        "            # store the cumulative rewards\n",
        "            cumulative_reward += reward\n",
        "            cumulative_rewards_wrt_time_steps[t] = cumulative_reward\n",
        "\n",
        "            # reset the environment\n",
        "            if t == self.maze_switch_time:\n",
        "                state, _ = self.env.reset(shift_maze=\"right\")\n",
        "            else:\n",
        "                if done:\n",
        "                    state, _ = self.env.reset()\n",
        "                else:\n",
        "                    state = next_state\n",
        "\n",
        "        return cumulative_rewards_wrt_time_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWAQ9RzJvJKF"
      },
      "outputs": [],
      "source": [
        "\"\"\" Here is the implementation of Dyna-Q+ without the footnote strategy.\n",
        "    We will inherit from the Dyna-Q above.\n",
        "\n",
        "    Recall, to compute the \"bonus reward\" = r + kappa * sqrt(elapsed time) for each planning step,\n",
        "    We have to store (s, a, r, s', t) in Dyna-Q+ instead of (s, a, r, s') in Dyna-Q in the model.\n",
        "    Therefore, since we have to store an extra t, we only have to change three methods:\n",
        "        - self.run: Because we use it to collect (s, a, r, s', t)\n",
        "        - self.update_model: Because we use it to add transitions to the model.\n",
        "        - self.run_planning_step: Because we use it to compute the \"bonus reward\" = r + kappa * sqrt(time elasped)\n",
        "\"\"\"\n",
        "class TabularDynaQPlusAgent(TabularDynaQAgent):\n",
        "    def __init__(self, env, info):\n",
        "        super().__init__(env, info)\n",
        "\n",
        "        # add one parameter \"kappa\" to compute the bonus reward defined as: r + kappa * sqrt(elapsed time steps)\n",
        "        self.kappa = info['kappa']\n",
        "\n",
        "    def update_model(self, s, a, r, next_s, last_t):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (list): contains the location of the agent\n",
        "            a (string): name of the action\n",
        "            r (int): reward\n",
        "            next_s (list): contains the location of the agent after taking the action\n",
        "            last_t (int): the real time step that the transition is encountered\n",
        "        \"\"\"\n",
        "        \"\"\" CODE HERE \"\"\"\n",
        "        # similar to the previous update. This time add last_t as one component\n",
        "\n",
        "\n",
        "    def run_planning_step(self, current_t):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            current_t (int): current time step in the learning process. We use it as well as the time recorded in\n",
        "            each transition to compute the elasped time.\n",
        "        \"\"\"\n",
        "        for _ in range(self.n):\n",
        "            \"\"\" CODE HERE: Dyna-Q with bonus reward no footnote \"\"\"\n",
        "            # randomly sample a previously observe state\n",
        "\n",
        "            # randomly sample an action previously taken in state\n",
        "\n",
        "            # render the reward and the next state given the state and action use the model\n",
        "\n",
        "            # compute the \"bonus\" reward = r + kappa * sqrt(time elasped)\n",
        "\n",
        "            # update the Q table using Q-learning\n",
        "\n",
        "    def run(self):\n",
        "        # record the reward\n",
        "        cumulative_reward = 0\n",
        "\n",
        "        # save the cumulative reward w.r.t. each time step\n",
        "        cumulative_rewards_wrt_time_steps = np.zeros(self.k)\n",
        "\n",
        "        # Dyna-Q starts\n",
        "        state, _ = self.env.reset()\n",
        "        for t in range(self.k):\n",
        "            \"\"\" CODE HERE: finish the code for Dyna-Q+\n",
        "                Please use state, action, reward, next_state, done as variable names to avoid bugs.\n",
        "            \"\"\"\n",
        "            # derive an action with epsilon-greedy policy\n",
        "\n",
        "            # take the action and observe reward and next state\n",
        "\n",
        "            # update the Q table using Q-learning\n",
        "\n",
        "            # update model\n",
        "\n",
        "            # planning step\n",
        "\n",
        "            # uncomment print info for debug\n",
        "            # print(f\"t = {t}, s = {state}, a = {action}, r = {reward}, next_s = {next_state}, done = {done}\")\n",
        "\n",
        "            \"\"\" SAVE PLOTTING RESULTS DO NOT CHANGE \"\"\"\n",
        "            # store the cumulative rewards\n",
        "            cumulative_reward += reward\n",
        "            cumulative_rewards_wrt_time_steps[t] = cumulative_reward\n",
        "\n",
        "            # reset the environment\n",
        "            if t == self.maze_switch_time:\n",
        "                state, _ = self.env.reset(shift_maze=\"right\")\n",
        "            else:\n",
        "                if done:\n",
        "                    state, _ = self.env.reset()\n",
        "                else:\n",
        "                    state = next_state\n",
        "\n",
        "        return cumulative_rewards_wrt_time_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwrhXabKvJKG"
      },
      "outputs": [],
      "source": [
        "\"\"\" Here is the implementation of Dyna-Q+ with the footnote strategy.\n",
        "    We will inherit from the Dyna-Q+ without footnote above.\n",
        "\n",
        "    Recall, the only difference between the two variants of Dyna-Q+ is whether we consider untried actions\n",
        "    from a state during the planning stage. Therefore, based on the footnote, we only have to change one method:\n",
        "        - self.run_planning_step:\n",
        "            - We will compute the \"bonus reward\" = r + kappa * sqrt(time elasped)\n",
        "            - We will try untried actions from state (See footnote on p.168 for more details)\n",
        "\"\"\"\n",
        "class TabularDynaQPlusAgentFontNote(TabularDynaQPlusAgent):\n",
        "    def __init__(self, env, info):\n",
        "        super().__init__(env, info)\n",
        "\n",
        "    def run_planning_step(self, current_t):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            current_t (int): current time step in the learning process. We use it as well as the time recorded in\n",
        "            each transition to compute the elasped time.\n",
        "        \"\"\"\n",
        "        for _ in range(self.n):\n",
        "            \"\"\" CODE HERE: Dyna-Q+ with footnote + bonus reward \"\"\"\n",
        "            # randomly sample a previously observe state\n",
        "\n",
        "            # randomly sample an action (all actions in the action space\n",
        "            # are valid from one state based on the footnote)\n",
        "\n",
        "            # check if the action is tried before\n",
        "                # If True, render the reward and the next state given the state and action use the model\n",
        "                # If False, set values for the reward, next_state, and t suggested by the footnote\n",
        "\n",
        "            # compute the \"bonus\" reward\n",
        "\n",
        "            # update the Q table using Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niqJ4kwIvJKG"
      },
      "source": [
        "# Run experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN1SAbh8vJKG"
      },
      "outputs": [],
      "source": [
        "\"\"\" Here is the code to reproduce the Figure 8.4. It takes on average around 1 min (Macbook Pro M1)\n",
        "    to run one method using the following parameters for experiments:\n",
        "        - learning step = 3000\n",
        "        - planning step = 250\n",
        "        - alpha (Q-learning update step) = 0.1\n",
        "        - gamma (discount factor) = 0.95\n",
        "        - epsilon (epsilon-greedy) = 0.1\n",
        "        - kappa (\"bonus reward\") = 5e-4\n",
        "\n",
        "    We run 10 trials and plot the results.\n",
        "\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "    # set the random seed\n",
        "    np.random.seed(1234)\n",
        "    random.seed(1234)\n",
        "\n",
        "    # set hyper-parameters\n",
        "    params = {\n",
        "        \"switch_time_step\": 1000,\n",
        "        \"learning_step\": 3000,\n",
        "        \"planning_step\": 250,\n",
        "        \"alpha\": 0.1,\n",
        "        \"gamma\": 0.95,\n",
        "        \"epsilon\": 0.1,\n",
        "        \"kappa\": 5e-4\n",
        "    }\n",
        "\n",
        "    \"\"\" Set running trials for experiments\n",
        "    \"\"\"\n",
        "    run_trial_num = 10\n",
        "\n",
        "    \"\"\" Run Dyna-Q\n",
        "    \"\"\"\n",
        "    results_dyna_q = []\n",
        "    for r in tqdm.trange(run_trial_num):\n",
        "        # create the environment\n",
        "        my_env = BlockingMaze()\n",
        "\n",
        "        # create the agent for Dyna-Q\n",
        "        dyna_q_agent = TabularDynaQAgent(my_env, params)\n",
        "        result = dyna_q_agent.run()\n",
        "\n",
        "        results_dyna_q.append(result)\n",
        "\n",
        "    \"\"\" Run Dyna-Q+ without the footnote\n",
        "    \"\"\"\n",
        "    results_dyna_q_plus = []\n",
        "    for r in tqdm.trange(run_trial_num):\n",
        "        # create the environment\n",
        "        my_env = BlockingMaze()\n",
        "\n",
        "        # create the agent for Dyna-Q\n",
        "        dyna_q_plus_agent = TabularDynaQPlusAgent(my_env, params)\n",
        "        result = dyna_q_plus_agent.run()\n",
        "\n",
        "        results_dyna_q_plus.append(result)\n",
        "\n",
        "    \"\"\" Run Dyna-Q+ with the footnote\n",
        "    \"\"\"\n",
        "    results_dyna_q_plus_fn = []\n",
        "    for r in tqdm.trange(run_trial_num):\n",
        "        # create the environment\n",
        "        my_env = BlockingMaze()\n",
        "\n",
        "        # create the agent for Dyna-Q\n",
        "        dyna_q_plus_agent_fn = TabularDynaQPlusAgentFontNote(my_env, params)\n",
        "        result = dyna_q_plus_agent_fn.run()\n",
        "\n",
        "        results_dyna_q_plus_fn.append(result)\n",
        "\n",
        "    \"\"\" Plot the results\n",
        "    \"\"\"\n",
        "    plot_curves([np.array(results_dyna_q), np.array(results_dyna_q_plus), np.array(results_dyna_q_plus_fn)],\n",
        "                [\"Dyan-Q\", \"Dyna-Q+ no footnote\", \"Dyna-Q+ with footnote\"],\n",
        "                [\"b\", \"g\", \"r\"],\n",
        "                \"Cumulative reward\", \"Reproduced Figure 8.4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym-qEcQpvJKG"
      },
      "source": [
        "# Q3: (b) - Implement Dyna-Q and Dyna-Q+ to reproduce Figure 8.5\n",
        "\n",
        "- Use the **ShortcutMaze** environment;\n",
        "- Reproduce the Figure 8.5 on page 167\n",
        "    - X axis: the learning time step. In other words, it is the number of time steps that agent really interacts with the environment;\n",
        "    - Y axis: the cumulative reward, a sum of all rewards received by the agent till a particular time step t.\n",
        "- Hyperparameters:\n",
        "    - Dyna-Q:\n",
        "        - learning time steps: 6000\n",
        "        - planning steps number: 250\n",
        "        - alpha: 0.1\n",
        "        - gamma: 0.95\n",
        "        - epsilon: 0.1\n",
        "        - time to switch the maze: 3000\n",
        "    - Dyna-Q+:\n",
        "        - learning time steps: 6000\n",
        "        - planning steps number: 250\n",
        "        - alpha: 0.1\n",
        "        - gamma: 0.95\n",
        "        - epsilon: 0.1\n",
        "        - kappa: 5e-4\n",
        "        - time to switch the maze: 3000\n",
        "        \n",
        "Note that, to implement the Dyna-Q+, please read the section 8.3 and the footnote on page 168 carefully. For Dyna-Q+, we ask you to implement **two variants**. For the first variant, it does **not** use the strategy in the footnote. For the second, it use the strategy proposed in the footnote. Finally, besides the two lines in Figure 8.5, please add a third line for Dyna-Q+ without using the footnote strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jMCJxqVvJKG"
      },
      "outputs": [],
      "source": [
        "\"\"\" Great! You have successfully implement Dyna-Q and Dyna-Q+.\n",
        "    To reproduce the Figure 8.5, just change the environment to be ShortcutMaze.\n",
        "\n",
        "    It takes on average 2 mins on a Macbook Pro with M1.\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # set the random seed\n",
        "    np.random.seed(1234)\n",
        "    random.seed(1234)\n",
        "\n",
        "    # set hyper-parameters\n",
        "    params = {\n",
        "        \"switch_time_step\": 3000,\n",
        "        \"learning_step\": 6000,\n",
        "        \"planning_step\": 250,\n",
        "        \"alpha\": 0.1,\n",
        "        \"gamma\": 0.95,\n",
        "        \"epsilon\": 0.1,\n",
        "        \"kappa\": 5e-4\n",
        "    }\n",
        "\n",
        "    \"\"\" Set running trials for experiments\n",
        "    \"\"\"\n",
        "    run_trial_num = 10\n",
        "\n",
        "    \"\"\" Run Dyna-Q\n",
        "    \"\"\"\n",
        "    results_dyna_q = []\n",
        "    for r in tqdm.trange(run_trial_num):\n",
        "        # create the environment\n",
        "        my_env = ShortcutMaze()\n",
        "\n",
        "        # create the agent for Dyna-Q\n",
        "        dyna_q_agent = TabularDynaQAgent(my_env, params)\n",
        "        result = dyna_q_agent.run()\n",
        "\n",
        "        results_dyna_q.append(result)\n",
        "\n",
        "    \"\"\" Run Dyna-Q+ without the footnote\n",
        "    \"\"\"\n",
        "    results_dyna_q_plus = []\n",
        "    for r in tqdm.trange(run_trial_num):\n",
        "        # create the environment\n",
        "        my_env = ShortcutMaze()\n",
        "\n",
        "        # create the agent for Dyna-Q\n",
        "        dyna_q_plus_agent = TabularDynaQPlusAgent(my_env, params)\n",
        "        result = dyna_q_plus_agent.run()\n",
        "\n",
        "        results_dyna_q_plus.append(result)\n",
        "\n",
        "    \"\"\" Run Dyna-Q+ with the footnote\n",
        "    \"\"\"\n",
        "    results_dyna_q_plus_fn = []\n",
        "    for r in tqdm.trange(run_trial_num):\n",
        "        # create the environment\n",
        "        my_env = ShortcutMaze()\n",
        "\n",
        "        # create the agent for Dyna-Q\n",
        "        dyna_q_plus_agent_fn = TabularDynaQPlusAgentFontNote(my_env, params)\n",
        "        result = dyna_q_plus_agent_fn.run()\n",
        "\n",
        "        results_dyna_q_plus_fn.append(result)\n",
        "\n",
        "    plot_curves([np.array(results_dyna_q), np.array(results_dyna_q_plus), np.array(results_dyna_q_plus_fn)],\n",
        "                [\"Dyan-Q\", \"Dyna-Q+ no footnote\", \"Dyna-Q+ with footnote\"],\n",
        "                [\"b\", \"g\", \"r\"],\n",
        "                \"Cumulative reward\", \"Reproduced Figure 8.4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihZF8E_5vJKG"
      },
      "source": [
        "# Q4: Modify Dyna-Q to solve the Stochastic Windy GridWorld\n",
        "\n",
        "Please implement your modification to the original Dyan-Q algorithm and make it suitable to resolve a stochastic environment. In this case, we use the **stochastic Windy GridWorld** to test your implemtation.\n",
        "\n",
        "The environment implementation is as follows. You can use it directly or implement your own one. However, if you implement your own version, please make sure it is **correct** and **bug-free** and **well commented**. We would not be able to give credits if the environment implementation does not work properly or can not be understood.\n",
        "\n",
        "You can adopt your implementation of Dyna-Q above and make the changes. To show your results, you are asked to provide the followings:\n",
        "\n",
        "1. **Plot a figure** similar to Figure 8.2 in RL2e. Specifically, the x axis is the number of finished episodes; the y axis is the number of the time steps per episode. Note that, you do **not** need to plot curves for different planning steps (i.e., n). Just tune the n that works the best for you and plot the curve for that particular n (i.e., you might find n = 100 works the best in your case).\n",
        "2. **Print out the optimal path** that the algorithm finds by the end of the learning. In particular, you should write a code to do the followings:\n",
        "    - Reset the agent back to the start location;\n",
        "    - Write a for loop to let the agent interact with the environment automatically;\n",
        "    - At each time step, derive a greedy action using the Q table by the end of the learning;\n",
        "    - Observe the next state and reward by taking the action in the environment;\n",
        "    - Print out the followings for each time step:\n",
        "        - Time step stamp\n",
        "        - State (agent's previous location)\n",
        "        - Action (action just taken by the agent)\n",
        "        - Reward (reward for taking that action from previous location)\n",
        "        - Next state (location that agent results in after taking the action)\n",
        "        - Done (termination signal to show whether the episode is terminated)\n",
        "    \n",
        " For example, at time step 0, your code should print the followings:\n",
        "\n",
        "     \"time step = 0, state = [3, 0], action = \"up\", reward = 0, next_state = [2, 0], done = False\"\n",
        "\n",
        " A good policy can guide the agent from the start location to the goal location successfully.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpQuFzTSvJKG"
      },
      "outputs": [],
      "source": [
        "class StochasticWindyGridWorld(object):\n",
        "    def __init__(self):\n",
        "        # We define the grid for the DynaMaze domain\n",
        "        self.grid = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "        self.grid_height, self.grid_width = self.grid.shape\n",
        "\n",
        "        # We define the observation space consisting of all empty cells\n",
        "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()\n",
        "\n",
        "        # We define the action space\n",
        "        self.action_space = {\n",
        "            \"up\": np.array([-1, 0]),\n",
        "            \"down\": np.array([1, 0]),\n",
        "            \"left\": np.array([0, -1]),\n",
        "            \"right\": np.array([0, 1])\n",
        "        }\n",
        "        # We define the action names\n",
        "        self.action_names = ['up', 'down', 'left', 'right']\n",
        "\n",
        "        # We define the start state\n",
        "        self.start_location = [3, 0]\n",
        "\n",
        "        # We define the goal state\n",
        "        self.goal_location = [3, 7]\n",
        "\n",
        "        # Define the wind list\n",
        "        self.wind_space = (0, 0, 0, 1, 1, 1, 2, 2, 1, 0)\n",
        "\n",
        "        # We define other useful variables\n",
        "        self.agent_location = None\n",
        "        self.action = None\n",
        "\n",
        "    def reset(self):\n",
        "        # We reset the agent location to the start state\n",
        "        self.agent_location = self.start_location\n",
        "\n",
        "        # We set the information\n",
        "        info = {}\n",
        "        return self.agent_location, info\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action (string): name of the action (e.g., \"up\"/\"down\"/\"left\"/\"right\")\n",
        "        \"\"\"\n",
        "        # Convert the agent's location to array\n",
        "        loc_arr = np.array(self.agent_location)\n",
        "\n",
        "        # Convert the action name to movement array\n",
        "        act_arr = self.action_space[action]\n",
        "\n",
        "        # Make the wind stochastic\n",
        "        mean_wind = self.wind_space[self.agent_location[1]]\n",
        "        if mean_wind:  # if the wind exists\n",
        "            wind = np.random.choice([mean_wind - 1,\n",
        "                                     mean_wind,\n",
        "                                     mean_wind + 1],\n",
        "                                    p=[1.0/3.0, 1.0/3.0, 1.0/3.0])\n",
        "        else:\n",
        "            wind = 0\n",
        "        # Compute the wind array\n",
        "        wind_arr = -1 * np.array([wind, 0], dtype=int)\n",
        "\n",
        "        # compute the next state\n",
        "        next_agent_location = np.clip(loc_arr + act_arr + wind_arr,\n",
        "                                      a_min=np.array([0, 0]),\n",
        "                                      a_max=np.array([self.grid_height - 1, self.grid_width - 1])).tolist()\n",
        "\n",
        "        # Compute the reward\n",
        "        reward = 0 if next_agent_location == self.goal_location else -1\n",
        "\n",
        "        # Check the termination\n",
        "        terminated = True if reward == 0 else False\n",
        "\n",
        "        # Update the action location\n",
        "        self.agent_location = next_agent_location\n",
        "        self.action = action\n",
        "\n",
        "        return next_agent_location, reward, terminated, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        # plot the agent and the goal\n",
        "        # agent = 1\n",
        "        # goal = 2\n",
        "        plot_arr = self.grid.copy()\n",
        "        plot_arr[self.agent_location[0], self.agent_location[1]] = 2\n",
        "        plot_arr[self.goal_location[0], self.goal_location[1]] = 3\n",
        "        plt.clf()\n",
        "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
        "        plt.imshow(plot_arr)\n",
        "        plt.show(block=False)\n",
        "        plt.pause(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7hkSBTnvJKG"
      },
      "source": [
        "# Implement your modification to Dyna-Q here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIYtjAb_vJKG"
      },
      "outputs": [],
      "source": [
        "\"\"\" CODE YOUR MODIFICATION TO DYNA-Q HERE:\n",
        "    Tip: you can inherit the Dyna-Q above and change the methods as you wish.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYFo_82SvJKG"
      },
      "outputs": [],
      "source": [
        "\"\"\" CODE YOUR SOLUTION TO PLOT THE FIGURE HERE:\n",
        "    Tips: To plot the Figure 8.2, during the learning process, you can maintain a list to store the number of\n",
        "    time steps for each completed episode.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmoKIdWpvJKG"
      },
      "source": [
        "# Print out the optimal path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh0GNjOdvJKG"
      },
      "outputs": [],
      "source": [
        "def get_greedy_policy(state, Q):\n",
        "    \"\"\" CODE HERE: How to derive a greedy action using the Q values learned from Dyna-Q\"\"\"\n",
        "    # render the index of the state\n",
        "\n",
        "    # render all q values for state\n",
        "\n",
        "    # find the index of the maximal q values\n",
        "\n",
        "    # return the action corresponding to the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diUZHPr_vJKG"
      },
      "outputs": [],
      "source": [
        "# save the Q from the learning above\n",
        "# replace the None with the Q table you obtained.\n",
        "# Tips: you may also need to implement functions to get the index for states and actions. Therefore,\n",
        "# we suggest you to inherit the Dyna-Q above and only modify some methods to deal with the stochastic env.\n",
        "# Assign the learned object of the modified Dyna-Q to Q. You will have the flexibility to render indices and\n",
        "# render Q values, which is an attribute in the object.\n",
        "Q = None\n",
        "\n",
        "# create an instance of the env to print the optimal path\n",
        "test_env = StochasticWindyGridWorld()\n",
        "\n",
        "# reset the agent back to the start location\n",
        "state, _ = test_env.reset()\n",
        "\n",
        "for t in range(1000):\n",
        "    \"\"\" CODE HERE:\n",
        "        Implement a function that returns a greedy action (string) based on the current state\n",
        "        using the Q table obtained by the modified Dyna-Q.\n",
        "        Note that, action should be string variable (e.g., \"up\").\n",
        "        Replace the None with your implementation\n",
        "    \"\"\"\n",
        "    action = get_greedy_policy(state, Q)\n",
        "\n",
        "    # interact with the environment\n",
        "    next_state, reward, done, _, _ = test_env.step(action)\n",
        "\n",
        "    # print the path\n",
        "    print(f\"Time step = {t}, State = {state}, Action = {action}, Reward = {reward}, Next_state = {next_state}, Done = {done}\")\n",
        "\n",
        "    # check termination\n",
        "    if done:\n",
        "        break\n",
        "    else:\n",
        "        state = next_state\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}