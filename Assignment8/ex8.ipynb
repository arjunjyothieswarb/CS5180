{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/arjunjyothieswarb/CS5180/blob/main/Assignment8/ex8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "4yADT8TBwEA_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "- pygame: https://www.pygame.org/wiki/GettingStarted\n",
    "- ipywidgets: https://ipywidgets.readthedocs.io/en/latest/user_install.html\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBCan83DwEBA"
   },
   "source": [
    "## Implement Deep Neural Networks using Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kiqa4VzwEBA"
   },
   "source": [
    "In this exercise, we will use the Pytorch library (https://pytorch.org/) to build and train our deep neural networks. In the deep learning literature, especially in the research community, Pytorch is SUPER popular due to its automatic differentiation and dynamic computational graph (i.e., the graph is automatically generated, which is different from tensorflow where you have to define them beforehand). Briefly spearking, using Pytorch, you only have to build your neural network, define the forward pass, and the loss function. The library will automatically compute the weights and perform the backpropagation for you. For more details about Pytorch, we recommend you check the tutorails on the offical website to learn the basics (https://pytorch.org/tutorials/). If you come across any errors with training, try restarting the kernel first.\n",
    "\n",
    "Please try to learn the basics as much as you can. If you have any questions, feel free to ask them on Piazza or TA hours.\n",
    "\n",
    "Tested on Python 3.11.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRqvMX6TwEBA"
   },
   "source": [
    "## Please install the Pytorch library on your computer before you run this notebook.\n",
    "\n",
    "The installation instructions can be found here. (https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "sZCg0DtmwEBA",
    "outputId": "f32d97db-f769-433b-cae4-667854dc8b63"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Onb2Z7mHwEBA"
   },
   "source": [
    "# Q1: Nonlinear Function Approximation with Neural Networks\n",
    "\n",
    "We design this question to help you get familiar with the basics in the Pytorch library. Please read the question carefully in the problem set because we will focus more on Pytorch implementation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jud_DIQSwEBB"
   },
   "source": [
    "### Preparing the training set\n",
    "\n",
    "First, we create a training set to train the neural network model. Specifically, the dataset contains N training samples. For each sample, it is represented as (x, y), where x is input to the non-linear function \"f(x) = 1 + x^2\" and y is the output value.\n",
    "\n",
    "We provide a scaffolding code below to generate a training set that contains N samples for a particular non-linear function. Please complete the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nKv1ps_FwEBB"
   },
   "outputs": [],
   "source": [
    "\"\"\" Function is used to create a dataset that contains \"num\" samples given a particular non-linear function.\n",
    "    In this case, the non-linear function is f(x) = 1 + x ^ 2.\n",
    "\"\"\"\n",
    "def create_dataset(num, start_val, end_val):\n",
    "    \"\"\" Function that generates a dataset\n",
    "\n",
    "        Args:\n",
    "            num (int): number of samples to generate\n",
    "            start_val (float): the minimal value of x (included)\n",
    "            end_val (float): the maximal value of x (included)\n",
    "\n",
    "        Returns:\n",
    "            dataset (list): a list consists of (x, y) pairs.\n",
    "    \"\"\"\n",
    "    def nonlinear_function(val):\n",
    "        \"\"\" CODE HERE: non-linear function: 1 + val ^ 2\n",
    "        \"\"\"\n",
    "        return (1 + (val**2))\n",
    "\n",
    "    \"\"\" CODE HERE:\n",
    "        - create \"num\" even values between start_val and end_val using numpy.linspace\n",
    "        - create the dataset, which is a list of training tuples. e.g. (x, y)\n",
    "    \"\"\"\n",
    "    X = np.linspace(start_val, end_val, num)\n",
    "    dataset = [(x, nonlinear_function(x)) for x in X]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3QwTcLYwEBB"
   },
   "source": [
    "### Construct the neural network using Pytorch\n",
    "\n",
    "Here, we will define a Neural Network using the Pytorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxQclmSVwEBB"
   },
   "outputs": [],
   "source": [
    "\"\"\" Normally, using Pytorch, we will implement the Neural Network as a class.\n",
    "\n",
    "    Basically, we have to do the following steps:\n",
    "        - Inherit from the torch.nn.Module that contains the basics of Neural Networks in Pytorch.\n",
    "        - Define the architecture of the Neural Network in the \"def __init__() function\"\n",
    "            e.g., you can find the functions to create layers with different types under torch.nn.\n",
    "                  for example, we can build one linear layer use torch.nn.Linear() as follows.\n",
    "        - Define how you would like to perform a forward propagation using you neural network in the\n",
    "          \"def forward()\"\"\n",
    "\"\"\"\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_hidden_layer, dim_hidden_layer):\n",
    "        \"\"\" Args:\n",
    "                num_hidden_layer (int): number of the hidden layers\n",
    "                dim_hidden_layer (int): dimension for each hidden layer. You can use different dimensions for\n",
    "                                        different layers. But, we use the same dimension for all hidden layers\n",
    "                                        just for simplicity.\n",
    "\n",
    "            In this exercise, you are asked to design a 4-layers (2 hidden layers) fully connected neural network.\n",
    "        \"\"\"\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # define the input dimension\n",
    "        self.input_dim = 1\n",
    "\n",
    "        # define the hidden dimension\n",
    "        self.hidden_num = num_hidden_layer\n",
    "\n",
    "        # define the number of the hidden layers\n",
    "        self.hidden_dim = dim_hidden_layer\n",
    "\n",
    "        # define the output dimension\n",
    "        self.output_dim = 1\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "            Create a fully connected neural network here\n",
    "        \"\"\"\n",
    "        # define the input linear layer here\n",
    "\n",
    "        # define the activation function after the input layer (use the ReLU as the activation function)\n",
    "\n",
    "        # define the first hidden layer here\n",
    "\n",
    "        # define the activation function after the first hidden layer (use the ReLU as the activation function)\n",
    "\n",
    "        # define the second hidden layer here\n",
    "\n",
    "        # define the activation function after the second hidden layer (use the ReLU as the activation function)\n",
    "\n",
    "        # define the output layer here\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Function that defines the forward propagation\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "            Implement each forward propagation using the corresponding layers you defined above.\n",
    "        \"\"\"\n",
    "        # forward x through the input layer\n",
    "\n",
    "        # apply activation\n",
    "\n",
    "        # forward x throught the first hidden layer\n",
    "\n",
    "        # apply activation\n",
    "\n",
    "        # forward x throught the second hidden layer\n",
    "\n",
    "        # apply activation\n",
    "\n",
    "        # forward x throught the output layer\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pIRpS36wEBB"
   },
   "source": [
    "### Implement the training process\n",
    "\n",
    "Here, we provide you the scaffolding code to train the Neural Network. Using Pytorch, you have to learn to perform 3 essential steps:\n",
    "\n",
    "1. Sample a training batch\n",
    "2. Perform a forward propagation. In other words, given the sampled batch data, compute the prediction values using your Neural Network.\n",
    "3. Perform one step backpropagation. The gradients computation and the backpropagation are done automatically by Pytorch, which is the key reason that it is popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzhorzbjwEBB"
   },
   "outputs": [],
   "source": [
    "\"\"\" We implement a class to train a neural network\n",
    "\"\"\"\n",
    "class NeuralNetTrainer(object):\n",
    "    def __init__(self, dataset, network_model, params):\n",
    "        \"\"\" To train a Neural Network, we need:\n",
    "                - The training set\n",
    "                - The Neural Network model\n",
    "                - A Loss function\n",
    "                - An Optimizer\n",
    "                - other parameters\n",
    "            Thanks to Pytorch, we can use the built-in Loss function and the Optimizer.\n",
    "\n",
    "            Args:\n",
    "                dataset (list): a list contains all training data\n",
    "                network_model (nn.Module): a Pytorch defined neural network\n",
    "                params (dict): a dictionary stores the training parameters\n",
    "        \"\"\"\n",
    "        # Dataset\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # We can specify the device to train the model: cpu or GPU\n",
    "        self.device = torch.device(params['device'])\n",
    "        # Send the model to the device\n",
    "        self.model = network_model.to(self.device)\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                - Create the ADAM optimizer\n",
    "                - MSE loss\n",
    "        \"\"\"\n",
    "        # Define the Adam optimizer with specified leaarning rate and weight decay in \"params\"\n",
    "\n",
    "        # We use a simple Mean Square Error (MSE) loss.\n",
    "\n",
    "        # Save the training parameters\n",
    "        self.params = params\n",
    "\n",
    "    def sample_mini_batch(self, dataset, batch_size):\n",
    "        \"\"\" Function is used to sample a subset of the dataset to train the model.\n",
    "            We usually call it \"mini-batch\" data in machine learning, which is widely\n",
    "            used in stochastic gradient descend.\n",
    "\n",
    "            Args:\n",
    "                dataset (list):  a list contains all training data\n",
    "                batch_size (int): size of the sampled training data.\n",
    "\n",
    "            Returns:\n",
    "                input_tensor (torch.tensor): A tensor variable with size |B| x |D_x|, where B = batch_size, and D_x is\n",
    "                                             the dimension of x in one sampled data.\n",
    "                output_tensor (torch.tensor): A tensor variable with size |B| x |D_y|, where B = batch_size, and D_y is\n",
    "                                             the dimension of y in one sampled data.\n",
    "        \"\"\"\n",
    "        # We should always shuffle the whole dataset before sampling\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                - Sample a batch of training data\n",
    "        \"\"\"\n",
    "        # Sample a batch data from the original dataset\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                - Split the data into x list and y list\n",
    "        \"\"\"\n",
    "        # Split the x and y in the sampled data\n",
    "\n",
    "        # Convert the input and output into tensor.\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32).to(self.device).view(-1, 1)\n",
    "        output_tensor = torch.tensor(output_data, dtype=torch.float32).to(self.device).view(-1, 1)\n",
    "\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Function is used to run the training\n",
    "        \"\"\"\n",
    "        # Define the number of epochs to train the model\n",
    "        ep_bar = tqdm.trange(self.params['epoch_num'], desc=\"epoch bar\")\n",
    "\n",
    "        # Save the training loss for plotting.\n",
    "        loss = torch.tensor([0])\n",
    "        training_loss = []\n",
    "\n",
    "        # Loop for every training epoch\n",
    "        for ep in ep_bar:\n",
    "            # For every epoch, we update the model with a fixed number (i.e., iteration_num in params)\n",
    "            # of sampled batch data.\n",
    "            for it in range(self.params['iteration_num']):\n",
    "                # Sample a batch data\n",
    "                x_tensor, gt_y_tensor = self.sample_mini_batch(self.dataset, self.params['batch_size'])\n",
    "\n",
    "                \"\"\" CODE HERE:\n",
    "                        - Perform a forward propagation\n",
    "                \"\"\"\n",
    "                # Forward propagation\n",
    "\n",
    "\n",
    "                \"\"\" COER HERE:\n",
    "                        - Compute the loss after the forward propagation\n",
    "                \"\"\"\n",
    "                # Compute the MSE loss value\n",
    "\n",
    "                # Save the loss for plotting\n",
    "                training_loss.append(loss.item())\n",
    "\n",
    "                \"\"\" CODE HERE:\n",
    "                        - Complete the backpropagation\n",
    "                \"\"\"\n",
    "                # Perform one step back propagation\n",
    "\n",
    "            # Set the value to tqdm bar\n",
    "            ep_bar.set_description(f\"Loss = {loss.item()}\")\n",
    "\n",
    "        # Plot the training loss\n",
    "        plt.title(\"MSE loss curve\")\n",
    "        plt.plot(range(len(training_loss)), training_loss)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_learned_function(self, dataset):\n",
    "        \"\"\" Function to plot the learned non-linear function model (blue) v.s. the groud truth (red)\n",
    "\n",
    "            Args:\n",
    "                dataset (list): a list variable contains all evaluation (x, y) pairs.\n",
    "        \"\"\"\n",
    "        x_tensor, y_tensor = self.sample_mini_batch(dataset, len(dataset))\n",
    "\n",
    "\n",
    "        # compute the prediction for all data to evaluate\n",
    "        with torch.no_grad():\n",
    "            pred_y_tensor = self.model(x_tensor)\n",
    "\n",
    "        # convert the data from tensor to list\n",
    "        gt_x_list = x_tensor.cpu().numpy().tolist()\n",
    "        gt_y_list = y_tensor.cpu().numpy().tolist()\n",
    "        pred_y_list = pred_y_tensor.cpu().numpy().tolist()\n",
    "\n",
    "        # plot the results\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\"Preiction (blue) v.s. Ground truth (red)\")\n",
    "        ax.scatter(gt_x_list, gt_y_list, label=\"gt\", color=\"r\")\n",
    "        ax.scatter(gt_x_list, pred_y_list, label=\"pred\", color=\"b\")\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb4UsOBMwEBC"
   },
   "source": [
    "### Plot the loss during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIdHdKh6wEBC"
   },
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'epoch_num': 10,\n",
    "    'iteration_num': 2000,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 5e-4,\n",
    "    'batch_size': 32,\n",
    "    'device': \"cpu\"\n",
    "}\n",
    "\n",
    "my_network = NeuralNet(num_hidden_layer=2, dim_hidden_layer=8)\n",
    "\n",
    "train_dataset = create_dataset(500, -10, 10)\n",
    "\n",
    "my_trainer = NeuralNetTrainer(dataset=train_dataset, network_model=my_network, params=train_params)\n",
    "my_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQH_T5aVwEBC"
   },
   "source": [
    "### Plot the approximation function\n",
    "\n",
    "Here, we want to show how accurate the learned non-linear function is. Specifically, we create three datasets that you have to evaluate as follows:\n",
    "\n",
    "1. One training dataset: it contains all the training data you use to train the model above. (500 samples);\n",
    "2. One test dataset: it contains 100 test samples with x > 10;\n",
    "3. One test dataset: it contains 100 test samples with x < -10.\n",
    "\n",
    "Using the three dataset above, visualize the prediction value as well the ground truth value for all samples in one dataset. Use the \"plot_learned_function\" in the \"NeuralNetTrainer\" class above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7d2JcybwEBC"
   },
   "outputs": [],
   "source": [
    "# training dataset to evaluate\n",
    "train_dataset = create_dataset(500, -10, 10)\n",
    "\n",
    "# test set 1: one test dataset contains samples with 10 < x < 30\n",
    "upper_test_dataset = create_dataset(100, 10, 30)\n",
    "\n",
    "# test set 2: one test dataset contains samples with -30 < x < -10\n",
    "lower_test_dataset = create_dataset(100, -30, -10)\n",
    "\n",
    "# Plot your results for all samples in the training set\n",
    "my_trainer.plot_learned_function(train_dataset)\n",
    "\n",
    "# Plot your results for all samples in test set 1\n",
    "my_trainer.plot_learned_function(upper_test_dataset)\n",
    "\n",
    "# Plot your results for all samples in test set 2\n",
    "my_trainer.plot_learned_function(lower_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6-iljfpwEBC"
   },
   "source": [
    "# Q2: DQN with Four Rooms\n",
    "\n",
    "Here, let's write a DQN agent to resolve the FourRooms problem in this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-YFrksKwEBC"
   },
   "outputs": [],
   "source": [
    "\"\"\" Here is the implementation of the FourRooms\n",
    "    Note that, the reward function is changed to be:\n",
    "        - If the agent reaches the goal, it receives 0 and the episode terminates.\n",
    "        - For other time step, the agent receives -1 reward.\n",
    "\"\"\"\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # We define the grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # We define the observation space consisting of all empty cells\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # We define the action space\n",
    "        self.action_space = {'up': np.array([0, 1]),\n",
    "                             'down': np.array([0, -1]),\n",
    "                             'left': np.array([-1, 0]),\n",
    "                             'right': np.array([1, 0])}\n",
    "        self.action_names = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        # We define the start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # We define the goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # We find all wall cells\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, we define a timeout: maximal time steps = 459\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # We define other useful variables\n",
    "        self.agent_location = None  # track the agent's location in one episode.\n",
    "        self.action = None  # track the agent's action\n",
    "        self.t = 0  # track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates to the Four Rooms coordinates (i.e, The origin locates at bottom left).\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): a list variable consists of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): a list variable consists of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # We reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # We reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # We set the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (string): a string variable (i.e., \"UP\"). All feasible values are [\"up\", \"down\", \"left\", \"right\"].\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == \"left\" or action == \"right\":\n",
    "                action = np.random.choice([\"up\", \"down\"], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([\"right\", \"left\"], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_space[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        \"\"\"Note that, the reward function is changed as follows.\n",
    "        \"\"\"\n",
    "        # Compute the reward\n",
    "        reward = 0.0 if next_agent_location == self.goal_location else -1.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 0, done = True\n",
    "        # If the time steps reaches the maximal number, reward = -1, done = True.\n",
    "        if reward == 0.0 or self.t == self.max_time_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # Update the agent's location, action and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        my_env = FourRooms()\n",
    "        state, _ = my_env.reset()\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = np.random.choice(list(my_env.action_space.keys()), 1)[0]\n",
    "\n",
    "            next_state, reward, done, _, _ = my_env.step(action)\n",
    "            my_env.render()\n",
    "\n",
    "            if done:\n",
    "                state, _ = my_env.reset()\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-yrSFbgwEBC"
   },
   "source": [
    "## Define a Deep Q network\n",
    "\n",
    "Before, we write a DQN agent. Let's define a Deep Q network as we did in Q1. Otherwise, you could also adapt your\n",
    "implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "re5yl-sBwEBC"
   },
   "outputs": [],
   "source": [
    "# customized weight initialization\n",
    "def customized_weights_init(m):\n",
    "    # compute the gain\n",
    "    gain = nn.init.calculate_gain('relu')\n",
    "    # init the convolutional layer\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # init the params using uniform\n",
    "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    # init the linear layer\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # init the params using uniform\n",
    "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ixt3XU1wEBD"
   },
   "outputs": [],
   "source": [
    "class DeepQNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layer, dim_hidden_layer, output_dim):\n",
    "        super(DeepQNet, self).__init__()\n",
    "\n",
    "        \"\"\"CODE HERE: construct your Deep neural network\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"CODE HERE: implement your forward propagation\n",
    "        \"\"\"\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVBZHCxzwEBD"
   },
   "source": [
    "## Define a Experience Replay Buffer\n",
    "\n",
    "One main contribution of DQN is proposing to use the replay buffer. Here is the implementation of a simple replay buffer as a list of transitions (i.e., [(s, a, r, s', d), ....])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0GiZ88PwEBD"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\" Implement the Replay Buffer as a class, which contains:\n",
    "            - self._data_buffer (list): a list variable to store all transition tuples.\n",
    "            - add: a function to add new transition tuple into the buffer\n",
    "            - sample_batch: a function to sample a batch training data from the Replay Buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\"Args:\n",
    "               buffer_size (int): size of the replay buffer\n",
    "        \"\"\"\n",
    "        # total size of the replay buffer\n",
    "        self.total_size = buffer_size\n",
    "\n",
    "        # create a list to store the transitions\n",
    "        self._data_buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_buffer)\n",
    "\n",
    "    def add(self, obs, act, reward, next_obs, done):\n",
    "        # create a tuple\n",
    "        trans = (obs, act, reward, next_obs, done)\n",
    "\n",
    "        # interesting implementation\n",
    "        if self._next_idx >= len(self._data_buffer):\n",
    "            self._data_buffer.append(trans)\n",
    "        else:\n",
    "            self._data_buffer[self._next_idx] = trans\n",
    "\n",
    "        # increase the index\n",
    "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
    "\n",
    "    def _encode_sample(self, indices):\n",
    "        \"\"\" Function to fetch the state, action, reward, next state, and done arrays.\n",
    "\n",
    "            Args:\n",
    "                indices (list): list contains the index of all sampled transition tuples.\n",
    "        \"\"\"\n",
    "        # lists for transitions\n",
    "        obs_list, actions_list, rewards_list, next_obs_list, dones_list = [], [], [], [], []\n",
    "\n",
    "        # collect the data\n",
    "        for idx in indices:\n",
    "            # get the single transition\n",
    "            data = self._data_buffer[idx]\n",
    "            obs, act, reward, next_obs, d = data\n",
    "            # store to the list\n",
    "            obs_list.append(np.array(obs, copy=False))\n",
    "            actions_list.append(np.array(act, copy=False))\n",
    "            rewards_list.append(np.array(reward, copy=False))\n",
    "            next_obs_list.append(np.array(next_obs, copy=False))\n",
    "            dones_list.append(np.array(d, copy=False))\n",
    "        # return the sampled batch data as numpy arrays\n",
    "        return np.array(obs_list), np.array(actions_list), np.array(rewards_list), np.array(next_obs_list), np.array(\n",
    "            dones_list)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Args:\n",
    "                batch_size (int): size of the sampled batch data.\n",
    "        \"\"\"\n",
    "        # sample indices with replaced\n",
    "        indices = [np.random.randint(0, len(self._data_buffer)) for _ in range(batch_size)]\n",
    "        return self._encode_sample(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mgHvv0twEBD"
   },
   "source": [
    "## Define a shedule for epsilon-greedy policy\n",
    "\n",
    "Here, we define a shedule function to return the epsilon for each time step t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW8hkE3nwEBD"
   },
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    \"\"\" This schedule returns the value linearly\"\"\"\n",
    "    def __init__(self, start_value, end_value, duration):\n",
    "        # start value\n",
    "        self._start_value = start_value\n",
    "        # end value\n",
    "        self._end_value = end_value\n",
    "        # time steps that value changes from the start value to the end value\n",
    "        self._duration = duration\n",
    "        # difference between the start value and the end value\n",
    "        self._schedule_amount = end_value - start_value\n",
    "\n",
    "    def get_value(self, time):\n",
    "        # logic: if time > duration, use the end value, else use the scheduled value\n",
    "        return self._start_value + self._schedule_amount * min(1.0, time * 1.0 / self._duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziGVaMO6wEBD"
   },
   "source": [
    "## Define the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8syRwxVrwEBD"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    # initialize the agent\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 ):\n",
    "        # save the parameters\n",
    "        self.params = params\n",
    "\n",
    "        # environment parameters\n",
    "        self.action_dim = params['action_dim']\n",
    "        self.obs_dim = params['observation_dim']\n",
    "\n",
    "        # executable actions\n",
    "        self.action_space = params['action_space']\n",
    "\n",
    "        # create value network\n",
    "        self.behavior_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                   num_hidden_layer=params['hidden_layer_num'],\n",
    "                                   dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                   output_dim=params['action_dim'])\n",
    "        # create target network\n",
    "        self.target_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                          num_hidden_layer=params['hidden_layer_num'],\n",
    "                                          dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                          output_dim=params['action_dim'])\n",
    "\n",
    "        # initialize target network with behavior network\n",
    "        self.behavior_policy_net.apply(customized_weights_init)\n",
    "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
    "\n",
    "        # send the agent to a specific device: cpu or gpu\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.behavior_policy_net.to(self.device)\n",
    "        self.target_policy_net.to(self.device)\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.behavior_policy_net.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # get action\n",
    "    def get_action(self, obs, eps):\n",
    "        if np.random.random() < eps:  # with probability eps, the agent selects a random action\n",
    "            action = np.random.choice(self.action_space, 1)[0]\n",
    "            return action\n",
    "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
    "            obs = self._arr_to_tensor(obs).view(1, -1)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.behavior_policy_net(obs)\n",
    "                action = q_values.max(dim=1)[1].item()\n",
    "            return self.action_space[int(action)]\n",
    "\n",
    "    # update behavior policy\n",
    "    def update_behavior_policy(self, batch_data):\n",
    "        # convert batch data to tensor and put them on device\n",
    "        batch_data_tensor = self._batch_to_tensor(batch_data)\n",
    "\n",
    "        # get the transition data\n",
    "        obs_tensor = batch_data_tensor['obs']\n",
    "        actions_tensor = batch_data_tensor['action']\n",
    "        next_obs_tensor = batch_data_tensor['next_obs']\n",
    "        rewards_tensor = batch_data_tensor['reward']\n",
    "        dones_tensor = batch_data_tensor['done']\n",
    "\n",
    "        \"\"\"CODE HERE:\n",
    "                Compute the predicted Q values using the behavior policy network\n",
    "        \"\"\"\n",
    "        # compute the q value estimation using the behavior network\n",
    "\n",
    "        # compute the TD target using the target network\n",
    "\n",
    "        # compute the loss\n",
    "\n",
    "        # minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return td_loss.item()\n",
    "\n",
    "    # update update target policy\n",
    "    def update_target_policy(self):\n",
    "        # hard update\n",
    "        \"\"\"CODE HERE:\n",
    "                Copy the behavior policy network to the target network\n",
    "        \"\"\"\n",
    "\n",
    "    # auxiliary functions\n",
    "    def _arr_to_tensor(self, arr):\n",
    "        arr = np.array(arr)\n",
    "        arr_tensor = torch.from_numpy(arr).float().to(self.device)\n",
    "        return arr_tensor\n",
    "\n",
    "    def _batch_to_tensor(self, batch_data):\n",
    "        # store the tensor\n",
    "        batch_data_tensor = {'obs': [], 'action': [], 'reward': [], 'next_obs': [], 'done': []}\n",
    "        # get the numpy arrays\n",
    "        obs_arr, action_arr, reward_arr, next_obs_arr, done_arr = batch_data\n",
    "        # convert to tensors\n",
    "        batch_data_tensor['obs'] = torch.tensor(obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['action'] = torch.tensor(action_arr).long().view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['reward'] = torch.tensor(reward_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['next_obs'] = torch.tensor(next_obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['done'] = torch.tensor(done_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        return batch_data_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7i_u9lZwEBD"
   },
   "source": [
    "## Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnUFI3_NwEBD"
   },
   "outputs": [],
   "source": [
    "def train_dqn_agent(env, params):\n",
    "    # create the DQN agent\n",
    "    my_agent = DQNAgent(params)\n",
    "\n",
    "    # create the epsilon-greedy schedule\n",
    "    my_schedule = LinearSchedule(start_value=params['epsilon_start_value'],\n",
    "                                 end_value=params['epsilon_end_value'],\n",
    "                                 duration=params['epsilon_duration'])\n",
    "\n",
    "    # create the replay buffer\n",
    "    replay_buffer = ReplayBuffer(params['replay_buffer_size'])\n",
    "\n",
    "    # training variables\n",
    "    episode_t = 0\n",
    "    rewards = []\n",
    "    train_returns = []\n",
    "    train_loss = []\n",
    "    loss = 0\n",
    "\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # start training\n",
    "    pbar = tqdm.trange(params['total_training_time_step'])\n",
    "    last_best_return = 0\n",
    "    for t in pbar:\n",
    "        # scheduled epsilon at time step t\n",
    "        eps_t = my_schedule.get_value(t)\n",
    "        # get one epsilon-greedy action\n",
    "        action = my_agent.get_action(obs, eps_t)\n",
    "\n",
    "        # step in the environment\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # add to the buffer\n",
    "        replay_buffer.add(obs, env.action_names.index(action), reward, next_obs, done)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # check termination\n",
    "        if done:\n",
    "            # compute the return\n",
    "            G = 0\n",
    "            for r in reversed(rewards):\n",
    "                G = r + params['gamma'] * G\n",
    "\n",
    "            if G > last_best_return:\n",
    "                torch.save(my_agent.behavior_policy_net.state_dict(), f\"./{params['model_name']}\")\n",
    "\n",
    "            # store the return\n",
    "            train_returns.append(G)\n",
    "            episode_idx = len(train_returns)\n",
    "\n",
    "            # print the information\n",
    "            pbar.set_description(\n",
    "                f\"Ep={episode_idx} | \"\n",
    "                f\"G={np.mean(train_returns[-10:]) if train_returns else 0:.2f} | \"\n",
    "                f\"Eps={eps_t}\"\n",
    "            )\n",
    "\n",
    "            # reset the environment\n",
    "            episode_t, rewards = 0, []\n",
    "            obs, _ = env.reset()\n",
    "        else:\n",
    "            # increment\n",
    "            obs = next_obs\n",
    "            episode_t += 1\n",
    "\n",
    "        if t > params['start_training_step']:\n",
    "            # update the behavior model\n",
    "            if not np.mod(t, params['freq_update_behavior_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "\n",
    "            # update the target model\n",
    "            if not np.mod(t, params['freq_update_target_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "\n",
    "    # save the results\n",
    "    return train_returns, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df4XJL9gwEBD"
   },
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3UHVe-DwEBD"
   },
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouLmSXZxwEBD"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # create environment\n",
    "    my_env = FourRooms()\n",
    "\n",
    "    # create training parameters\n",
    "    train_parameters = {\n",
    "        'observation_dim': 2,\n",
    "        'action_dim': 4,\n",
    "        'action_space': my_env.action_names,\n",
    "        'hidden_layer_num': 2,\n",
    "        'hidden_layer_dim': 128,\n",
    "        'gamma': 0.99,\n",
    "\n",
    "        'total_training_time_step': 500_000,\n",
    "\n",
    "        'epsilon_start_value': 1.0,\n",
    "        'epsilon_end_value': 0.01,\n",
    "        'epsilon_duration': 250_000,\n",
    "\n",
    "        'replay_buffer_size': 50000,\n",
    "        'start_training_step': 2000,\n",
    "        'freq_update_behavior_policy': 4,\n",
    "        'freq_update_target_policy': 2000,\n",
    "\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 1e-3,\n",
    "\n",
    "        'model_name': \"four_room.pt\"\n",
    "    }\n",
    "\n",
    "    # create experiment\n",
    "    train_returns, train_loss = train_dqn_agent(my_env, train_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qysrJARwEBE"
   },
   "outputs": [],
   "source": [
    "plot_curves([np.array([train_returns])], ['dqn'], ['r'], 'discounted return', 'discounted returns wrt episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQlgBgxdwEBE"
   },
   "outputs": [],
   "source": [
    "plot_curves([np.array([train_loss])], ['dqn'], ['r'], 'training loss', 'loss wrt training steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC-Bt6ugwEBE"
   },
   "source": [
    "## Q3: DQN with Classic Controls\n",
    "\n",
    "In this notebook, we will implement DQN and run it on four environments which have a continuous state-space and discrete action-space.\n",
    "\n",
    " * Cart Pole: Balance a pole on a moving cart (https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    " * Lunar Lander: Fly and land a spaceship in the landing spot (https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "*Note: If you are having trouble loading Lunar Lander due to Box2D/SWIG issues, please refer to the [Gymnasium documentation](https://www.gymlibrary.dev/content/installation/#box2d) or online for more information.* One way is if Box2D is not installed, you can install it by run `pip install swig` followed by `pip install \"gymnasium[box2d]\"` or `pip install box2d box2d-kengz\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8hVG_7IwEBE"
   },
   "outputs": [],
   "source": [
    "# Envs for training (no rendering)\n",
    "envs = {\n",
    "    'cartpole': gym.make('CartPole-v1'),\n",
    "    'lunarlander': gym.make('LunarLander-v3'),\n",
    "}\n",
    "# Envs for visualization\n",
    "envs_vis = {\n",
    "    'cartpole': gym.make('CartPole-v1', render_mode=\"human\"),\n",
    "    'lunarlander': gym.make('LunarLander-v3', render_mode=\"human\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvAx266pwEBE"
   },
   "outputs": [],
   "source": [
    "def render(env, policy=None):\n",
    "    \"\"\"Graphically render an episode using the given policy\n",
    "\n",
    "    :param env: Gymnasium environment\n",
    "    :param policy: Function which maps state to action.  If None, the random\n",
    "                   policy is used.\n",
    "    \"\"\"\n",
    "\n",
    "    if policy is None:\n",
    "        # Random policy\n",
    "        def policy(state):\n",
    "            return env.action_space.sample()\n",
    "\n",
    "    # Basic gym loop\n",
    "    state, info = env.reset()\n",
    "    while True:\n",
    "        action = policy(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo1Iay9BwEBE"
   },
   "outputs": [],
   "source": [
    "#  Jupyter UI\n",
    "\n",
    "def button_callback(button):\n",
    "    for b in buttons:\n",
    "        b.disabled = True\n",
    "\n",
    "    env = envs_vis[button.description]\n",
    "    render(env)\n",
    "    env.close()\n",
    "\n",
    "    for b in buttons:\n",
    "        b.disabled = False\n",
    "\n",
    "buttons = []\n",
    "for env_id in envs_vis.keys():\n",
    "    button = widgets.Button(description=env_id)\n",
    "    button.on_click(button_callback)\n",
    "    buttons.append(button)\n",
    "\n",
    "print('Click a button to run a random policy:')\n",
    "widgets.HBox(buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDdHmEWLwEBE"
   },
   "source": [
    "## Part (a): Exponential $\\varepsilon$-greedy decay\n",
    "\n",
    "Instead of using a fixed value of $\\varepsilon$, it is common to anneal $\\varepsilon$ over time according to a schedule (such that initially almost all actions are exploratory). DQN used a linear decay schedule, but there we will use exponential decay, defined as:\n",
    "$$\\varepsilon_t = a \\exp (b t)$$\n",
    "where $a$ and $b$ are the parameters of the schedule. Beyond a specified number of time steps, $\\varepsilon$ will be kept fixed at a small constant value to maintain continual exploration.\n",
    "\n",
    "The interface to the scheduler receives the initial value, the final value, and in how many steps to go from initial to final. Your task is to compute parameters `a` and `b` to make the scheduler work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGlNTj3vwEBH"
   },
   "outputs": [],
   "source": [
    "class ExponentialSchedule:\n",
    "    def __init__(self, value_from, value_to, num_steps):\n",
    "        \"\"\"Exponential schedule from `value_from` to `value_to` in `num_steps` steps.\n",
    "\n",
    "        $value(t) = a \\exp (b t)$\n",
    "\n",
    "        :param value_from: Initial value\n",
    "        :param value_to: Final value\n",
    "        :param num_steps: Number of steps for the exponential schedule\n",
    "        \"\"\"\n",
    "        self.value_from = value_from\n",
    "        self.value_to = value_to\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # YOUR CODE HERE: Determine the `a` and `b` parameters such that the schedule is correct\n",
    "        self.a = ...\n",
    "        self.b = ...\n",
    "\n",
    "    def value(self, step) -> float:\n",
    "        \"\"\"Return exponentially interpolated value between `value_from` and `value_to`interpolated value between.\n",
    "\n",
    "        Returns {\n",
    "            `value_from`, if step == 0 or less\n",
    "            `value_to`, if step == num_steps - 1 or more\n",
    "            the exponential interpolation between `value_from` and `value_to`, if 0 <= steps < num_steps\n",
    "        }\n",
    "\n",
    "        :param step: The step at which to compute the interpolation\n",
    "        :rtype: Float. The interpolated value\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE: Implement the schedule rule as described in the docstring,\n",
    "        # using attributes `self.a` and `self.b`.\n",
    "        value = ...\n",
    "\n",
    "        return value\n",
    "\n",
    "\n",
    "# DO NOT EDIT: Test code\n",
    "\n",
    "def _test_schedule(schedule, step, value, ndigits=5):\n",
    "    \"\"\"Tests that the schedule returns the correct value.\"\"\"\n",
    "    v = schedule.value(step)\n",
    "    if not round(v, ndigits) == round(value, ndigits):\n",
    "        raise Exception(\n",
    "            f'For step {step}, the scheduler returned {v} instead of {value}'\n",
    "        )\n",
    "\n",
    "_schedule = ExponentialSchedule(0.1, 0.2, 3)\n",
    "_test_schedule(_schedule, -1, 0.1)\n",
    "_test_schedule(_schedule, 0, 0.1)\n",
    "_test_schedule(_schedule, 1, 0.141421356237309515)\n",
    "_test_schedule(_schedule, 2, 0.2)\n",
    "_test_schedule(_schedule, 3, 0.2)\n",
    "del _schedule\n",
    "\n",
    "_schedule = ExponentialSchedule(0.5, 0.1, 5)\n",
    "_test_schedule(_schedule, -1, 0.5)\n",
    "_test_schedule(_schedule, 0, 0.5)\n",
    "_test_schedule(_schedule, 1, 0.33437015248821106)\n",
    "_test_schedule(_schedule, 2, 0.22360679774997905)\n",
    "_test_schedule(_schedule, 3, 0.14953487812212207)\n",
    "_test_schedule(_schedule, 4, 0.1)\n",
    "_test_schedule(_schedule, 5, 0.1)\n",
    "del _schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4y9oQ6NwEBI"
   },
   "source": [
    "## Part (b): Replay memory\n",
    "\n",
    "Now we will implement the replay memory (also called the replay buffer), the data-structure where we store previous experiences so that we can re-sample and train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7McaW-2JwEBI"
   },
   "outputs": [],
   "source": [
    "# Batch namedtuple, i.e. a class which contains the given attributes\n",
    "Batch = namedtuple(\n",
    "    'Batch', ('states', 'actions', 'rewards', 'next_states', 'dones')\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_size, state_size):\n",
    "        \"\"\"Replay memory implemented as a circular buffer.\n",
    "\n",
    "        Experiences will be removed in a FIFO manner after reaching maximum\n",
    "        buffer size.\n",
    "\n",
    "        Args:\n",
    "            - max_size: Maximum size of the buffer\n",
    "            - state_size: Size of the state-space features for the environment\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        # Preallocating all the required memory, for speed concerns\n",
    "        self.states = torch.empty((max_size, state_size))\n",
    "        self.actions = torch.empty((max_size, 1), dtype=torch.long)\n",
    "        self.rewards = torch.empty((max_size, 1))\n",
    "        self.next_states = torch.empty((max_size, state_size))\n",
    "        self.dones = torch.empty((max_size, 1), dtype=torch.bool)\n",
    "\n",
    "        # Pointer to the current location in the circular buffer\n",
    "        self.idx = 0\n",
    "        # Indicates number of transitions currently stored in the buffer\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "\n",
    "        :param state: 1-D np.ndarray of state-features\n",
    "        :param action: Integer action\n",
    "        :param reward: Float reward\n",
    "        :param next_state: 1-D np.ndarray of state-features\n",
    "        :param done: Boolean value indicating the end of an episode\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE: Store the input values into the appropriate\n",
    "        # attributes, using the current buffer position `self.idx`\n",
    "\n",
    "        ...\n",
    "\n",
    "        # DO NOT EDIT\n",
    "        # Circulate the pointer to the next position\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        # Update the current buffer size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size) -> Batch:\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "\n",
    "        If the buffer contains less that `batch_size` transitions, sample all\n",
    "        of them.\n",
    "\n",
    "        :param batch_size: Number of transitions to sample\n",
    "        :rtype: Batch\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE: Randomly sample an appropriate number of\n",
    "        # transitions *without replacement*. If the buffer contains less than\n",
    "        # `batch_size` transitions, return all of them. The return type must\n",
    "        # be a `Batch`.\n",
    "\n",
    "        sample_indices = ...\n",
    "        batch = Batch(...)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def populate(self, env, num_steps):\n",
    "        \"\"\"Populate this replay memory with `num_steps` from the random policy.\n",
    "\n",
    "        :param env: Gymnasium environment\n",
    "        :param num_steps: Number of steps to populate the replay memory\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE: Run a random policy for `num_steps` time-steps and\n",
    "        # populate the replay memory with the resulting transitions.\n",
    "        # Hint: Use the self.add() method.\n",
    "\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFv-v5rSwEBI"
   },
   "source": [
    "## Part (c): Q-network\n",
    "\n",
    "In this section, we define the object that DQN learns -- the Q-value neural network.\n",
    "\n",
    "We use the PyTorch framework to define this neural network. PyTorch is a numeric computation library akin to NumPy, which also features automatic differentiation. This means that the library automatically computes the gradients for many differentiable operations, something we will exploit to train our models without having to manually program the gradients' code.\n",
    "*Caveats: Sometimes we have to pay explicit attention to whether the operations we are using are implemented by the library (most are), and there are a number of operations which do not play well with automatic differentiation (most notably, in-place assignments).*\n",
    "\n",
    "If you are unfamiliar with PyTorch, this will be a great opportunity to learn the basics. The official tutorials are a good start:\\\n",
    "https://pytorch.org/tutorials\n",
    "\n",
    "Do not worry about learning the advanced details; the basics are enough. If you can understand the following MNIST code example and are able to run it yourself to train an MNIST digit classifier, you should know more than enough PyTorch to complete the assignment).\\\n",
    "https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "\n",
    "This library is a tool, and as many tools you will have to learn how to use it well. Sometimes not using it well means that your program will crash.  Sometimes it means that your program will not crash but will not be computing the correct outputs. And sometimes it means that it will compute the correct things, but is less efficient than it could otherwise be. This library is very popular these days, and online resources abound, so take your time to learn the basics. If you are having problems, first try to debug it yourself, and also look up the errors you get online. You can also use Piazza and office hours to ask for help with problems.\n",
    "\n",
    "In the next cell, we inherit from the base class `torch.nn.Module` to implement our Q-network, which takes state-vectors and returns the respective action-values. Recall that the Q-network outputs the Q-values of **all** actions in the given input state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kihJMckjwEBI"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, *, num_layers=3, hidden_dim=256):\n",
    "        \"\"\"Deep Q-Network PyTorch model.\n",
    "\n",
    "        Args:\n",
    "            - state_dim: Dimensionality of states\n",
    "            - action_dim: Dimensionality of actions\n",
    "            - num_layers: Number of total linear layers\n",
    "            - hidden_dim: Number of neurons in the hidden layers\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # YOUR CODE HERE: Define the layers of your model such that\n",
    "        # * there are `num_layers` nn.Linear modules / layers\n",
    "        # * all activations except the last should be ReLU activations\n",
    "        #   (this can be achieved either using a nn.ReLU() object or the nn.functional.relu() method)\n",
    "        # * the last activation can either be missing, or you can use nn.Identity()\n",
    "        # Hint: A regular Python list of layers is tempting, but PyTorch does not register\n",
    "        # these parameters in its computation graph. See nn.ModuleList or nn.Sequential\n",
    "\n",
    "        ...\n",
    "\n",
    "    def forward(self, states) -> torch.Tensor:\n",
    "        \"\"\"Q function mapping from states to action-values.\n",
    "\n",
    "        :param states: (*, S) torch.Tensor where * is any number of additional\n",
    "                dimensions, and S is the dimensionality of state-space\n",
    "        :rtype: (*, A) torch.Tensor where * is the same number of additional\n",
    "                dimensions as the `states`, and A is the dimensionality of the\n",
    "                action-space. This represents the Q values Q(s, .)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE: Use the defined layers and activations to compute\n",
    "        # the action-values tensor associated with the input states.\n",
    "        # Hint: Do not worry about the * arguments above (previous dims in tensor).\n",
    "        # PyTorch functions typically handle those properly.\n",
    "\n",
    "        ...\n",
    "\n",
    "    # DO NOT EDIT: Utility methods for cloning and storing models.\n",
    "\n",
    "    @classmethod\n",
    "    def custom_load(cls, data):\n",
    "        model = cls(*data['args'], **data['kwargs'])\n",
    "        model.load_state_dict(data['state_dict'])\n",
    "        return model\n",
    "\n",
    "    def custom_dump(self):\n",
    "        return {\n",
    "            'args': (self.state_dim, self.action_dim),\n",
    "            'kwargs': {\n",
    "                'num_layers': self.num_layers,\n",
    "                'hidden_dim': self.hidden_dim,\n",
    "            },\n",
    "            'state_dict': self.state_dict(),\n",
    "        }\n",
    "\n",
    "\n",
    "# DO NOT EDIT: Test code\n",
    "\n",
    "def _test_dqn_forward(dqn_model, input_shape, output_shape):\n",
    "    \"\"\"Tests that the dqn returns the correctly shaped tensors.\"\"\"\n",
    "    inputs = torch.torch.randn((input_shape))\n",
    "    outputs = dqn_model(inputs)\n",
    "\n",
    "    if not isinstance(outputs, torch.FloatTensor):\n",
    "        raise Exception(\n",
    "            f'DQN.forward returned type {type(outputs)} instead of torch.Tensor'\n",
    "        )\n",
    "\n",
    "    if outputs.shape != output_shape:\n",
    "        raise Exception(\n",
    "            f'DQN.forward returned tensor with shape {outputs.shape} instead of {output_shape}'\n",
    "        )\n",
    "\n",
    "    if not outputs.requires_grad:\n",
    "        raise Exception(\n",
    "            f'DQN.forward returned tensor which does not require a gradient (but it should)'\n",
    "        )\n",
    "\n",
    "dqn_model = DQN(10, 4)\n",
    "_test_dqn_forward(dqn_model, (64, 10), (64, 4))\n",
    "_test_dqn_forward(dqn_model, (2, 3, 10), (2, 3, 4))\n",
    "del dqn_model\n",
    "\n",
    "dqn_model = DQN(64, 16)\n",
    "_test_dqn_forward(dqn_model, (64, 64), (64, 16))\n",
    "_test_dqn_forward(dqn_model, (2, 3, 64), (2, 3, 16))\n",
    "del dqn_model\n",
    "\n",
    "# Testing custom dump / load\n",
    "dqn1 = DQN(10, 4, num_layers=10, hidden_dim=20)\n",
    "dqn2 = DQN.custom_load(dqn1.custom_dump())\n",
    "assert dqn2.state_dim == 10\n",
    "assert dqn2.action_dim == 4\n",
    "assert dqn2.num_layers == 10\n",
    "assert dqn2.hidden_dim == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PV8y38twEBI"
   },
   "source": [
    "## Part (d): Single-batch update\n",
    "\n",
    "Recall that the Q-network in DQN is trained periodically using batches of experiences sampled from the replay memory. The following function computes the loss on this batch (one-step TD errors, using the Q-network and the target network) and uses the optimizer to perform one step of gradient descent using the gradient of this loss with respect to the Q-network parameters (automatically, thanks to PyTorch!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Frz2ScQSwEBI"
   },
   "outputs": [],
   "source": [
    "def train_dqn_batch(optimizer, batch, dqn_model, dqn_target, gamma) -> float:\n",
    "    \"\"\"Perform a single batch-update step on the given DQN model.\n",
    "\n",
    "    :param optimizer: nn.optim.Optimizer instance\n",
    "    :param batch: Batch of experiences (class defined earlier)\n",
    "    :param dqn_model: The DQN model to be trained\n",
    "    :param dqn_target: The target DQN model, ~NOT~ to be trained\n",
    "    :param gamma: The discount factor\n",
    "    :rtype: Float. The scalar loss associated with this batch\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE: Compute the values and target_values tensors using the\n",
    "    # given models and the batch of data.\n",
    "    # Recall that 'Batch' is a named tuple consisting of\n",
    "    # ('states', 'actions', 'rewards', 'next_states', 'dones')\n",
    "    # Hint: Remember that we should not pass gradients through the target network\n",
    "    values = ...\n",
    "    target_values = ...\n",
    "\n",
    "    # DO NOT EDIT\n",
    "\n",
    "    assert (\n",
    "        values.shape == target_values.shape\n",
    "    ), 'Shapes of values tensor and target_values tensor do not match.'\n",
    "\n",
    "    # Testing that the values tensor requires a gradient,\n",
    "    # and the target_values tensor does not\n",
    "    assert values.requires_grad, 'values tensor requires gradients'\n",
    "    assert (\n",
    "        not target_values.requires_grad\n",
    "    ), 'target_values tensor should not require gradients'\n",
    "\n",
    "    # Computing the scalar MSE loss between computed values and the TD-target\n",
    "    # DQN originally used Huber loss, which is less sensitive to outliers\n",
    "    loss = F.mse_loss(values, target_values)\n",
    "\n",
    "    optimizer.zero_grad()  # Reset all previous gradients\n",
    "    loss.backward()  # Compute new gradients\n",
    "    optimizer.step()  # Perform one gradient-descent step\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1oFYkuBwEBI"
   },
   "source": [
    "## Part (e): DQN training loop\n",
    "\n",
    "This is the main training loop for DQN. Please refer to Algorithm 1 in the DQN paper (reproduced in lecture slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A821jBbRwEBI"
   },
   "outputs": [],
   "source": [
    "def train_dqn(\n",
    "    env,\n",
    "    num_steps,\n",
    "    *,\n",
    "    num_saves=5,\n",
    "    replay_size,\n",
    "    replay_prepopulate_steps=0,\n",
    "    batch_size,\n",
    "    exploration,\n",
    "    gamma,\n",
    "):\n",
    "    \"\"\"\n",
    "    DQN algorithm.\n",
    "\n",
    "    Compared to previous training procedures, we will train for a given number\n",
    "    of time-steps rather than a given number of episodes. The number of\n",
    "    time-steps will be in the range of millions, which still results in many\n",
    "    episodes being executed.\n",
    "\n",
    "    Args:\n",
    "        - env: The Gymnasium environment\n",
    "        - num_steps: Total number of steps to be used for training\n",
    "        - num_saves: How many models to save to analyze the training progress\n",
    "        - replay_size: Maximum size of the ReplayMemory\n",
    "        - replay_prepopulate_steps: Number of steps with which to prepopulate\n",
    "                                    the memory\n",
    "        - batch_size: Number of experiences in a batch\n",
    "        - exploration: An ExponentialSchedule\n",
    "        - gamma: The discount factor\n",
    "\n",
    "    Returns: (saved_models, returns)\n",
    "        - saved_models: Dictionary whose values are trained DQN models\n",
    "        - returns: Numpy array containing the return of each training episode\n",
    "        - lengths: Numpy array containing the length of each training episode\n",
    "        - losses: Numpy array containing the loss of each training batch\n",
    "    \"\"\"\n",
    "    # Check that environment states are compatible with our DQN representation\n",
    "    assert (\n",
    "        isinstance(env.observation_space, gym.spaces.Box)\n",
    "        and len(env.observation_space.shape) == 1\n",
    "    )\n",
    "\n",
    "    # Get the state_size from the environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "\n",
    "    # Initialize the DQN and DQN-target models\n",
    "    dqn_model = DQN(state_size, env.action_space.n)\n",
    "    dqn_target = DQN.custom_load(dqn_model.custom_dump())\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(dqn_model.parameters())\n",
    "\n",
    "    # Initialize the replay memory and prepopulate it\n",
    "    memory = ReplayMemory(replay_size, state_size)\n",
    "    memory.populate(env, replay_prepopulate_steps)\n",
    "\n",
    "    # Initialize lists to store returns, lengths, and losses\n",
    "    rewards = []\n",
    "    returns = []\n",
    "    lengths = []\n",
    "    losses = []\n",
    "\n",
    "    # Initialize structures to store the models at different stages of training\n",
    "    t_saves = np.linspace(0, num_steps, num_saves - 1, endpoint=False)\n",
    "    saved_models = {}\n",
    "\n",
    "    i_episode = 0  # Use this to indicate the index of the current episode\n",
    "    t_episode = 0  # Use this to indicate the time-step inside current episode\n",
    "\n",
    "    state, info = env.reset()  # Initialize state of first episode\n",
    "    G=0\n",
    "\n",
    "    # Iterate for a total of `num_steps` steps\n",
    "    pbar = tqdm.trange(num_steps)\n",
    "    for t_total in pbar:\n",
    "        # Use t_total to indicate the time-step from the beginning of training\n",
    "\n",
    "        # Save model\n",
    "        if t_total in t_saves:\n",
    "            model_name = f'{100 * t_total / num_steps:04.1f}'.replace('.', '_')\n",
    "            saved_models[model_name] = copy.deepcopy(dqn_model)\n",
    "\n",
    "        # YOUR CODE HERE:\n",
    "        #  * sample an action from the DQN using epsilon-greedy\n",
    "        #  * use the action to advance the environment by one step\n",
    "        #  * store the transition into the replay memory\n",
    "\n",
    "        ...\n",
    "\n",
    "        # YOUR CODE HERE: Once every 4 steps,\n",
    "        #  * sample a batch from the replay memory\n",
    "        #  * perform a batch update (use the train_dqn_batch() method)\n",
    "\n",
    "        ...\n",
    "\n",
    "        # YOUR CODE HERE: Once every 10_000 steps,\n",
    "        #  * update the target network (use the dqn_model.state_dict() and\n",
    "        #    dqn_target.load_state_dict() methods)\n",
    "\n",
    "        ...\n",
    "\n",
    "        if done:\n",
    "            # YOUR CODE HERE: Anything you need to do at the end of an episode,\n",
    "            # e.g., compute return G, store returns/lengths,\n",
    "            # reset variables, indices, lists, etc.\n",
    "\n",
    "            ...\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Episode: {i_episode} | Steps: {t_episode + 1} | Return: {G:5.2f} | Epsilon: {eps:4.2f}'\n",
    "            )\n",
    "\n",
    "            ...\n",
    "        else:\n",
    "            # YOUR CODE HERE: Anything you need to do within an episode\n",
    "            ...\n",
    "\n",
    "    saved_models['100_0'] = copy.deepcopy(dqn_model)\n",
    "\n",
    "    return (\n",
    "        saved_models,\n",
    "        np.array(returns),\n",
    "        np.array(lengths),\n",
    "        np.array(losses),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz3kiArSwEBI"
   },
   "source": [
    "## Part (f): Evaluation of DQN on the 4 environments\n",
    "\n",
    "In the following section, run DQN on some/all of the 4 environments loaded in the beginning of this notebook (Cart Pole, Mountain Car, Acrobot, Lunar Lander).\n",
    "\n",
    "Each trial (in each environment) is trained for 1.5 million steps, which takes substantially longer than previous assignments, estimated to be around 1-2 hours on a typical desktop/laptop CPU. Because of this, we only expect you to train for one trial in each environment. Additionally, it is fine to only evaluate a minimum of 2 out of the 4 environments, although we recommend trying all 4 (and/or multiple trials) if you are able to.\n",
    "\n",
    "Since there is only one trial, we cannot obtain meaningful averages / confidence bands as in past assignments. Instead, we will just apply a moving average to smooth out the data in our graphs (you should plot both the raw data and the moving average).\n",
    "\n",
    "Obviously, during development and debugging, you should set the number of training steps (and possibly other hyperparameters) to be much lower. However, please remember to restore the original values when you perform the final evaluation. Also, different environments train at different speeds -- be cognizant of this when choosing an environment to develop in (e.g., Mountain car is designed as a hard exploration problem). We recommending starting in Cart Pole because it is an easier problem and the environment runs faster. For reference, we can run this environment at ~650 steps/s = 40K steps/min, and it usually takes 2000-4000 episodes = ~100K steps (with the default exponential schedule) to see some initial signs of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hg81StWzwEBI"
   },
   "outputs": [],
   "source": [
    "def moving_average(data, *, window_size = 50):\n",
    "    \"\"\"Smooths 1-D data array using a moving average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-D numpy.array\n",
    "        window_size: Size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: A 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSZULa4bwEBI"
   },
   "source": [
    "### Cart Pole\n",
    "\n",
    "Test your implentation on the Cart Pole environment. Training will take much longer than in the previous homeworks, so this time you will not have to find good hyperparameters or train multiple runs. This cell should take about 1-2 hours to run. After training, run the last cell in this notebook to view the policies which were obtained at 0%, 25%, 50%, 75% and 100% of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzJW-VulwEBJ"
   },
   "outputs": [],
   "source": [
    "env = envs['cartpole']\n",
    "gamma = 0.99\n",
    "\n",
    "# We train for many time-steps; as usual, you can decrease this during development / debugging,\n",
    "# but make sure to restore it to 1_500_000 before submitting\n",
    "num_steps = 1_500_000\n",
    "num_saves = 5  # Save models at 0%, 25%, 50%, 75% and 100% of training\n",
    "\n",
    "replay_size = 200_000\n",
    "replay_prepopulate_steps = 50_000\n",
    "\n",
    "batch_size = 64\n",
    "exploration = ExponentialSchedule(1.0, 0.05, 1_000_000)\n",
    "\n",
    "# This should take about 1-2 hours on a generic 4-core laptop\n",
    "dqn_models, returns, lengths, losses = train_dqn(\n",
    "    env,\n",
    "    num_steps,\n",
    "    num_saves=num_saves,\n",
    "    replay_size=replay_size,\n",
    "    replay_prepopulate_steps=replay_prepopulate_steps,\n",
    "    batch_size=batch_size,\n",
    "    exploration=exploration,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "assert len(dqn_models) == num_saves\n",
    "assert all(isinstance(value, DQN) for value in dqn_models.values())\n",
    "\n",
    "# Saving computed models to disk, so that we can load and visualize them later\n",
    "checkpoint = {key: dqn.custom_dump() for key, dqn in dqn_models.items()}\n",
    "torch.save(checkpoint, f'checkpoint_{env.spec.id}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0kbPw40wEBJ",
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "#### Plot the returns, lengths, and losses obtained while running DQN on the Cart Pole environment.\n",
    "\n",
    "Again, plot both the raw data and the moving average **in the same plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azkaBvYJwEBJ"
   },
   "outputs": [],
   "source": [
    "plt.plot(returns)\n",
    "plt.plot(moving_average(returns))\n",
    "plt.show()\n",
    "plt.plot(lengths)\n",
    "plt.plot(moving_average(lengths))\n",
    "plt.show()\n",
    "plt.plot(losses)\n",
    "plt.plot(moving_average(losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtB9V5NIwEBJ"
   },
   "source": [
    "### Lunar Lander\n",
    "\n",
    "Test your implentation on the Lunar Lander environment. Training will take much longer than in the previous homeworks, so this time you will not have to find good hyperparameters or train multiple runs. This cell should take about 1-2 hours to run. After training, run the last cell in this notebook to view the policies which were obtained at 0%, 25%, 50%, 75% and 100% of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JXpBGw0wEBJ"
   },
   "outputs": [],
   "source": [
    "env = envs['lunarlander']\n",
    "gamma = 0.99\n",
    "\n",
    "# We train for many time-steps; as usual, you can decrease this during development / debugging,\n",
    "# but make sure to restore it to 1_500_000 before submitting\n",
    "num_steps = 1_500_000\n",
    "num_saves = 5  # save models at 0%, 25%, 50%, 75% and 100% of training\n",
    "\n",
    "replay_size = 200_000\n",
    "replay_prepopulate_steps = 50_000\n",
    "\n",
    "batch_size = 64\n",
    "exploration = ExponentialSchedule(1.0, 0.05, 1_000_000)\n",
    "\n",
    "# This should take about 1-2 hours on a generic 4-core laptop\n",
    "dqn_models, returns, lengths, losses = train_dqn(\n",
    "    env,\n",
    "    num_steps,\n",
    "    num_saves=num_saves,\n",
    "    replay_size=replay_size,\n",
    "    replay_prepopulate_steps=replay_prepopulate_steps,\n",
    "    batch_size=batch_size,\n",
    "    exploration=exploration,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "assert len(dqn_models) == num_saves\n",
    "assert all(isinstance(value, DQN) for value in dqn_models.values())\n",
    "\n",
    "# Saving computed models to disk, so that we can load and visualize them later\n",
    "checkpoint = {key: dqn.custom_dump() for key, dqn in dqn_models.items()}\n",
    "torch.save(checkpoint, f'checkpoint_{env.spec.id}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tG1m_9ZwEBJ"
   },
   "source": [
    "#### Plot the returns, lengths, and losses obtained while running DQN on the Lunar Lander environment.\n",
    "\n",
    "Again, plot both the raw data and the moving average **in the same plot**, i.e., you should have 2 plots total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq_8LzX1wEBJ"
   },
   "outputs": [],
   "source": [
    "plt.plot(returns)\n",
    "plt.plot(moving_average(returns))\n",
    "plt.show()\n",
    "plt.plot(lengths)\n",
    "plt.plot(moving_average(lengths))\n",
    "plt.show()\n",
    "plt.plot(losses)\n",
    "plt.plot(moving_average(losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq5IQtyHwEBJ"
   },
   "source": [
    "### Visualization of the trained policies\n",
    "\n",
    "Run the cell below and push the buttons to view the progress of the policy trained using DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gTN0BUewEBJ"
   },
   "outputs": [],
   "source": [
    "buttons_all = []\n",
    "for key_env, env in envs_vis.items():\n",
    "    try:\n",
    "        checkpoint = torch.load(f'checkpoint_{env.spec.id}.pt')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    else:\n",
    "        buttons = []\n",
    "        for key, value in checkpoint.items():\n",
    "            dqn = DQN.custom_load(value)\n",
    "\n",
    "            def make_callback(env, dqn):\n",
    "                def button_callback(button):\n",
    "                    for b in buttons_all:\n",
    "                        b.disabled = True\n",
    "\n",
    "                    render(env, lambda state: dqn(torch.tensor(state, dtype=torch.float)).argmax().item())\n",
    "\n",
    "                    for b in buttons_all:\n",
    "                        b.disabled = False\n",
    "\n",
    "                return button_callback\n",
    "\n",
    "            button = widgets.Button(description=f'{key.replace(\"_\", \".\")}%')\n",
    "            button.on_click(make_callback(env, dqn))\n",
    "            buttons.append(button)\n",
    "\n",
    "        print(f'{key_env}:')\n",
    "        display(widgets.HBox(buttons))\n",
    "        buttons_all.extend(buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvbaESnpwEBJ"
   },
   "source": [
    "### Analysis\n",
    "\n",
    "For each environment that you trained in, describe the progress of the training in terms of the behavior of the agent at each of the 5 phases of training (i.e. 0%, 25%, 50%, 75%, 100%). Make sure you view each phase a few times so that you can see all sorts of variations.\n",
    "\n",
    "Describe something for each phase. Start by describing the behavior at phase 0%, then, for each next phase, describe how it differs from the previous one, how it improves and/or how it becomes worse. At the final phase (100%), also describe the observed behavior in absolute terms, and whether it has achieved optimality.\n",
    "\n",
    "*Note: You may need to restart the kernel after rendering some episodes. Do not manually close the Pygame window. Even if you restart the kernel, you do not need to re-train on the environments; the relevant Q-network parameters should be stored in the corresponding PyTorch checkpoint .pt file.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgjjLbqowEBJ"
   },
   "source": [
    "#### Cart Pole\n",
    "\n",
    "* 0%) YOUR ANSWER HERE.\n",
    "* 25%) YOUR ANSWER HERE.\n",
    "* 50%) YOUR ANSWER HERE.\n",
    "* 75%) YOUR ANSWER HERE.\n",
    "* 100%) YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wyITqyjwEBJ"
   },
   "source": [
    "#### Lunar Lander\n",
    "\n",
    "* 0%) YOUR ANSWER HERE.\n",
    "* 25%) YOUR ANSWER HERE.\n",
    "* 50%) YOUR ANSWER HERE.\n",
    "* 75%) YOUR ANSWER HERE.\n",
    "* 100%) YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFcuij_kwEBJ"
   },
   "source": [
    "# Q4 Extra Credit, DQN with Atari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvBIeCnFwEBJ"
   },
   "outputs": [],
   "source": [
    "\"\"\" Implement your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZSINuMPwEBK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
