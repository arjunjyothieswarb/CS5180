{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunjyothieswarb/CS5180/blob/main/ex3_fall24/ex3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f00b9b14",
      "metadata": {
        "id": "f00b9b14"
      },
      "source": [
        "# Please install the following python libraries\n",
        "- python3: https://www.python.org/\n",
        "- numpy: https://numpy.org/install/\n",
        "- tqdm: https://github.com/tqdm/tqdm#installation\n",
        "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
        "- scipy: https://scipy.org/install/ (**New package needs to be installed**)\n",
        "\n",
        "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
        "    \n",
        "Please install the ipywidgets as follows:\n",
        "\n",
        "    with pip, do\n",
        "    - pip install ipywidgets\n",
        "    \n",
        "    with conda, do\n",
        "    - conda install -c conda-forge ipywidgets\n",
        "    \n",
        "Restart your notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "2hTmLs_J2hq0"
      },
      "id": "2hTmLs_J2hq0"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzOXdXJV2T42",
        "outputId": "d4de5cdc-d732-4f3f-cb4b-01fc13d3849c"
      },
      "id": "SzOXdXJV2T42",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.9)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing"
      ],
      "metadata": {
        "id": "rS7HyGMC2nn7"
      },
      "id": "rS7HyGMC2nn7"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c9717c8",
      "metadata": {
        "scrolled": true,
        "id": "1c9717c8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import poisson\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886e5946",
      "metadata": {
        "id": "886e5946"
      },
      "source": [
        "# Q4 - Implementing dynamic programming algorithms in the GridWorld domain.\n",
        "\n",
        "In this question, you are asked to implement the **value iteration** (See the pseudocode on page 83) and **policy iteration** (See the pseudocode on page 80).\n",
        "\n",
        "- The implementation of the GridWorld is given in the follow **GridWorld** class. Please use it to compute the dynamics and the reward\n",
        "- Please implement the value iteration and policy iteration in the following blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88988d93",
      "metadata": {
        "scrolled": true,
        "id": "88988d93"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK\"\"\"\n",
        "# The GridWorld domain in Example 3.5\n",
        "class GridWorld(object):\n",
        "    def __init__(self):\n",
        "        # define the state space\n",
        "        self.state_space = [\n",
        "            [0, 0], [0, 1], [0, 2], [0, 3], [0, 4],\n",
        "            [1, 0], [1, 1], [1, 2], [1, 3], [1, 4],\n",
        "            [2, 0], [2, 1], [2, 2], [2, 3], [2, 4],\n",
        "            [3, 0], [3, 1], [3, 2], [3, 3], [3, 4],\n",
        "            [4, 0], [4, 1], [4, 2], [4, 3], [4, 4]\n",
        "        ]\n",
        "\n",
        "        # define special states\n",
        "        self.A, self.B = np.array([0, 1]), np.array([0, 3])\n",
        "        self.A_prime, self.B_prime = np.array([4, 1]), np.array([2, 3])\n",
        "\n",
        "        # define the action space\n",
        "        self.action_space = {\n",
        "            \"north\": [-1, 0],\n",
        "            \"south\": [1, 0],\n",
        "            \"west\": [0, -1],\n",
        "            \"east\": [0, 1]\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, s, a) -> (list, float):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (list): a list contains the position of the current state\n",
        "            a (str): name of the action\n",
        "        \"\"\"\n",
        "        # convert the state to numpy array\n",
        "        s_arr = np.array(s)\n",
        "        # convert the action to numpy array\n",
        "        a_arr = np.array(self.action_space[a])\n",
        "\n",
        "        # compute the next state and reward using the dynamics function\n",
        "        next_s, r = self.dynamics_func(s_arr, a_arr)\n",
        "\n",
        "        # return the next state and the reward\n",
        "        return next_s, r\n",
        "\n",
        "    def dynamics_func(self, s_arr, a_arr) -> (list, float):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s_arr (numpy.array): numpy array contains the position of the current state\n",
        "            a_arr (numpy.array): numpy array contains the change of the current state\n",
        "        \"\"\"\n",
        "        # check for special states A and B\n",
        "        # From state A, all four actions yield a reward of +10 and take the agent to A_prime\n",
        "        if np.array_equal(s_arr, self.A):\n",
        "            return self.A_prime.tolist(), 10.0\n",
        "\n",
        "        # From state B, all actions yield a reward of +5 and take the agent to B prime.\n",
        "        if np.array_equal(s_arr, self.B):\n",
        "            return self.B_prime.tolist(), 5.0\n",
        "\n",
        "        # check for normal states\n",
        "        # compute the next state position and reward\n",
        "        next_s = s_arr + a_arr\n",
        "        if next_s.tolist() not in self.state_space:\n",
        "            # Actions that would take the agent off the grid leave its location unchanged, but also result in a reward\n",
        "            # of -1\n",
        "            return s_arr.tolist(), -1.0\n",
        "        else:\n",
        "            # Other actions result in a reward of 0\n",
        "            return next_s.tolist(), 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2c5ef860",
      "metadata": {
        "scrolled": true,
        "id": "2c5ef860"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK\"\"\"\n",
        "# Function to print the optimal state value\n",
        "def print_optimal_state_value(s_v):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        s_v (numpy.array): a 2-D numpy array contains the optimal state values with size 5 x 5\n",
        "    \"\"\"\n",
        "    print(\"=============================\")\n",
        "    print(\"==  Optimal State Value    ==\")\n",
        "    print(\"=============================\")\n",
        "    print(s_v.round(decimals=1))\n",
        "    print(\"=============================\")\n",
        "\n",
        "# Function to print the optimal policy\n",
        "def print_optimal_policy(s_v, env, ga):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        s_v (numpy.array): a 2-D numpy array contains the optimal state value with size 5 x 5\n",
        "        env (env): the grid-world environment\n",
        "        ga (float): gamma\n",
        "    \"\"\"\n",
        "    print(\"=============================\")\n",
        "    print(\"==     Optimal Policy      ==\")\n",
        "    print(\"=============================\")\n",
        "    action_names = list(env.action_space.keys())\n",
        "    for i in range(5):\n",
        "        for j in range(5):\n",
        "            q_v = []\n",
        "            for a in env.action_space.keys():\n",
        "                next_s, r = env.step([i, j], a)\n",
        "                q_v.append(r + ga * s_v[next_s[0], next_s[1]])\n",
        "            q_v = np.array(q_v)\n",
        "\n",
        "            actions = np.where(q_v == q_v.max())[0]\n",
        "            actions = [action_names[a] for a in actions]\n",
        "\n",
        "            print(f\"{[i, j]} = {actions}\")\n",
        "        print(\"------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "448cf882",
      "metadata": {
        "id": "448cf882"
      },
      "source": [
        "## Q4 - (a): Implement value iteration. Please complete the implementation of the value iteration below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6f3aa8c0",
      "metadata": {
        "scrolled": true,
        "id": "6f3aa8c0"
      },
      "outputs": [],
      "source": [
        "# We provide the scaffolding code of running the value iteration\n",
        "# Please implement the value iteration algorithm below \"\"\"CODE HERE\"\"\"\n",
        "def run_value_iteration(env, threshold, gamma):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        env: the grid-world environment, we use it to compute:\n",
        "            - the next state: s'\n",
        "            - the transition probability: p(s'|s,a)\n",
        "            - the reward : r\n",
        "        threshold: threshold determining the estimation threshold\n",
        "        gamma: the discounted factor\n",
        "\n",
        "        Note: we use the vanilla implementation, where we maintain two separate numpy arrays to store the\n",
        "              state value and the updated state value.\n",
        "    \"\"\"\n",
        "    # initialize the state value to be 0\n",
        "    state_value = np.zeros((5, 5))\n",
        "\n",
        "    # iteration counter\n",
        "    iter_counter = 0\n",
        "\n",
        "    # loop forever\n",
        "    while True:\n",
        "        # Logic: assuming the value iteration should be terminated for the current iteration\n",
        "        # unless there exists one state whose value estimation error > threshold. i.e. (abs(new_v - old_v) > threshold)\n",
        "        is_terminal = True\n",
        "\n",
        "        # save the new state value\n",
        "        new_state_value = np.zeros_like(state_value)\n",
        "\n",
        "        # loop all states\n",
        "        # each state is the position of the agent in the grid. e.g., [i, j]\n",
        "        # where i, j in [0, 4]\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "                \"\"\" CODE HERE \"\"\"\n",
        "                # obtain the current state value estimation\n",
        "                old_v = state_value[i,j]\n",
        "\n",
        "                # compute the updated state value V(s) using equation 4.10.\n",
        "                # note that,\n",
        "                # 1. Use p(s'|s, a) rather than p(s', r|s, a).\n",
        "                # 2. The environment is deterministic. In other words, there is only one\n",
        "                #    possible s' and r given s and a.\n",
        "\n",
        "                new_v = float('-inf')\n",
        "                for act in env.action_space.keys():\n",
        "                    next_s, r = env.step([i,j], act)\n",
        "                    val = r + (gamma * state_value[next_s[0], next_s[1]])\n",
        "                    new_v = max(new_v, val)\n",
        "\n",
        "                # check the termination\n",
        "                # set is_terminal = False if |new_v - old_v| > threshold\n",
        "                if(abs(new_v - old_v) > threshold):\n",
        "                    is_terminal = False\n",
        "\n",
        "\n",
        "                \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
        "                # store the updated value in the new_state_value\n",
        "                new_state_value[i, j] = new_v\n",
        "\n",
        "        # update the current state value with the updated values\n",
        "        state_value = new_state_value.copy()\n",
        "\n",
        "        # terminate the loop\n",
        "        if is_terminal:\n",
        "            break\n",
        "\n",
        "    return state_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "80bc95cf",
      "metadata": {
        "id": "80bc95cf",
        "outputId": "e4d579f3-f746-4072-9741-4a5a4c827289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================\n",
            "==  Optimal State Value    ==\n",
            "=============================\n",
            "[[22.  24.4 22.  19.4 17.5]\n",
            " [19.8 22.  19.8 17.8 16. ]\n",
            " [17.8 19.8 17.8 16.  14.4]\n",
            " [16.  17.8 16.  14.4 13. ]\n",
            " [14.4 16.  14.4 13.  11.7]]\n",
            "=============================\n",
            "=============================\n",
            "==     Optimal Policy      ==\n",
            "=============================\n",
            "[0, 0] = ['east']\n",
            "[0, 1] = ['north', 'south', 'west', 'east']\n",
            "[0, 2] = ['west']\n",
            "[0, 3] = ['north', 'south', 'west', 'east']\n",
            "[0, 4] = ['west']\n",
            "------------------------------\n",
            "[1, 0] = ['north', 'east']\n",
            "[1, 1] = ['north']\n",
            "[1, 2] = ['north', 'west']\n",
            "[1, 3] = ['west']\n",
            "[1, 4] = ['west']\n",
            "------------------------------\n",
            "[2, 0] = ['north', 'east']\n",
            "[2, 1] = ['north']\n",
            "[2, 2] = ['north', 'west']\n",
            "[2, 3] = ['north', 'west']\n",
            "[2, 4] = ['north', 'west']\n",
            "------------------------------\n",
            "[3, 0] = ['north', 'east']\n",
            "[3, 1] = ['north']\n",
            "[3, 2] = ['north', 'west']\n",
            "[3, 3] = ['north', 'west']\n",
            "[3, 4] = ['north', 'west']\n",
            "------------------------------\n",
            "[4, 0] = ['north', 'east']\n",
            "[4, 1] = ['north']\n",
            "[4, 2] = ['north', 'west']\n",
            "[4, 3] = ['north', 'west']\n",
            "[4, 4] = ['north', 'west']\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "\"\"\"DO NOT CHANGE: it is used to run the value iteration above\"\"\"\n",
        "# run value iteration (DO NOT CHANGE)\n",
        "# create the envrionment\n",
        "my_grid = GridWorld()\n",
        "my_grid.reset()\n",
        "\n",
        "# threshold determining the accuracy of the estimation\n",
        "threshold = 1e-3\n",
        "\n",
        "# discounted factor\n",
        "gamma = 0.9\n",
        "\n",
        "# run the value iteration\n",
        "state_value = run_value_iteration(my_grid, threshold, gamma)\n",
        "\n",
        "# print the optimal state value\n",
        "print_optimal_state_value(state_value)\n",
        "\n",
        "# print the optimal policy\n",
        "print_optimal_policy(state_value, my_grid, gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb1f397",
      "metadata": {
        "id": "bcb1f397"
      },
      "source": [
        "## Q4 - (b): Implement policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "ba03a52e",
      "metadata": {
        "scrolled": false,
        "id": "ba03a52e"
      },
      "outputs": [],
      "source": [
        "# We provide the scaffolding code of implement the policy evaluation\n",
        "# Please implement the value iteration algorithm below \"\"\"CODE HERE\"\"\"\n",
        "def policy_evaluation(env, policy, threshold, gamma):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        env: the grid-world environment, we use it to compute:\n",
        "            - the next state: s'\n",
        "            - the transition probability: p(s'|s,a)\n",
        "            - the reward : r\n",
        "        policy (numpy.array): a 2-D numpy array stores the action to take at each location.\n",
        "        threshold (float): threshold determining the estimation threshold\n",
        "        gamma (float): the discounted factor\n",
        "\n",
        "        Note: we use the vanilla implementation, where we maintain two separate numpy arrays to store the\n",
        "              state value and the updated state value.\n",
        "    \"\"\"\n",
        "    # initialize the state values\n",
        "    state_value = np.zeros((5, 5))\n",
        "\n",
        "    # action_space = [\"north\", \"south\", \"west\", \"east\"]\n",
        "\n",
        "    # start evaluate the current policy\n",
        "    while True:\n",
        "        # set terminal flag similar to the value iteration flag.\n",
        "        is_terminal = True\n",
        "\n",
        "        # new state value\n",
        "        new_state_value = np.zeros_like(state_value)\n",
        "\n",
        "        action_space = list(env.action_space.keys())\n",
        "\n",
        "        # loop all states\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "                \"\"\" CODE HERE \"\"\"\n",
        "                # store the old state value\n",
        "                old_v = state_value[i,j]\n",
        "\n",
        "\n",
        "                # update the state value using the equation 4.5\n",
        "                # Hints: how many next state are there given a deterministic policy and\n",
        "                # a deterministic environment.\n",
        "                new_v = 0\n",
        "                # for k in range(4):\n",
        "                #     next_s, r = env.step([i,j], action_space[k])\n",
        "                #     new_v = new_v + (0.25 * (r + (gamma*state_value[next_s[0], next_s[1]])))\n",
        "                next_s, r = env.step([i,j], action_space[policy[i,j]])\n",
        "                new_v = r + (gamma * state_value[next_s[0], next_s[1]])\n",
        "                # check termination\n",
        "                # set is_terminal = False if |new_v - old_v| > threshold\n",
        "                if(abs(new_v - old_v) > threshold):\n",
        "                    is_terminal = False\n",
        "\n",
        "                \"\"\" DO NOT CHANGE BELOW \"\"\"\n",
        "                # store the updated state value for state [i, j]\n",
        "                new_state_value[i, j] = new_v\n",
        "\n",
        "        # update state value\n",
        "        state_value = new_state_value.copy()\n",
        "\n",
        "        # check termination\n",
        "        if is_terminal:\n",
        "            break\n",
        "\n",
        "    return state_value\n",
        "\n",
        "\n",
        "#################################\n",
        "# Implement policy improvement\n",
        "################################\n",
        "def policy_improvement(env, policy, state_value, gamma):\n",
        "    # set the policy improvement flag\n",
        "    policy_stable = True\n",
        "\n",
        "    # loop all states\n",
        "    for i in range(5):\n",
        "        for j in range(5):\n",
        "            \"\"\" CODE HERE \"\"\"\n",
        "            # store the old action\n",
        "            old_a = policy[i,j]\n",
        "\n",
        "            # compute a new greedy action based on the latest state value\n",
        "            new_a = None\n",
        "            new_val_array = np.array([0, 0, 0, 0])\n",
        "\n",
        "            # for k in range(4):\n",
        "            #     next_s, r = env.step([i,j], action_space[k])\n",
        "            #     new_val_array[k] = r + (gamma * state_value[next_s[0], next_s[1]])\n",
        "\n",
        "            # new_val = np.argwhere(new_val_array == new_val_array.max()).reshape(-1)\n",
        "            # # new_a = np.random.choice(new_val, 1).item()\n",
        "            # new_a = new_val[0]\n",
        "\n",
        "            new_val_arr = np.array([0, 0, 0, 0])\n",
        "            k=0\n",
        "            for act in env.action_space.keys():\n",
        "                next_s, r = env.step([i,j], act)\n",
        "                new_val_arr[k] = r + (gamma * state_value[next_s[0], next_s[1]])\n",
        "                k = k+1\n",
        "\n",
        "            new_val = np.argwhere(new_val_arr==new_val_arr.max()).reshape(-1)\n",
        "            # print(new_val)\n",
        "            new_a = np.random.choice(new_val,1)\n",
        "\n",
        "            # print(new_val)\n",
        "\n",
        "            # check if the policy is stable\n",
        "            if(new_a != old_a):\n",
        "                policy_stable = False\n",
        "\n",
        "            \"\"\" DO NOT CHANGE BELOW\"\"\"\n",
        "            # update the policy with the new greedy policy\n",
        "            policy[i, j] = new_a\n",
        "    # print(\"Hello\\n\", policy_stable)\n",
        "\n",
        "    return policy.astype(int), policy_stable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "f5bb9e51",
      "metadata": {
        "scrolled": false,
        "id": "f5bb9e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "afc1e7f6-c975-4b44-b060-987812ec226c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-ecd2c2754dd5>:113: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  policy[i, j] = new_a\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-472c8592b5b0>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# policy evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mstate_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# policy improvement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-ecd2c2754dd5>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(env, policy, threshold, gamma)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m#     next_s, r = env.step([i,j], action_space[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m#     new_v = new_v + (0.25 * (r + (gamma*state_value[next_s[0], next_s[1]])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mnew_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# check termination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-08294b01e5c7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# compute the next state and reward using the dynamics function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# return the next state and the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-08294b01e5c7>\u001b[0m in \u001b[0;36mdynamics_func\u001b[0;34m(self, s_arr, a_arr)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# compute the next state position and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mnext_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_arr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Actions that would take the agent off the grid leave its location unchanged, but also result in a reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# of -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"DO NOT CHANGE: it is used to run the policy iteration above\"\"\"\n",
        "# run policy iteration (DO NOT CHANGE)\n",
        "my_grid = GridWorld()\n",
        "my_grid.reset()\n",
        "\n",
        "# threshold and gamma\n",
        "threshold, gamma = 1e-3, 0.9\n",
        "\n",
        "# initialize a policy\n",
        "policy = np.random.randint(low=0, high=4, size=(5, 5))\n",
        "\n",
        "# run policy iteration\n",
        "while True:\n",
        "    # policy evaluation\n",
        "    state_value = policy_evaluation(my_grid, policy, threshold, gamma)\n",
        "\n",
        "    # policy improvement\n",
        "    policy, policy_stable = policy_improvement(my_grid, policy, state_value, gamma)\n",
        "\n",
        "    # check if policy is stable\n",
        "    if policy_stable:\n",
        "        break\n",
        "\n",
        "# print the optimal state value\n",
        "print_optimal_state_value(state_value)\n",
        "\n",
        "# print the optimal policy\n",
        "print_optimal_policy(state_value, my_grid, gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e20f9fb7",
      "metadata": {
        "id": "e20f9fb7"
      },
      "source": [
        "# Q5 - Jack's Car Rental Problem\n",
        "\n",
        "This is a challenging problem. We provide the implementation of the original problem. You can directly use it\n",
        "to solve the question (a); However, to solve question (b), the only thing you need to do is to modify the following method function in the class based on the descriptions in (b).\n",
        "\n",
        "\n",
        "    **compute_reward_modified()**\n",
        "\n",
        "We highly recommend you to read the implementation in more details because you will need it to complete the policy iteration below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a58611",
      "metadata": {
        "scrolled": true,
        "id": "a2a58611"
      },
      "outputs": [],
      "source": [
        "# Implementation the Jack's Car Rental problem in Example 4.2 (DO NOT CHANGE)\n",
        "class JackCarRental(object):\n",
        "    def __init__(self, max_car_num=20):\n",
        "        # Define the state space (the state is the number of cars at each location at the end of the day)\n",
        "        self.max_num = max_car_num\n",
        "        # Define the table for the state value\n",
        "        self.state_value = np.zeros((self.max_num + 1, self.max_num + 1))\n",
        "\n",
        "        # Define the action space (the number of the cars that Jack plans to move from lot1 to lot2)\n",
        "        # For example, if a >= 0, moving |a| cars from lot1 to lot2; Otherwise, moving |a| cars\n",
        "        # from lot2 to lot1.\n",
        "        self.action_space = np.linspace(start=-5, stop=5, num=11, dtype=int)\n",
        "\n",
        "        # Define a truncated version of the Poisson distribution\n",
        "        # Note that, Poisson distribution is a discrete distribution for infinite possible values.\n",
        "        # In this problem, we only want to consider the values with probability bigger than a threshold.\n",
        "        # We ignore other values because their probabilities of happening are too small to be considered.\n",
        "        self.truncated_threshold = 1e-6\n",
        "        self.request_distribution_lot1 = TruncatedPoissonDistribution(3, self.truncated_threshold)\n",
        "        self.return_distribution_lot1 = TruncatedPoissonDistribution(3, self.truncated_threshold)\n",
        "        self.request_distribution_lot2 = TruncatedPoissonDistribution(4, self.truncated_threshold)\n",
        "        self.return_distribution_lot2 = TruncatedPoissonDistribution(2, self.truncated_threshold)\n",
        "\n",
        "        # Pre-compute the transition function p(s'|s, a) and r(s, a) and store them as numpy arrays\n",
        "        self.p_lot1, self.r_lot1 = self.open_to_close(self.request_distribution_lot1, self.return_distribution_lot1)\n",
        "        self.p_lot2, self.r_lot2 = self.open_to_close(self.request_distribution_lot2, self.return_distribution_lot2)\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, s, a):\n",
        "        \"\"\" Step function: it returns the next state given the current state and the action\n",
        "        Args:\n",
        "            s (list of two integers): a list variable contains two integers that represent the number of the cars\n",
        "                                      at lot1 and lot2 by the end of the day, respectively.\n",
        "            a (int): an integer variable represents the number of cars that Jack wants to move from lot1 to lot2.\n",
        "                     Note, if a >= 0, Jack moves |a| cars from lot1 to lot2; Otherwise, he moves the cars conversely.\n",
        "        Returns:\n",
        "            next_s (list of two integers): a list variable contains two integers that represent the number of the cars\n",
        "                                           at lot1 and lot2 by the end of the day, respectively.\n",
        "            reward (float): a float variable represents the rewards by taking action a at state s. Note that: it\n",
        "                            aggregates the cost of moving cars and gain of renting out cars.\n",
        "            prob (float) between (0, 1): a float variable represents the probability of transit to next_s from s by\n",
        "                                         taking a. In other words, prob = p(next_s|s, a)\n",
        "        \"\"\"\n",
        "        \"\"\"Business ends\"\"\"\n",
        "        # Note the state is the number of cars at the two lots by the end of the day\n",
        "        car_num_lot1, car_num_lot2 = s\n",
        "\n",
        "        \"\"\"Car moving overnight\"\"\"\n",
        "        # We define the action as moving cars from lot1 to lot2:\n",
        "        # if a >= 0: moving cars from lot1 to lot2; Otherwise moving car from lot2 to lot1\n",
        "        # update the cars in both slots (overnight)\n",
        "        move_car_num = self.move_car(s, a)\n",
        "        car_num_lot1_after_move = car_num_lot1 - move_car_num\n",
        "        car_num_lot2_after_move = car_num_lot2 + move_car_num\n",
        "\n",
        "        \"\"\"Business starts\"\"\"\n",
        "        # compute the requests for lot1 and lot2\n",
        "        request_car_lot1_num = self.request_distribution_lot1.sample()\n",
        "        request_car_lot2_num = self.request_distribution_lot2.sample()\n",
        "\n",
        "        # compute the return for lot1 and lot2\n",
        "        return_car_lot1_num = self.return_distribution_lot1.sample()\n",
        "        return_car_lot2_num = self.return_distribution_lot2.sample()\n",
        "\n",
        "        \"\"\"Business ends\"\"\"\n",
        "        # compute the number of cars at each lot\n",
        "        car_num_lot1_new = self.update_car_num(car_num_lot1_after_move, request_car_lot1_num, return_car_lot1_num)\n",
        "        car_num_lot2_new = self.update_car_num(car_num_lot2_after_move, request_car_lot2_num, return_car_lot2_num)\n",
        "\n",
        "        # compute the p(s'|s, a)\n",
        "        prob = self.p_lot1[car_num_lot1_after_move][car_num_lot1_new] * \\\n",
        "            self.p_lot2[car_num_lot2_after_move][car_num_lot2_new]\n",
        "\n",
        "        # compute the reward\n",
        "        reward = self.compute_reward(move_car_num, car_num_lot1_after_move, car_num_lot2_after_move)\n",
        "\n",
        "        # compute the next state\n",
        "        next_s = [car_num_lot1_new, car_num_lot2_new]\n",
        "\n",
        "        # the return is the next_state, reward (cost + expected incomes from lot1 and lot2), p(s'|s, a)\n",
        "        return next_s, reward, prob\n",
        "\n",
        "    def compute_reward(self, moved_cars, car_num_lot1, car_num_lot2):\n",
        "        \"\"\" Compute the total reward between two consecutive days, it contains the following stages:\n",
        "                - When business ends, Jack moves |moved_cars| from lot1 to lot2. The reward is negative and equals to\n",
        "                  -2 * abs(moved_cars). Note, moved_cars is the actual movable cars.\n",
        "                - After car moving, when business starts the next day, requests come independently in lot1 and lot2.\n",
        "                  Therefore, the reward is positive. We aggregate the reward from lot1 and lot2 given the number of cars\n",
        "                  at lot1 and lot2 after car moving.\n",
        "            Args:\n",
        "                moved_cars (int): number of cars that are actually moved from lot1 to lot2 by Jack.\n",
        "                                  Note: if moved_cars >=0, Jack moves car from lot1 to lot2; Otherwise, he moves\n",
        "                                  from lot2 to lot1.\n",
        "                car_num_lot1 (int): number of cars at lot1 after the car moving\n",
        "                car_num_lot2 (int): number of cars at lot2 after the car moving\n",
        "        \"\"\"\n",
        "        # compute the cost + expected reward at lot1 + expected reward at lot2\n",
        "        return -2 * abs(moved_cars) + self.r_lot1[car_num_lot1] + self.r_lot2[car_num_lot2]\n",
        "\n",
        "    def compute_reward_modified(self, moved_cars, car_num_lot1, car_num_lot2):\n",
        "        \"\"\" Besides the cost of moving cars between lot1 and lot2, the reward function is adjusted based on the\n",
        "            following modifications:\n",
        "                - One car can be moved from lot1 to lot2 for free.\n",
        "                - If num_car_after_move > 10, additional $4 are charged at each time lot regardless of how many cars\n",
        "                - $2 per car moving fee\n",
        "                - $10 per car renting income\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\" CODE HERE \"\"\"\n",
        "        \"\"\" Modified the reward function based on the description in (b)\"\"\"\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def open_to_close(self, request_distribution, return_distribution):\n",
        "        \"\"\" Considering one lot (lot1 or lot2), the possible number of cars when business opens is [0, 25]\n",
        "            25 = 20 (maximal number when business closes) + (5 cars moved by Jack).\n",
        "            When business ends, the possible number of cars is [0, 20].\n",
        "\n",
        "            We consider another form of transition probability p(s'|s, a) and reward function r(s, a).\n",
        "\n",
        "            Since the dynamics for the overnight car moving is deterministic (i.e., by default, Jack moves certain\n",
        "            amount of cars between lot1 and lot2 once), the stochasticity comes from the request/return from the\n",
        "            customers. Therefore, we can re-write the transition function as:\n",
        "                p(lot_end|lot_end_yesterday, a) = p(lot_end|lot_open) * p(lot_open|lot_end_yesterday,a) = p(lot_end|lot_open),\n",
        "                where y is the number of cars at one lot after Jack moves the cars p(lot_open|lot_end_yesterday,a) = 1.\n",
        "\n",
        "            The stochasticity mainly comes from the second stage we call it which is modeled in this open_to_close function.\n",
        "        Args:\n",
        "            request_distribution (TruncatedPoissonDistribution): a truncated Possion distribution under a threshold for requesting\n",
        "            return_distribution (TruncatedPoissonDistribution): a truncated Possion distribution under a threshold for returning\n",
        "\n",
        "        Returns:\n",
        "            p_arr (np.array): it stores p(car_num_end|car_num_after_move) for all possible combinations. The shape is\n",
        "                              26 x 21.\n",
        "            r_arr (np.array): it stores the expected reward for each car_num_after_move in [0, 25]. It also equals to\n",
        "                              r(s, a) since the moving action is deterministic.\n",
        "        \"\"\"\n",
        "        # Numpy array to store the factorized transition function\n",
        "        # state_open take integer values from [0, 25]\n",
        "        # state_end take integer values from [0, 20]\n",
        "        # p(s_end|s_end_yesterday, a) = p(s_end|s_open) since the moving action is deterministic.\n",
        "        p_arr = np.zeros((26, 21))\n",
        "        # Numpy array to store the reward r(s, a) = r(s_open) because the moving the deterministic\n",
        "        r_arr = np.zeros(26)\n",
        "\n",
        "        for request_num, request_prob in request_distribution:\n",
        "            # the positive rewards only come from the request\n",
        "            # and the initial number of cars will matter\n",
        "            # since the reward contains stochasticity, we compute the expectation\n",
        "            # of the reward for each possible state at one parking lot\n",
        "            for n in range(26):\n",
        "                r_arr[n] += request_prob * 10 * min(n, request_num)\n",
        "\n",
        "            # compute the transition function\n",
        "            for return_num, return_prob in return_distribution:\n",
        "                for n in range(26):\n",
        "                    # compute the new n by the end of the day\n",
        "                    new_n = self.update_car_num(n, request_num, return_num)\n",
        "                    # update the probability incrementally\n",
        "                    p_arr[n][new_n] += request_prob * return_prob\n",
        "\n",
        "        return p_arr, r_arr\n",
        "\n",
        "    @staticmethod\n",
        "    def update_car_num(cars_num, request_cars_num, return_cars_num):\n",
        "        \"\"\" Update the number of cars in lot1/lot2 after requesting and returning\n",
        "        Args:\n",
        "            cars_num (int): Number of the cars at lot1/lot2 when the business starts\n",
        "            request_cars_num (int): Number of the cars requested by the customers\n",
        "            return_cars_num (int): Number of the cars returned by the customers\n",
        "        \"\"\"\n",
        "        return min(max(cars_num - request_cars_num, 0) + return_cars_num, 20)\n",
        "\n",
        "    @staticmethod\n",
        "    def move_car(state, action):\n",
        "        \"\"\" Compute the actual number of cars to be moved from lot1 to lot2\n",
        "        Args:\n",
        "            state (list): a list variable contains the number of cars (int) at lot1 and lot2.\n",
        "            action (int): an integer between [-5, 5]\n",
        "        \"\"\"\n",
        "        # Note: action is the number of cars Jack wants to move from lot1 to lot2. But the actual number should follow\n",
        "        # the condition [-1 * car_num_lot2, car_num_lot1]\n",
        "        return int(np.clip(action, a_min=-state[1], a_max=state[0]))\n",
        "\n",
        "\n",
        "# Implement the truncated Poisson distribution\n",
        "class TruncatedPoissonDistribution(object):\n",
        "    def __init__(self, mean, threshold):\n",
        "        # Check the validation of the mean and threshold\n",
        "        assert isinstance(mean, int), mean > 0\n",
        "        assert 0 < threshold < 1.0\n",
        "\n",
        "        # Store the mean of the Poisson distribution\n",
        "        self.mean = mean\n",
        "        # Store the threshold of p(k). Only k with p(k) > threshold would be considered\n",
        "        self.truncated_threshold = threshold\n",
        "\n",
        "        # create a list to store the selected discrete values and their probabilities\n",
        "        self.truncated_values, self.truncated_prob = self.truncate_poisson()\n",
        "\n",
        "    def truncate_poisson(self):\n",
        "        \"\"\" Create the truncated Poisson distribution with a finite set of ks and a normalized probabilities.\n",
        "        \"\"\"\n",
        "        # Create original Poisson distribution\n",
        "        distribution = poisson(self.mean)\n",
        "\n",
        "        # Find the maximal k to be considered\n",
        "        max_k = 0\n",
        "        while distribution.pmf(max_k) > self.truncated_threshold:\n",
        "            max_k += 1\n",
        "\n",
        "        # Create the truncated value list\n",
        "        value_list = list(np.linspace(start=0, stop=max_k, num=max_k+1, dtype=int))\n",
        "\n",
        "        # Create the probability\n",
        "        prob_list = [distribution.pmf(k) for k in value_list]\n",
        "\n",
        "        # Normalize the probability for the truncated values\n",
        "        prob_list = (prob_list / np.sum(prob_list)).tolist()\n",
        "\n",
        "        return value_list, prob_list\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\" Sample a k using the new truncated values and probabilities.\n",
        "        \"\"\"\n",
        "        return np.random.choice(a=self.truncated_values, p=self.truncated_prob)\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\" Iterate all ks and its corresponding probabilities\n",
        "        \"\"\"\n",
        "        return zip(self.truncated_values, self.truncated_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a858cfe2",
      "metadata": {
        "scrolled": true,
        "id": "a858cfe2"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE: it is used to visualize the policy\"\"\"\n",
        "def plot_policy(policy, title):\n",
        "    policy = np.flip(policy, axis=0)\n",
        "\n",
        "    car_num_lot1 = list(np.linspace(start=20, stop=0, num=21, dtype=int))\n",
        "    car_num_lot2 = list(np.linspace(start=0, stop=20, num=21, dtype=int))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(policy, cmap=\"RdGy\")\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries\n",
        "    ax.set_xticks(np.arange(len(car_num_lot2)), labels=car_num_lot2)\n",
        "    ax.set_yticks(np.arange(len(car_num_lot1)), labels=car_num_lot1)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(car_num_lot1)):\n",
        "        for j in range(len(car_num_lot2)):\n",
        "            text = ax.text(j, i, policy[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    ax.set_title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375301d9",
      "metadata": {
        "id": "375301d9"
      },
      "source": [
        "## Q5 - (a): Implement the policy iteration algorithm and generate the Figure 4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7610120",
      "metadata": {
        "scrolled": true,
        "id": "b7610120"
      },
      "outputs": [],
      "source": [
        "def compute_expected_return(state, action, env, gamma, state_value):\n",
        "    \"\"\" Function is used to compute the expected return given s and a.\n",
        "        It returns the value = sum_s' p(s'|s, a)[r(s, a) + gamma * V(s')]\n",
        "    Args:\n",
        "        state (list): state (i.e, s)\n",
        "        action (int): action (i.e, a)\n",
        "        env: the jack's car rental environment\n",
        "        gamma (float): discount factor\n",
        "        state_value (numpy.array): current state value\n",
        "    \"\"\"\n",
        "    # compute the number of car to move\n",
        "    car_num_to_move = env.move_car(state, action)\n",
        "\n",
        "    # compute the state after moving\n",
        "    state_after_move = [state[0] - car_num_to_move, state[1] + car_num_to_move]\n",
        "\n",
        "    # compute the expectation using the model of the env\n",
        "    # new_v(s) <--- \\sum_{s'} p(s'|s, a)[r(s, a) + \\gamma * old_v(s')]\n",
        "    new_v = 0\n",
        "    # The space of |s'| = 21 x 21. Because each lot will have the number of cars within [0, 20].\n",
        "    for n_1 in range(21):\n",
        "        for n_2 in range(21):\n",
        "            # compute the transition probability p(s' | s, a) for one possible s'\n",
        "            # p(s'|s, a) = p([n1, n2] | s, a) = p([n1, n2] | s_after_move) * p(s_after_move|s, a)\n",
        "            #            = p([n1, n2] | s_after_move) since p(s_after_move|s,a) = 1 given a is deterministic.\n",
        "            #            = p(n1 | s_after_move) * p(n2 | s_after_move) since lot1 and lot2 evolves independently.\n",
        "            #            = p(n1 | s_after_move_lot1) * p(n2 | s_after_move_lot2)\n",
        "            # prob = p(s'|s, a)\n",
        "            prob = env.p_lot1[state_after_move[0]][n_1] * env.p_lot2[state_after_move[1]][n_2]\n",
        "\n",
        "            # compute the reward = cost + expected rewards for lot1 and lot2 given the state after moving the car.\n",
        "            # reward = r(s, a)\n",
        "            reward = env.compute_reward(moved_cars=car_num_to_move,\n",
        "                                        car_num_lot1=state_after_move[0],\n",
        "                                        car_num_lot2=state_after_move[1])\n",
        "\n",
        "            \"\"\" CODE HERE \"\"\"\n",
        "            \"\"\" incrementally compute the new_v for the state-action pair (i.e., the expected return of (s, a))\"\"\"\n",
        "\n",
        "    return new_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671a473b",
      "metadata": {
        "scrolled": true,
        "id": "671a473b"
      },
      "outputs": [],
      "source": [
        "# Implement the policy evaluation\n",
        "def policy_evaluation(state_value, policy, env, threshold, gamma):\n",
        "    # iteration counter\n",
        "    iter_counter = 0\n",
        "\n",
        "    # check termination\n",
        "    while True:\n",
        "        # iteration counter\n",
        "        iter_counter += 1\n",
        "\n",
        "        # assume the current iteration should be terminated\n",
        "        is_terminal = True\n",
        "\n",
        "        # create a numpy array to store the new state value\n",
        "        new_state_value = np.zeros_like(state_value)\n",
        "\n",
        "        # loop all valid states\n",
        "        for i in range(21):\n",
        "            for j in range(21):\n",
        "                \"\"\" CODE HERE \"\"\"\n",
        "                \"\"\" Complete the update part of the policy evaluation\"\"\"\n",
        "                \"\"\" Please use the compute_expected_return function above to update the new state value\"\"\"\n",
        "                # state (e.g. [i, j])\n",
        "\n",
        "                # store the old V value\n",
        "\n",
        "                # get the action from the policy\n",
        "\n",
        "                # compute the expected return\n",
        "\n",
        "                # compute the error for this state\n",
        "                # note that: is_terminal is True if and only if all states has value estimation error < the threshold\n",
        "\n",
        "                \"\"\" DO NOT CHANGE BELOW\"\"\"\n",
        "                # store the updated state value\n",
        "                new_state_value[i, j] = new_v\n",
        "\n",
        "        # update the state value table\n",
        "        state_value = new_state_value.copy()\n",
        "\n",
        "        # check the termination\n",
        "        if is_terminal:\n",
        "            break\n",
        "\n",
        "    return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48603eb",
      "metadata": {
        "scrolled": true,
        "id": "a48603eb"
      },
      "outputs": [],
      "source": [
        "# Implement the policy improvement\n",
        "def policy_improvement(state_value, policy, env, gamma):\n",
        "    # assume the policy is already stable\n",
        "    is_stable = True\n",
        "    # loop the state space\n",
        "    for i in range(21):\n",
        "        for j in range(21):\n",
        "            \"\"\" CODE HERE \"\"\"\n",
        "            \"\"\" Complete the update part of the policy improvement\"\"\"\n",
        "            \"\"\" Still please use the compute_expected_return function above to compute the new state value\"\"\"\n",
        "            # state\n",
        "\n",
        "            # obtain the old action\n",
        "\n",
        "            # compute a new greedy action\n",
        "\n",
        "            # check if the policy is stable at state [i, j]\n",
        "\n",
        "            \"\"\" DO NOT CHANGE BELOW \"\"\"\n",
        "            # update the policy\n",
        "            policy[i, j] = new_a\n",
        "\n",
        "    return policy, is_stable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1b3234",
      "metadata": {
        "scrolled": true,
        "id": "4b1b3234"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK: it is the policy iteration\"\"\"\n",
        "def run_policy_iteration(env, threshold, gamma):\n",
        "    # initialize the policy\n",
        "    policy = np.zeros((21, 21), dtype=int)\n",
        "\n",
        "    # initialize the state value\n",
        "    state_value = np.zeros((21, 21))\n",
        "\n",
        "    # run policy iteration\n",
        "    policy_iter_counter = 0\n",
        "\n",
        "    # save the policies in the iteration\n",
        "    results_list = []\n",
        "\n",
        "    # start policy iteration\n",
        "    while True:\n",
        "        # print info\n",
        "        print(f\"======================================\")\n",
        "        print(f\"==   Policy iteration = {policy_iter_counter}\")\n",
        "        print(f\"======================================\")\n",
        "\n",
        "        # policy evaluation\n",
        "        print(f\"Iter {policy_iter_counter}: Policy evaluation starts.\")\n",
        "        state_value = policy_evaluation(state_value, policy, env, threshold, gamma)\n",
        "        print(f\"Iter {policy_iter_counter}: Policy evaluation ends.\")\n",
        "\n",
        "        # policy improvement\n",
        "        print(f\"Iter {policy_iter_counter}: Policy improvement starts.\")\n",
        "        policy, is_stable = policy_improvement(state_value, policy, env, gamma)\n",
        "        print(f\"Iter {policy_iter_counter}: Policy improvement ends.\")\n",
        "\n",
        "        # save to the list\n",
        "        results_list.append({\"state_value\": state_value.copy(),\n",
        "                             \"policy\": policy.copy(),\n",
        "                             \"title\": f\"Iteration = {policy_iter_counter}\"})\n",
        "\n",
        "        if is_stable:\n",
        "            break\n",
        "        else:\n",
        "            policy_iter_counter += 1\n",
        "\n",
        "    print(\"======================\")\n",
        "    print(\"Policy iteration ends.\")\n",
        "\n",
        "    return results_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c615ef9d",
      "metadata": {
        "scrolled": false,
        "id": "c615ef9d"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK: it is used to run the policy iteration in the Jack's Car Rental Problem\"\"\"\n",
        "\"\"\"IT WILL TOOK SOME TIME. PLEASE BE PATIENT.\"\"\"\n",
        "# run the policy iteration\n",
        "env = JackCarRental()\n",
        "env.reset()\n",
        "\n",
        "# set the threshold for policy evaluation\n",
        "threshold = 1e-3\n",
        "\n",
        "# set the gamma\n",
        "gamma = 0.9\n",
        "\n",
        "# run the policy iteration\n",
        "results_list = run_policy_iteration(env, threshold, gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f3c85ae",
      "metadata": {
        "scrolled": false,
        "id": "7f3c85ae"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK: it is used to visualize the policy for each iteration\"\"\"\n",
        "# plot the final policy\n",
        "for res in results_list:\n",
        "    policy = res['policy']\n",
        "    title = res['title']\n",
        "    plot_policy(policy, title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb5bcd8",
      "metadata": {
        "id": "7bb5bcd8"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK: it is used to visualize the value function\"\"\"\n",
        "#Plot the value function\n",
        "def plot_optimal_values(state_value):\n",
        "    x = np.linspace(0, 20, 21)\n",
        "    y = np.linspace(0, 20, 21)\n",
        "\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = state_value\n",
        "\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                    cmap='viridis', edgecolor='none')\n",
        "    ax.set_title('State Value')\n",
        "\n",
        "state_value = results_list[0]['state_value']\n",
        "plot_optimal_values(state_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c46957",
      "metadata": {
        "id": "d3c46957"
      },
      "source": [
        "## Q5 - (b): Apply the implemented policy iteration on the modified Jack's car rental problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7983167",
      "metadata": {
        "scrolled": true,
        "id": "e7983167"
      },
      "outputs": [],
      "source": [
        "def compute_expected_return(state, action, env, gamma, state_value):\n",
        "    \"\"\" Function is used to compute the expected return given s and a.\n",
        "        It returns the value = sum_s' p(s'|s, a)[r(s, a) + gamma * V(s')]\n",
        "    Args:\n",
        "        state (list): state (i.e, s = [i, j])\n",
        "        action (int): action (i.e, a = 5)\n",
        "        env: the jack's car rental environment\n",
        "        gamma (float): discount factor\n",
        "        state_value (numpy.array): current state value\n",
        "    \"\"\"\n",
        "    # compute the number of car to move\n",
        "    car_num_to_move = env.move_car(state, action)\n",
        "\n",
        "    # compute the state after moving\n",
        "    state_after_move = [state[0] - car_num_to_move, state[1] + car_num_to_move]\n",
        "\n",
        "    # compute the expectation using the model of the env\n",
        "    # new_v(s) <--- \\sum_{s'} p(s'|s, a)[r(s, a) + \\gamma * old_v(s')]\n",
        "    new_v = 0\n",
        "    # The space of |s'| = 21 x 21. Because each lot will have the number of cars within [0, 20].\n",
        "    for n_1 in range(21):\n",
        "        for n_2 in range(21):\n",
        "            # compute the transition probability p(s' | s, a) for one possible s'\n",
        "            # p(s'|s, a) = p([n1, n2] | s, a) = p([n1, n2] | s_after_move) * p(s_after_move|s, a)\n",
        "            #            = p([n1, n2] | s_after_move) since p(s_after_move|s,a) = 1 given a is deterministic.\n",
        "            #            = p(n1 | s_after_move) * p(n2 | s_after_move) since lot1 and lot2 evolves independently.\n",
        "            #            = p(n1 | s_after_move_lot1) * p(n2 | s_after_move_lot2)\n",
        "            # prob = p(s'|s, a)\n",
        "            prob = env.p_lot1[state_after_move[0]][n_1] * env.p_lot2[state_after_move[1]][n_2]\n",
        "\n",
        "            # compute the reward = cost + expected rewards for lot1 and lot2 given the state after moving the car.\n",
        "            # reward = r(s, a)\n",
        "            reward = env.compute_reward_modified(moved_cars=car_num_to_move,\n",
        "                                                 car_num_lot1=state_after_move[0],\n",
        "                                                 car_num_lot2=state_after_move[1])\n",
        "\n",
        "            \"\"\" CODE HERE \"\"\"\n",
        "            \"\"\" incrementally compute the state value for the state-action pair (i.e., the expected return of (s, a))\"\"\"\n",
        "\n",
        "    return new_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67637640",
      "metadata": {
        "scrolled": true,
        "id": "67637640"
      },
      "outputs": [],
      "source": [
        "# Implement the policy evaluation\n",
        "def policy_evaluation(state_value, policy, env, threshold, gamma):\n",
        "    # iteration counter\n",
        "    iter_counter = 0\n",
        "\n",
        "    # check termination\n",
        "    while True:\n",
        "        # iteration counter\n",
        "        iter_counter += 1\n",
        "\n",
        "        # assume the current iteration should be terminated\n",
        "        is_terminal = True\n",
        "\n",
        "        # create a numpy array to store the new state value\n",
        "        new_state_value = np.zeros_like(state_value)\n",
        "\n",
        "        # loop all valid states\n",
        "        for i in range(21):\n",
        "            for j in range(21):\n",
        "                \"\"\" CODE HERE \"\"\"\n",
        "                \"\"\" Complete the update part of the policy evaluation\"\"\"\n",
        "                \"\"\" Please use the compute_expected_return function above to compute the new state value\"\"\"\n",
        "                # state (e.g., [i, j])\n",
        "\n",
        "                # store the old V value\n",
        "\n",
        "                # get the action from the policy\n",
        "\n",
        "                # compute the expected return\n",
        "\n",
        "                # compute the error for this state\n",
        "                # note that: is_terminal is True if and only if all states has value estimation error < the threshold\n",
        "\n",
        "                \"\"\" DO NOT CHANGE BELOW\"\"\"\n",
        "                # store the updated state value\n",
        "                new_state_value[i, j] = new_v\n",
        "\n",
        "        # update the state value table\n",
        "        state_value = new_state_value.copy()\n",
        "\n",
        "        # check the termination\n",
        "        if is_terminal:\n",
        "            break\n",
        "\n",
        "    return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9dec0d",
      "metadata": {
        "scrolled": true,
        "id": "fa9dec0d"
      },
      "outputs": [],
      "source": [
        "# Implement the policy improvement\n",
        "def policy_improvement(state_value, policy, env, gamma):\n",
        "    # assume the policy is already stable\n",
        "    is_stable = True\n",
        "    # loop the state space\n",
        "    for i in range(21):\n",
        "        for j in range(21):\n",
        "            \"\"\" CODE HERE \"\"\"\n",
        "            \"\"\" Complete the update part of the policy improvement\"\"\"\n",
        "            \"\"\" Still please use the compute_expected_return function above to compute the new state value\"\"\"\n",
        "            # state\n",
        "\n",
        "            # obtain the old action\n",
        "\n",
        "            # compute a new greedy action\n",
        "\n",
        "            # check if the policy is stable at state [i, j]\n",
        "\n",
        "            \"\"\" DO NOT CHANGE BELOW \"\"\"\n",
        "            # update the policy\n",
        "            policy[i, j] = new_a\n",
        "\n",
        "    return policy, is_stable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d126fd26",
      "metadata": {
        "scrolled": true,
        "id": "d126fd26"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE BELOW\"\"\"\n",
        "def run_policy_iteration(env, threshold, gamma):\n",
        "    # initialize the policy\n",
        "    policy = np.zeros((21, 21), dtype=int)\n",
        "\n",
        "    # initialize the state value\n",
        "    state_value = np.zeros((21, 21))\n",
        "\n",
        "    # run policy iteration\n",
        "    policy_iter_counter = 0\n",
        "\n",
        "    # save the policies in the iteration\n",
        "    results_list = []\n",
        "\n",
        "    # start policy iteration\n",
        "    while True:\n",
        "        # print info\n",
        "        print(f\"======================================\")\n",
        "        print(f\"==   Policy iteration = {policy_iter_counter}\")\n",
        "        print(f\"======================================\")\n",
        "\n",
        "        # policy evaluation\n",
        "        print(f\"Iter {policy_iter_counter}: Policy evaluation starts.\")\n",
        "        state_value = policy_evaluation(state_value, policy, env, threshold, gamma)\n",
        "        print(f\"Iter {policy_iter_counter}: Policy evaluation ends.\")\n",
        "\n",
        "        # policy improvement\n",
        "        print(f\"Iter {policy_iter_counter}: Policy improvement starts.\")\n",
        "        policy, is_stable = policy_improvement(state_value, policy, env, gamma)\n",
        "        print(f\"Iter {policy_iter_counter}: Policy improvement ends.\")\n",
        "\n",
        "        # save to the list\n",
        "        results_list.append({\"state_value\": state_value.copy(),\n",
        "                             \"policy\": policy.copy(),\n",
        "                             \"title\": f\"Iteration = {policy_iter_counter}\"})\n",
        "\n",
        "        if is_stable:\n",
        "            break\n",
        "        else:\n",
        "            policy_iter_counter += 1\n",
        "\n",
        "    print(\"======================\")\n",
        "    print(\"Policy iteration ends.\")\n",
        "\n",
        "    return results_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e2c720",
      "metadata": {
        "scrolled": true,
        "id": "61e2c720"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK\"\"\"\n",
        "\"\"\"IT WILL TOOK SOME TIME. PLEASE BE PATIENT.\"\"\"\n",
        "# run the policy iteration\n",
        "env = JackCarRental()\n",
        "env.reset()\n",
        "\n",
        "# set the threshold for policy evaluation\n",
        "threshold = 1e-3\n",
        "\n",
        "# set the gamma\n",
        "gamma = 0.9\n",
        "\n",
        "# run the policy iteration\n",
        "results_list = run_policy_iteration(env, threshold, gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6df76e",
      "metadata": {
        "scrolled": false,
        "id": "bd6df76e"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK\"\"\"\n",
        "# plot the final policy\n",
        "for res in results_list:\n",
        "    policy = res['policy']\n",
        "    title = res['title']\n",
        "    plot_policy(policy, title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f0e082",
      "metadata": {
        "scrolled": true,
        "id": "95f0e082"
      },
      "outputs": [],
      "source": [
        "\"\"\"DO NOT CHANGE THIS BLOCK\"\"\"\n",
        "# Plot the value function\n",
        "def plot_optimal_values(state_value):\n",
        "    x = np.linspace(0, 20, 21)\n",
        "    y = np.linspace(0, 20, 21)\n",
        "\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = state_value\n",
        "\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                    cmap='viridis', edgecolor='none')\n",
        "    ax.set_title('State Value')\n",
        "\n",
        "state_value = results_list[0]['state_value']\n",
        "plot_optimal_values(state_value)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}