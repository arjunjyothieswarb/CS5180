{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d815c0f",
   "metadata": {},
   "source": [
    "# Ex8 Q4: Policy-gradient methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace42bed",
   "metadata": {},
   "source": [
    "## Four Rooms environment\n",
    "\n",
    "In the question, we will implement several policy-gradient methods and apply them once again on our favorite domain, Four Rooms. The environment is implemented below in a Gymnasium-like interface. Code for plotting learning curves with confidence bands is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3785f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # The grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # Observation (state) space consists of all empty cells\n",
    "        # To improve interpretability, we flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # Action space\n",
    "        self.action_movement = {0: np.array([0, 1]),  # up\n",
    "                                1: np.array([0, -1]),  # down\n",
    "                                2: np.array([-1, 0]),  # left\n",
    "                                3: np.array([1, 0])}  # right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # Goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # Wall locations\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, with a timeout of 459 steps\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # Tracking variables during a single episode\n",
    "        self.agent_location = None  # Track the agent's location in one episode.\n",
    "        self.action = None  # Track the agent's action\n",
    "        self.t = 0  # Track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates ((row, col), origin is top left)\n",
    "        to the Four Rooms coordinates ((x, y), origin is bottom left)\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): List variable consisting of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): List variable consisting of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # Reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # Reset the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int): Int variable (i.e., 0 for \"up\"). See self.action_movement above for more details.\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == 2 or action == 3:\n",
    "                action = np.random.choice([0, 1], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([2, 3], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_movement[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls; if so, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        # Compute the reward (1 iff next state is goal location)\n",
    "        reward = 1.0 if next_agent_location == self.goal_location else 0.0\n",
    "\n",
    "        # Check termination/truncation\n",
    "        # If agent reaches the goal, reward = 1, terminated = True\n",
    "        # If timeout is reached, reward = 0, truncated = True\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        if reward == 1.0:\n",
    "            terminated = True\n",
    "        elif self.t == self.max_time_steps:\n",
    "            truncated = True\n",
    "\n",
    "        # Update the agent's location, action, and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        # Plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action_movement[self.action]}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        env = FourRooms()\n",
    "        state, info = env.reset()\n",
    "\n",
    "        for _ in range(1000):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            env.render()\n",
    "            if terminated or truncated:\n",
    "                state, info = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "# Un-comment to run test function\n",
    "# FourRooms.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e1bb2-8f0b-4200-aa0e-e0ec0caed3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, *, window_size = 50):\n",
    "    \"\"\"Smooths 1-D data array using a moving average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-D numpy.array\n",
    "        window_size: Size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: A 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title, smoothing = True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): List of results arrays to plot\n",
    "        legend_list (list): List of legends corresponding to each result array\n",
    "        color_list (list): List of color corresponding to each result array\n",
    "        ylabel (string): Label of the vertical axis\n",
    "\n",
    "        Make sure the elements in the arr_list, legend_list, and color_list\n",
    "        are associated with each other correctly (in the same order).\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # Set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the vertical labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # Plot results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # Compute the standard error (of raw data, not smoothed)\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # Plot the mean\n",
    "        averages = moving_average(arr.mean(axis=0)) if smoothing else arr.mean(axis=0)\n",
    "        h, = ax.plot(range(arr.shape[1]), averages, color=color, label=legend)\n",
    "        # Plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), averages - arr_err, averages + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # Save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # Plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300440a8",
   "metadata": {},
   "source": [
    "## Part (a): REINFORCE algorithm (page 328)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78769dd7",
   "metadata": {},
   "source": [
    "### Policy network\n",
    "\n",
    "We will use and learn a neural network to represent the stochastic policy $\\pi(a|s)$. The architecture of the neural network should be:\n",
    "\n",
    "Layer 1: Linear, input size 3 (size of observation space, see below), output size 128\\\n",
    "Activation 1: ReLU\\\n",
    "Layer 2: Linear, input size 128, output size 4 (size of action space)\\\n",
    "Activation 2: Softmax (to ensure output is a probability distribution)\n",
    "\n",
    "For the input observation, instead of using the original state = $[x, y]$, we represent each state as $\\left[ \\frac{x}{10}, \\frac{y}{10}, 1 \\right]$, which normalizes the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae35b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the neural network here\n",
    "        \"\"\"\n",
    "        \n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the forward propagation\n",
    "        \"\"\"\n",
    "        \n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01422cc1",
   "metadata": {},
   "source": [
    "### REINFORCE agent with policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f083e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    def __init__(self):\n",
    "        # Create the policy network\n",
    "        self.policy_net = PolicyNet()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\" Function to derive an action given a state\n",
    "            Args:\n",
    "                state (list): [x/10, y/10, 1]\n",
    "                \n",
    "            Returns:\n",
    "                action index (int), log_prob (ln(\\pi(action|state)))\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).float().view(1, -1)\n",
    "        probs = self.policy_net(state_tensor)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86734f10",
   "metadata": {},
   "source": [
    "### REINFORCE training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d444c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgentTrainer(object):\n",
    "    def __init__(self, agent, env, params):\n",
    "        # Agent object\n",
    "        self.agent = agent\n",
    "        \n",
    "        # Environment object\n",
    "        self.env = env\n",
    "        \n",
    "        # Training parameters\n",
    "        self.params = params\n",
    "\n",
    "        # Lists to store the log probabilities and rewards for one episode\n",
    "        self.saved_log_probs = []\n",
    "        self.saved_rewards = []\n",
    "\n",
    "        # Gamma\n",
    "        self.gamma = params['gamma']\n",
    "\n",
    "        # Create the optimizer\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Use the Adam optimizer with the learning rate in params\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_state_feature(state):\n",
    "        return [state[0] / 10, state[1] / 10, 1]\n",
    "\n",
    "    def update_agent_policy_network(self):\n",
    "        # List to store the policy loss for each time step\n",
    "        policy_loss = []\n",
    "        \n",
    "        # List to store the return for each time step\n",
    "        returns = deque()\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Compute the return for each time step\n",
    "                \n",
    "                Hint: We usually compute the return from the end. Remember to append it \n",
    "                      correctly. You can use \"returns.appendleft(G)\"\n",
    "        \"\"\"\n",
    "        # Compute returns for every time step\n",
    "\n",
    "        ...\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                We now have the return and log probability for each time step.\n",
    "                Compute the `policy loss' for each time step\n",
    "                (whose gradient appears in the pseudocode).\n",
    "        \"\"\"\n",
    "        for log_prob, r in zip(self.saved_log_probs, returns):\n",
    "            # Compute the policy loss for each time step\n",
    "            ...\n",
    "\n",
    "        # Sum all the policy loss terms across all time steps\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement one step of backpropagation (gradient descent)\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "        \n",
    "        # After backpropagation, clear the data\n",
    "        del self.saved_log_probs[:]\n",
    "        del self.saved_rewards[:]\n",
    "\n",
    "        return returns[0].item(), policy_loss.item()\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\" Function to collect one episode from the environment\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the code to collect one episode. \n",
    "                \n",
    "                Specifically, we only collect the rewards and corresponding log probability, which\n",
    "                should be stored in \"self.saved_rewards\" and \"self.saved_log_probs\", respectively. \n",
    "                \n",
    "                This is because we only need the return at each time step and log probability\n",
    "                to update the weights of the policy.\n",
    "      \n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def run_train(self):        \n",
    "        # Lists to store the returns and losses during the training\n",
    "        train_returns = []\n",
    "        train_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        ep_bar = tqdm.trange(self.params['num_episodes'])\n",
    "        for ep in ep_bar:\n",
    "            \"\"\" YOUR CODE HERE:\n",
    "                    Implement the REINFORCE algorithm here.\n",
    "            \"\"\"\n",
    "            # Collect one episode\n",
    "            ...\n",
    "\n",
    "            # Update the policy using the collected episode\n",
    "            G, loss = None, None\n",
    "            \n",
    "            # Save the return and loss\n",
    "            train_returns.append(G)\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "            # Add description\n",
    "            ep_bar.set_description(f\"Episode: {ep} | Return: {G} | Loss: {loss:.2f}\")\n",
    "        \n",
    "        return train_returns, train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6b9da",
   "metadata": {},
   "source": [
    "### Evaluation of REINFORCE on Four Rooms\n",
    "\n",
    "We will run 10 trials with 10K episodes as in past assignments, which will take between 1-2 hours. If this takes too long, you can halve both the trials and number of episodes within each trial. As usual, you should set this to be much lower during development and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adf86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    my_env = FourRooms()\n",
    "\n",
    "    train_params = {\n",
    "        'num_episodes': 10000,\n",
    "        'num_trials': 10,\n",
    "        'learning_rate': 1e-3,\n",
    "        'gamma': 0.99\n",
    "    }\n",
    "\n",
    "    reinforce_returns = []\n",
    "    reinforce_losses = []\n",
    "    for _ in range(train_params['num_trials']):\n",
    "        my_agent = REINFORCEAgent()\n",
    "        my_trainer = REINFORCEAgentTrainer(my_agent, my_env, train_params)\n",
    "        returns, losses = my_trainer.run_train()\n",
    "        \n",
    "        reinforce_returns.append(returns)\n",
    "        reinforce_losses.append(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e1612",
   "metadata": {},
   "source": [
    "### Plot learning and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(reinforce_returns)], ['REINFORCE'], ['r'], 'Return', 'Return', smoothing = True)\n",
    "plot_curves([np.array(reinforce_losses)], ['REINFORCE'], ['b'], 'Loss', 'Loss', smoothing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3701b",
   "metadata": {},
   "source": [
    "## Part (b): REINFORCE with baseline (page 330)\n",
    "\n",
    "In this version of REINFORCE, we additionally learn a critic network (state-value function) to act as the baseline. In this context, the policy is sometimes referred to as the \"actor\", but the textbook reserves this terminology for the algorithm in part (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and value networks\n",
    "class REINFORCEBaselineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(REINFORCEBaselineNet, self).__init__()\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the critic network here. The architecture should be:\n",
    "                \n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "                Activation 1: ReLU\n",
    "                Layer 2: Linear, input size 128, output size 1\n",
    "                Activation 2: Identity (or none)\n",
    "        \"\"\"\n",
    "        \n",
    "        ...\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the actor network here. The architecture should be (same as before):\n",
    "                \n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "                Activation 1: ReLU\n",
    "                Layer 2: Linear, input size 128, output size 4\n",
    "                Activation 2: Softmax\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the forward propagation for both actor and critic networks\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "        return state_value, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE-with-baseline agent\n",
    "class REINFORCEBaselineAgent(object):\n",
    "    def __init__(self):\n",
    "        # Create the actor and critic networks\n",
    "        self.policy_net = REINFORCEBaselineNet()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Sample an action from the actor network, return the action and its log probability,\n",
    "        # and return the state value according to the critic network\n",
    "        state_tensor = torch.tensor(state).float().view(1, -1)\n",
    "        state_value, action_probs = self.policy_net(state_tensor)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REINFORCE-with-baseline training loop\n",
    "class REINFORCEBaselineAgentTrainer(object):\n",
    "    def __init__(self, agent, env, params):\n",
    "        # Agent object\n",
    "        self.agent = agent\n",
    "        \n",
    "        # Environment object\n",
    "        self.env = env\n",
    "        \n",
    "        # Training parameters\n",
    "        self.params = params\n",
    "\n",
    "        # Lists to store the log probabilities, state values, and rewards for one episode\n",
    "        self.saved_log_probs = []\n",
    "        self.saved_state_values = []\n",
    "        self.saved_rewards = []\n",
    "\n",
    "        # Gamma\n",
    "        self.gamma = params['gamma']\n",
    "\n",
    "        # Create the optimizer\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the Adam optimizer with the learning rate in params\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_state_feature(state):\n",
    "        return [state[0] / 10, state[1] / 10, 1]\n",
    "\n",
    "    def update_agent_policy_network(self):\n",
    "        # List to store the policy loss for each time step\n",
    "        policy_loss = []\n",
    "        \n",
    "        # List to store the value loss for each time step\n",
    "        value_loss = []\n",
    "        \n",
    "        # List to store the return for each time step\n",
    "        returns = deque()\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Compute the return for each time step\n",
    "                \n",
    "                Hint: We usually compute the return from the end. Remember to append it \n",
    "                      correctly. You can use \"returns.appendleft(G)\"\n",
    "        \"\"\"\n",
    "        # Compute returns for every time step\n",
    "\n",
    "        ...\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                We now have the return, state value, and log probability for each time step.\n",
    "                Compute the `policy loss' and `value loss' for each time step\n",
    "                (whose gradients appear in the pseudocode).\n",
    "        \"\"\"\n",
    "        for log_prob, val, r in zip(self.saved_log_probs, self.saved_state_values, Sreturns):\n",
    "            # Compute the policy and value loss for each time step\n",
    "            ...\n",
    "\n",
    "        # Compute the total loss\n",
    "        total_loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "\n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement one step of backpropagation (gradient descent)\n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "        # After backpropagation, clear the data\n",
    "        del self.saved_log_probs[:]\n",
    "        del self.saved_state_values[:]\n",
    "        del self.saved_rewards[:]\n",
    "\n",
    "        return returns[0].item(), total_loss.item()\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\" Function to collect one episode from the environment\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" YOUR CODE HERE:\n",
    "                Implement the code to collect one episode. \n",
    "                \n",
    "                Collect the rewards, state valuess and log probabilities, which should be stored in\n",
    "                \"self.saved_rewards\", \"self.saved_state_values\", and \"self.saved_log_probs\" respectively.      \n",
    "        \"\"\"\n",
    "\n",
    "        ...\n",
    "\n",
    "    def run_train(self):        \n",
    "        # Lists to store the returns and losses during the training\n",
    "        train_returns = []\n",
    "        train_losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        ep_bar = tqdm.trange(self.params['num_episodes'])\n",
    "        for ep in ep_bar:\n",
    "            \"\"\" YOUR CODE HERE:\n",
    "                    Implement the REINFORCE-with-baseline algorithm here.\n",
    "            \"\"\"\n",
    "            # Collect one episode\n",
    "            ...\n",
    "\n",
    "            # Update the policy using the collected episode\n",
    "            G, loss = None, None\n",
    "            \n",
    "            # Save the return and loss\n",
    "            train_returns.append(G)\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "            # Add description\n",
    "            ep_bar.set_description(f\"Episode: {ep} | Return: {G} | Loss: {loss:.2f}\")\n",
    "        \n",
    "        return train_returns, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351169da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    my_env = FourRooms()\n",
    "\n",
    "    train_params = {\n",
    "        'num_episodes': 10000,\n",
    "        'num_trials': 10,\n",
    "        'learning_rate': 1e-3,\n",
    "        'gamma': 0.99\n",
    "    }\n",
    "\n",
    "    reinforce_baseline_returns = []\n",
    "    reinforce_baseline_losses = []\n",
    "    for _ in range(train_params['num_trials']):\n",
    "        my_agent = REINFORCEBaselineAgent()\n",
    "        my_trainer = REINFORCEBaselineAgentTrainer(my_agent, my_env, train_params)\n",
    "        returns, losses = my_trainer.run_train()\n",
    "        \n",
    "        reinforce_baseline_returns.append(returns)\n",
    "        reinforce_baseline_losses.append(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126457f",
   "metadata": {},
   "source": [
    "### Plot learning and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(reinforce_baseline_returns)], ['REINFORCE with baseline'], ['r'], 'Return', 'Return', smoothing = True)\n",
    "plot_curves([np.array(reinforce_baseline_losses)], ['REINFORCE with baseline'], ['b'], 'Loss', 'Loss', smoothing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa5072",
   "metadata": {},
   "source": [
    "## Part (c): One-step actor-critic (page 332)\n",
    "\n",
    "Implement one-step actor-critic and apply it to Four Rooms, similar to the previous parts. Most of the previous code is reusable, although notice that updates can technically occur after every environment step (instead of waiting until the end of the episode), since REINFORCE uses a Monte-Carlo learning target whereas actor-critic uses the one-step TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1d668-c2b1-476e-a223-46aff9e82c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE:\n",
    "        Implement one-step actor-critic\n",
    "\"\"\"\n",
    "actor_critic_returns, actor_critic_losses = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc503cfb-5618-427c-bb31-b96b10facf01",
   "metadata": {},
   "source": [
    "### Plot learning and loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df0443-5863-4643-b407-e6adc644425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(actor_critic_returns)], ['One-step actor-critic'], ['r'], 'Return', 'Return', smoothing = True)\n",
    "plot_curves([np.array(actor_critic_losses)], ['One-step actor-critic'], ['b'], 'Loss', 'Loss', smoothing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf13d1-f666-4150-9cb7-b62eefff62f0",
   "metadata": {},
   "source": [
    "## Part (d): Compare REINFORCE, REINFORCE with baseline, and one-step actor-critic.\n",
    "\n",
    "Compare the performance of these three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4906214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(reinforce_returns), np.array(reinforce_baseline_returns), np.array(actor_critic_returns)], \n",
    "            ['REINFORCE', 'REINFORCE with baseline', 'One-step actor-critic'], \n",
    "            ['r', 'b', 'k'], 'Returns', \"Comparison between policy-gradient methods\")\n",
    "plot_curves([np.array(reinforce_returns), np.array(reinforce_baseline_returns), np.array(actor_critic_returns)], \n",
    "            ['REINFORCE', 'REINFORCE with baseline', 'One-step actor-critic'], \n",
    "            ['r', 'b', 'k'], 'Loss', \"Comparison between policy-gradient methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089533b-93e0-4fe7-8871-27f2272136bf",
   "metadata": {},
   "source": [
    "## Part (e): Advanced policy-gradient algorithms\n",
    "\n",
    "Many deep policy-gradient algorithms have been proposed in the past 10 years. Read about and implement one or more of these from scratch (e.g., DDPG, TD3, PPO, SAC) and evaluate them on Four Rooms. Compare with the methods above and discuss your findings.\n",
    "\n",
    "We recommend that you first read about some of these algorithms on OpenAI's \"Spinning up in deep RL\" pages, although you should not directly use their implementations in this assignment.\\\n",
    "https://spinningup.openai.com/en/latest/user/algorithms.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
